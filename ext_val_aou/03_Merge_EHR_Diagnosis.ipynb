{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)  # Data wrangling packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"config.R\" for utility functions. \n",
    "#Will also triggger loading of \n",
    "    \n",
    "    # user_config.JSON (including key for project_config)\n",
    "    # project_config.JSON\n",
    "    # preprocessing_visualizations.R\n",
    "    # preprocessing_functions.R\n",
    "\n",
    "user <- \"Jan\" \n",
    "source(\"config.r\")\n",
    "\n",
    "\n",
    "\n",
    "#If certain packages not installed yet via requirements.txt, install them here via\n",
    "# install.packages(\"package_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet assumes that you run setup first\n",
    "\n",
    "# This code lists objects in your Google Bucket\n",
    "\n",
    "# Get the bucket name\n",
    "my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# List objects in the bucket\n",
    "system(paste0(\"gsutil ls -r \", my_bucket), intern=T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query represents dataset \"All ICD Codes\" for domain \"person\" and was generated for All of Us Registered Tier Dataset v7\n",
    "person_86377274_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250709/person_86377274/person_86377274_*.csv\"\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {person_86377274_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(gender = col_character(), race = col_character(), ethnicity = col_character(), sex_at_birth = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "\n",
    "dataset_86377274_person_df <- read_bq_export_from_workspace_bucket(person_86377274_path)\n",
    "\n",
    "dim(dataset_86377274_person_df)\n",
    "\n",
    "head(dataset_86377274_person_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Columns to keep: None\n",
    "### Can get any necessary demographic information later using controlled tier access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EHR Data Old Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EHR Data: Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query represents dataset \"All ICD Codes\" for domain \"observation\" and was generated for All of Us Registered Tier Dataset v7\n",
    "observation_86377274_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250709/observation_86377274/observation_86377274_*.csv\"\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {observation_86377274_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), observation_type_concept_name = col_character(), value_as_string = col_character(), value_as_concept_name = col_character(), qualifier_concept_name = col_character(), unit_concept_name = col_character(), visit_occurrence_concept_name = col_character(), observation_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), unit_source_value = col_character(), qualifier_source_value = col_character(), value_source_value = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "\n",
    "dataset_86377274_observation_df <- read_bq_export_from_workspace_bucket(observation_86377274_path)\n",
    "\n",
    "dim(dataset_86377274_observation_df)\n",
    "\n",
    "head(dataset_86377274_observation_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique(dataset_86377274_observation_df$source_concept_code)\n",
    "#unique(dataset_86377274_observation_df$qualifier_concept_name)\n",
    "#colnames(dataset_86377274_observation_df)\n",
    "\n",
    "### Columns to keep:\n",
    "dataset_86377274_observation_df_filtered <- dataset_86377274_observation_df[c(\"person_id\", \"observation_datetime\", \"observation_type_concept_name\", \"visit_occurrence_id\", \"visit_occurrence_concept_name\", \"source_concept_name\", \"source_concept_code\", \"source_vocabulary\")]\n",
    "dataset_86377274_observation_df_filtered <- dataset_86377274_observation_df_filtered %>% \n",
    "    rename(all_of(c(condition_start_datetime = \"observation_datetime\", condition_type_concept_name = \"observation_type_concept_name\"))) %>%\n",
    "    add_column(condition_end_datetime = as.character(NA), .after = \"condition_start_datetime\") %>%\n",
    "    add_column(condition_type = \"observation\")\n",
    "dataset_86377274_observation_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EHR Data: Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query represents dataset \"All ICD Codes\" for domain \"procedure\" and was generated for All of Us Registered Tier Dataset v7\n",
    "procedure_86377274_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250709/procedure_86377274/procedure_86377274_*.csv\"\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {procedure_86377274_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), procedure_type_concept_name = col_character(), modifier_concept_name = col_character(), visit_occurrence_concept_name = col_character(), procedure_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), modifier_source_value = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "\n",
    "dataset_86377274_procedure_df <- read_bq_export_from_workspace_bucket(procedure_86377274_path)\n",
    "\n",
    "dim(dataset_86377274_procedure_df)\n",
    "\n",
    "head(dataset_86377274_procedure_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique(dataset_86377274_procedure_df$source_concept_code)\n",
    "\n",
    "### Columns to keep:\n",
    "dataset_86377274_procedure_df_filtered <- dataset_86377274_procedure_df[c(\"person_id\", \"procedure_datetime\", \"procedure_type_concept_name\", \"visit_occurrence_id\", \"visit_occurrence_concept_name\", \"source_concept_name\", \"source_concept_code\", \"source_vocabulary\")]\n",
    "dataset_86377274_procedure_df_filtered <- dataset_86377274_procedure_df_filtered %>% \n",
    "    rename(all_of(c(condition_start_datetime = \"procedure_datetime\", condition_type_concept_name = \"procedure_type_concept_name\"))) %>%\n",
    "    add_column(condition_end_datetime = as.character(NA), .after = \"condition_start_datetime\") %>%\n",
    "    add_column(condition_type = \"procedure\")\n",
    "dataset_86377274_procedure_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EHR Data: Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query represents dataset \"All ICD Codes\" for domain \"measurement\" and was generated for All of Us Registered Tier Dataset v7\n",
    "measurement_86377274_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250709/measurement_86377274/measurement_86377274_*.csv\"\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {measurement_86377274_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), measurement_type_concept_name = col_character(), operator_concept_name = col_character(), value_as_concept_name = col_character(), unit_concept_name = col_character(), visit_occurrence_concept_name = col_character(), measurement_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), unit_source_value = col_character(), value_source_value = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "\n",
    "dataset_86377274_measurement_df <- read_bq_export_from_workspace_bucket(measurement_86377274_path)\n",
    "\n",
    "dim(dataset_86377274_measurement_df)\n",
    "\n",
    "head(dataset_86377274_measurement_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique(dataset_86377274_measurement_df$source_concept_code)\n",
    "#colnames(dataset_86377274_measurement_df)\n",
    "\n",
    "### Columns to keep:\n",
    "dataset_86377274_measurement_df_filtered <- dataset_86377274_measurement_df[c(\"person_id\", \"measurement_datetime\", \"measurement_type_concept_name\", \"visit_occurrence_id\", \"visit_occurrence_concept_name\", \"source_concept_name\", \"source_concept_code\", \"source_vocabulary\")]\n",
    "dataset_86377274_measurement_df_filtered <- dataset_86377274_measurement_df_filtered %>% \n",
    "    rename(all_of(c(condition_start_datetime = \"measurement_datetime\", condition_type_concept_name = \"measurement_type_concept_name\"))) %>%\n",
    "    add_column(condition_end_datetime = as.character(NA), .after = \"condition_start_datetime\") %>%\n",
    "    add_column(condition_type = \"measurement\")\n",
    "dataset_86377274_measurement_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EHR Data: Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query represents dataset \"All ICD Codes\" for domain \"condition\" and was generated for All of Us Registered Tier Dataset v7\n",
    "condition_86377274_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250709/condition_86377274/condition_86377274_*.csv\"\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {condition_86377274_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  #col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), condition_type_concept_name = col_character(), stop_reason = col_character(), visit_occurrence_concept_name = col_character(), condition_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), condition_status_source_value = col_character(), condition_status_concept_name = col_character())\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), condition_type_concept_name = col_character(), stop_reason = col_character(), visit_occurrence_concept_name = col_character(), condition_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), condition_status_source_value = col_character(), condition_status_concept_name = col_character())\n",
    "    bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          #chunk_filtered <- chunk[c(\"person_id\", \"condition_start_datetime\", \"condition_end_datetime\", \"condition_type_concept_name\", \"visit_occurrence_id\", \"visit_occurrence_concept_name\", \"source_concept_name\", \"source_concept_code\", \"source_vocabulary\")]\n",
    "        chunk_filtered <- chunk[c(\"person_id\", \"condition_start_datetime\", \"source_concept_code\", \"source_vocabulary\")]  \n",
    "        chunk_filtered <- chunk_filtered %>% \n",
    "            add_column(condition_type = \"condition\")\n",
    "          chunk_filtered\n",
    "        }))\n",
    "}\n",
    "\n",
    "dataset_86377274_condition_df <- read_bq_export_from_workspace_bucket(condition_86377274_path)\n",
    "\n",
    "dim(dataset_86377274_condition_df)\n",
    "\n",
    "head(dataset_86377274_condition_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EHR Data New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query represents dataset \"All ICD Codes\" for domain \"observation\" and was generated for All of Us Registered Tier Dataset v8\n",
    "observation_86377274_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250709/observation_86377274/observation_86377274_*.csv\"\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  # Only specify col_types for columns we actually need\n",
    "  col_types <- cols(\n",
    "    person_id = col_double(),\n",
    "    observation_datetime = col_character(),\n",
    "    source_concept_name = col_character(), \n",
    "    source_concept_code = col_character(), \n",
    "    source_vocabulary = col_character(),\n",
    "    .default = col_skip()  # Skip all other columns during reading\n",
    "  )\n",
    "  \n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          \n",
    "          # Rename datetime column and add condition_type\n",
    "          chunk <- chunk %>% \n",
    "            rename(condition_start_datetime = observation_datetime) %>%\n",
    "            add_column(condition_type = \"observation\")\n",
    "          \n",
    "          return(chunk)\n",
    "        }))\n",
    "}\n",
    "\n",
    "dataset_86377274_observation_df <- read_bq_export_from_workspace_bucket(observation_86377274_path)\n",
    "\n",
    "dim(dataset_86377274_observation_df)\n",
    "head(dataset_86377274_observation_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query represents dataset \"All ICD Codes\" for domain \"procedure\" and was generated for All of Us Registered Tier Dataset v8\n",
    "procedure_86377274_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250709/procedure_86377274/procedure_86377274_*.csv\"\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  # Only specify col_types for columns we actually need\n",
    "  col_types <- cols(\n",
    "    person_id = col_double(),\n",
    "    procedure_datetime = col_character(),\n",
    "    source_concept_name = col_character(), \n",
    "    source_concept_code = col_character(), \n",
    "    source_vocabulary = col_character(),\n",
    "    .default = col_skip()  # Skip all other columns during reading\n",
    "  )\n",
    "  \n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          \n",
    "          # Rename datetime column and add condition_type\n",
    "          chunk <- chunk %>% \n",
    "            rename(condition_start_datetime = procedure_datetime) %>%\n",
    "            add_column(condition_type = \"procedure\")\n",
    "          \n",
    "          return(chunk)\n",
    "        }))\n",
    "}\n",
    "\n",
    "dataset_86377274_procedure_df <- read_bq_export_from_workspace_bucket(procedure_86377274_path)\n",
    "\n",
    "dim(dataset_86377274_procedure_df)\n",
    "head(dataset_86377274_procedure_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query represents dataset \"All ICD Codes\" for domain \"measurement\" and was generated for All of Us Registered Tier Dataset v8\n",
    "measurement_86377274_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250709/measurement_86377274/measurement_86377274_*.csv\"\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  # Only specify col_types for columns we actually need\n",
    "  col_types <- cols(\n",
    "    person_id = col_double(),\n",
    "    measurement_datetime = col_character(),\n",
    "    source_concept_name = col_character(), \n",
    "    source_concept_code = col_character(), \n",
    "    source_vocabulary = col_character(),\n",
    "    .default = col_skip()  # Skip all other columns during reading\n",
    "  )\n",
    "  \n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          \n",
    "          # Rename datetime column and add condition_type\n",
    "          chunk <- chunk %>% \n",
    "            rename(condition_start_datetime = measurement_datetime) %>%\n",
    "            add_column(condition_type = \"measurement\")\n",
    "          \n",
    "          return(chunk)\n",
    "        }))\n",
    "}\n",
    "\n",
    "dataset_86377274_measurement_df <- read_bq_export_from_workspace_bucket(measurement_86377274_path)\n",
    "\n",
    "dim(dataset_86377274_measurement_df)\n",
    "head(dataset_86377274_measurement_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Observations, procedures, measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output file path\n",
    "prelim_output_file <- paste0(data_path, \"/dataframes/AllofUs_v8_obs_proc_meas_codes.txt\")\n",
    "final_output_file <- paste0(data_path, \"/dataframes/df_diagnosis.txt\")\n",
    "condition_output_file <- paste0(data_path, \"/dataframes/AllofUs_v8_condition_codes.txt\")\n",
    "\n",
    "# Start with existing datasets (observation, procedure, measurement)\n",
    "message(\"Starting with existing datasets...\")\n",
    "print(paste(\"Observation rows:\", nrow(dataset_86377274_observation_df)))\n",
    "print(paste(\"Procedure rows:\", nrow(dataset_86377274_procedure_df)))\n",
    "print(paste(\"Measurement rows:\", nrow(dataset_86377274_measurement_df)))\n",
    "\n",
    "# Combine non-condition datasets first\n",
    "all_codes <- rbind(dataset_86377274_observation_df, \n",
    "                   dataset_86377274_procedure_df, \n",
    "                   dataset_86377274_measurement_df)\n",
    "\n",
    "message(paste(\"Combined non-condition data:\", nrow(all_codes), \"rows\"))\n",
    "\n",
    "# Write initial data to file (conditions are memory-heavy so this is a back up in case the kernel crashes)\n",
    "write_delim(all_codes, prelim_output_file, delim = \"\\t\")\n",
    "message(\"Written initial datasets to file\")\n",
    "\n",
    "# Copy to bucket\n",
    "my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "system(paste0(\"gsutil cp ./\", prelim_output_file, \" \", my_bucket, \"/data/\"), intern=T)\n",
    "message(\"File saved to bucket!\")\n",
    "\n",
    "# Clean up to free memory\n",
    "rm(dataset_86377274_observation_df, dataset_86377274_procedure_df, dataset_86377274_measurement_df)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output file\n",
    "condition_output_file <- paste0(data_path, \"/dataframes/AllofUs_v8_condition_codes.txt\")\n",
    "\n",
    "# Get list of all files\n",
    "condition_86377274_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250709/condition_86377274/condition_86377274_*.csv\"\n",
    "all_files <- system2('gsutil', args = c('ls', condition_86377274_path), stdout = TRUE, stderr = TRUE)\n",
    "\n",
    "# Initialize the output file with headers\n",
    "first_batch <- TRUE\n",
    "total_rows <- 0\n",
    "\n",
    "# Process files in small batches and append to final file\n",
    "batch_size <- 5\n",
    "num_batches <- ceiling(length(all_files) / batch_size)\n",
    "\n",
    "for (i in 1:num_batches) {\n",
    "  start_idx <- (i - 1) * batch_size + 1\n",
    "  end_idx <- min(i * batch_size, length(all_files))\n",
    "  \n",
    "  message(str_glue('Processing batch {i}/{num_batches} (files {start_idx} to {end_idx})'))\n",
    "  \n",
    "  file_batch <- all_files[start_idx:end_idx]\n",
    "  \n",
    "  # Process current batch - only specify col_types for columns we actually need\n",
    "  col_types <- cols(\n",
    "    person_id = col_double(),\n",
    "    condition_start_datetime = col_character(),\n",
    "    source_concept_name = col_character(), \n",
    "    source_concept_code = col_character(), \n",
    "    source_vocabulary = col_character(),\n",
    "    .default = col_skip()  # Skip all other columns during reading\n",
    "  )\n",
    "  \n",
    "  batch_data <- bind_rows(\n",
    "    map(file_batch, function(csv) {\n",
    "      message(str_glue('  Loading {basename(csv)}'))\n",
    "      chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "      \n",
    "      # Add condition_type column\n",
    "      chunk <- chunk %>% \n",
    "        add_column(condition_type = \"condition\")\n",
    "      \n",
    "      # No need to filter columns since we only read what we need\n",
    "      # and no need to create chunk_filtered\n",
    "      \n",
    "      return(chunk)\n",
    "    })\n",
    "  )\n",
    "  \n",
    "  # Append to final file (write headers only for first batch)\n",
    "  if (first_batch) {\n",
    "    write_delim(batch_data, condition_output_file, delim = \"\\t\")\n",
    "    first_batch <- FALSE\n",
    "  } else {\n",
    "    write_delim(batch_data, condition_output_file, delim = \"\\t\", append = TRUE, col_names = FALSE)\n",
    "  }\n",
    "  \n",
    "  total_rows <- total_rows + nrow(batch_data)\n",
    "  message(str_glue('  Batch {i} complete. Added {nrow(batch_data)} rows. Total so far: {total_rows}'))\n",
    "  \n",
    "  # Clean up immediately\n",
    "  rm(batch_data)\n",
    "  gc()\n",
    "  \n",
    "  # Small delay\n",
    "  Sys.sleep(1)\n",
    "}\n",
    "\n",
    "message(str_glue('Processing for condition data complete! Final file: {condition_output_file} with {total_rows} rows'))\n",
    "\n",
    "# Copy to bucket\n",
    "my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "system(paste0(\"gsutil cp ./\", prelim_output_file, \" \", my_bucket, \"/data/\"), intern=T)\n",
    "system(paste0(\"gsutil cp ./\", condition_output_file, \" \", my_bucket, \"/data/\"), intern=T)\n",
    "system(paste0(\"gsutil cp ./\", final_output_file, \" \", my_bucket, \"/data/\"), intern=T)\n",
    "\n",
    "message(\"File saved to bucket!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output file path\n",
    "prelim_output_file <- paste0(data_path, \"/dataframes/AllofUs_v8_obs_proc_meas_codes.txt\")\n",
    "final_output_file <- paste0(data_path, \"/dataframes/df_diagnosis.txt\")\n",
    "condition_output_file <- paste0(data_path, \"/dataframes/AllofUs_v8_condition_codes.txt\")\n",
    "\n",
    "# Re-load opm data and condition data\n",
    "message(\"Loading observation/procedure/measurement data...\")\n",
    "opm_data <- read_delim(prelim_output_file, delim = \"\\t\", show_col_types = FALSE)\n",
    "message(paste(\"OPM data loaded:\", nrow(opm_data), \"rows\"))\n",
    "\n",
    "message(\"Loading condition data...\")\n",
    "condition_data <- read_delim(condition_output_file, delim = \"\\t\", show_col_types = FALSE)\n",
    "message(paste(\"Condition data loaded:\", nrow(condition_data), \"rows\"))\n",
    "\n",
    "# Combine the datasets\n",
    "message(\"Combining datasets...\")\n",
    "all_codes <- rbind(opm_data, condition_data)\n",
    "\n",
    "# Clean up intermediate datasets to free memory\n",
    "rm(opm_data, condition_data)\n",
    "gc()\n",
    "\n",
    "# Final summary\n",
    "total_rows <- nrow(all_codes)\n",
    "message(str_glue('Processing complete!'))\n",
    "message(str_glue('Final file: {final_output_file}'))\n",
    "message(str_glue('Total rows: {total_rows}'))\n",
    "\n",
    "# Save the combined dataset\n",
    "message(\"Saving combined dataset...\")\n",
    "write_delim(all_codes, final_output_file, delim = \"\\t\")\n",
    "message(\"Combined dataset saved locally\")\n",
    "\n",
    "# Copy to bucket\n",
    "my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "system(paste0(\"gsutil cp \", final_output_file, \" \", my_bucket, \"/data/\"), intern=T)\n",
    "message(\"File saved to bucket!\")\n",
    "\n",
    "# Clean up\n",
    "rm(all_codes)\n",
    "gc()\n",
    "\n",
    "# Optional: Check the final file structure\n",
    "message(\"Final file structure:\")\n",
    "final_check <- read_delim(final_output_file, delim = \"\\t\", n_max = 5, show_col_types = FALSE)\n",
    "print(head(final_check))\n",
    "print(paste(\"Final file dimensions:\", paste(dim(final_check), collapse = \" x \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes <- read_delim(final_output_file, delim = \"\\t\", show_col_types = FALSE)\n",
    "\n",
    "\n",
    "colnames(all_codes)\n",
    "print(min(all_codes$condition_start_datetime))\n",
    "print(max(all_codes$condition_start_datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sys.getenv('WORKSPACE_BUCKET')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "362.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
