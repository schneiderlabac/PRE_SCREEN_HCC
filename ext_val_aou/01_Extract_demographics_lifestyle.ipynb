{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1 Readme\n",
    "\n",
    "1. \"Helpers\" will load necessary items to proceed with R processing, variable calling etc.\n",
    "2. The following items will download the raw All Of Us Data. There are two options to download items:\n",
    "\n",
    "#### 1. Live, direct\n",
    "Live from BigQuery (bq_table_download):\n",
    "\n",
    "Always freshest Tier-8 data.\n",
    "\n",
    "Great for smaller result sets or ad-hoc pulls.\n",
    "\n",
    "No intermediate files on disk.\n",
    "\n",
    "\n",
    "#### 2. Manual via Google Cloud Storage (GCS) -->\n",
    "Export → GCS → Read (bq_table_save + gsutil + read_csv):\n",
    "\n",
    "Useful for very large tables or when you want to persist a snapshot in GCS.\n",
    "\n",
    "Allows “download once, read many times” workflows.\n",
    "\n",
    "Shares the CSVs across team members or jobs.\n",
    "\n",
    "\n",
    "Standard is GCS Download. The data will be queried from AOU via BIGquery, and directly moved to our Google Cloud bucket. From there it will be read and merged onto the df_covariates. There are several consecutive steps for extracting separate data entities\n",
    "\n",
    "IMPORTANT: If you move these scripts to your own workspace, make sure to change the path for the google cloud bucket, this is different for every workspace!\n",
    "\n",
    "#TODO: Store survey_paths in temporary variables to replace manual exchange of that line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load \"config.R\" for utility functions. \n",
    "#Will also triggger loading of \n",
    "    \n",
    "    # user_config.JSON (including key for project_config)\n",
    "    # project_config.JSON\n",
    "    # preprocessing_visualizations.R\n",
    "    # preprocessing_functions.R\n",
    "\n",
    "user <- \"Jan\" \n",
    "source(\"config.r\")\n",
    "\n",
    "\n",
    "\n",
    "#If certain packages not installed yet via requirements.txt, install them here via\n",
    "# install.packages(\"package_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date of primary consent (LIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get date of primary consent\n",
    "# - https://support.researchallofus.org/hc/en-us/articles/13176125767188-How-to-find-participant-enrollment-data\n",
    "# - use to compute age from dob\n",
    "\n",
    "DATASET <- Sys.getenv('WORKSPACE_CDR')\n",
    "\n",
    "primary_consent_date_df <- bq_table_download(bq_project_query(\n",
    "    Sys.getenv(\"GOOGLE_PROJECT\"), page_size = 25000,\n",
    "    query = str_glue(\"\n",
    "-- Compute the count of unique participants in our All of Us cohort.\n",
    "SELECT DISTINCT\n",
    "    person_id,\n",
    "    MIN(observation_date) AS primary_consent_date\n",
    "FROM \n",
    "    `{DATASET}.concept`\n",
    "JOIN \n",
    "    `{DATASET}.concept_ancestor` \n",
    "        ON concept_id = ancestor_concept_id\n",
    "JOIN \n",
    "    `{DATASET}.observation` \n",
    "        ON descendant_concept_id = observation_source_concept_id\n",
    "WHERE \n",
    "    concept_name = 'Consent PII' AND concept_class_id = 'Module'\n",
    "GROUP BY 1\n",
    "\")))\n",
    "\n",
    "head(primary_consent_date_df)\n",
    "str(primary_consent_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ls()\n",
    "str(primary_consent_date_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dim(primary_consent_date_df)\n",
    "min(primary_consent_date_df$primary_consent_date)\n",
    "\n",
    "print(paste(\"NAs in 'Date of attending' Spalte: \", sum(is.na(primary_consent_date_df$`primary_consent_date`))))\n",
    "\n",
    "print(paste(\"Out of range from 2006-today: \", sum(primary_consent_date_df$`primary_consent_date` < as.Date(\"2006-01-01\") | \n",
    "    primary_consent_date_df$`primary_consent_date` > Sys.Date())))\n",
    "\n",
    "print(paste(\"Inconsistent format: \", sum(as.character(primary_consent_date_df$`primary_consent_date`, format = \"%Y-%m-%d\") != primary_consent_date_df$`primary_consent_date`)))\n",
    "\n",
    "df_covariates <- primary_consent_date_df\n",
    "\n",
    "write.table(df_covariates, file.path(data_path, \"dataframes/df_covariates_1.1.txt\"), sep=\"\\t\", quote=F, row.names=F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age, Sex, Ethnicity (Via GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# This query represents dataset \"df_Covariates\" for domain \"person\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_03039562_person_sql <- paste(\"\n",
    "    SELECT\n",
    "        person.person_id,\n",
    "        person.gender_concept_id,\n",
    "        p_gender_concept.concept_name as gender,\n",
    "        person.birth_datetime as date_of_birth,\n",
    "        person.race_concept_id,\n",
    "        p_race_concept.concept_name as race,\n",
    "        person.ethnicity_concept_id,\n",
    "        p_ethnicity_concept.concept_name as ethnicity,\n",
    "        person.sex_at_birth_concept_id,\n",
    "        p_sex_at_birth_concept.concept_name as sex_at_birth \n",
    "    FROM\n",
    "        `person` person \n",
    "    LEFT JOIN\n",
    "        `concept` p_gender_concept \n",
    "            ON person.gender_concept_id = p_gender_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_race_concept \n",
    "            ON person.race_concept_id = p_race_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_ethnicity_concept \n",
    "            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_sex_at_birth_concept \n",
    "            ON person.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "person_03039562_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"person_03039562\",\n",
    "  \"person_03039562_*.csv\")\n",
    "message(str_glue('The data will be written to {person_03039562_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_03039562_person_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  person_03039562_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {person_03039562_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(gender = col_character(), race = col_character(), ethnicity = col_character(), sex_at_birth = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "#person_03039562_path <- \"gs://fc-secure-b96fb036-3379-4be0-8834-e7e486f2b76e/bq_exports/davidz1@researchallofus.org/20240509/person_03039562/person_03039562_*.csv\"\n",
    "person_03039562_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250701/person_03039562/person_03039562_*.csv\"\n",
    "dataset_03039562_person_df <- read_bq_export_from_workspace_bucket(person_03039562_path)\n",
    "\n",
    "dim(dataset_03039562_person_df)\n",
    "\n",
    "head(dataset_03039562_person_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "unique(dataset_03039562_person_df$race)\n",
    "unique(dataset_03039562_person_df$ethnicity)\n",
    "race_counts <- table(dataset_03039562_person_df$race)\n",
    "\n",
    "# Get the proportions\n",
    "race_proportions <- prop.table(race_counts)\n",
    "\n",
    "# Combine counts and proportions into a data frame\n",
    "race_summary <- data.frame(\n",
    "  Count = race_counts,\n",
    "  Proportion = race_proportions\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(race_summary)\n",
    "race_summary <- race_summary[order(-race_summary$Count.Var1),]\n",
    "pie(race_counts, main=\"Proportion of Races\")\n",
    "\n",
    "# Further preprocessing of race/ethnicity see \"Ethnicity Processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df_covariates <- data.table::fread(file.path(data_path, \"dataframes/df_covariates_1.1.txt\"), sep=\"\\t\")\n",
    "dim(df_covariates)\n",
    "\n",
    "dataset_03039562_person_df <- dataset_03039562_person_df %>%\n",
    "    select(person_id, date_of_birth, sex_at_birth, race, ethnicity)\n",
    "\n",
    "df_covariates <- merge(df_covariates, dataset_03039562_person_df, by=\"person_id\")\n",
    "df_covariates$date_of_birth <- as.Date(df_covariates$date_of_birth)\n",
    "df_covariates$primary_consent_date <- as.Date(df_covariates$primary_consent_date)\n",
    "df_covariates$AGE <- as.numeric(difftime(df_covariates$primary_consent_date, df_covariates$date_of_birth, unit=\"days\"))/365.25\n",
    "\n",
    "df_covariates <- df_covariates %>%\n",
    "    select(-date_of_birth)\n",
    "\n",
    "# df_covariates$AGE_cat <- cut(df_covariates$AGE,\n",
    "#                         breaks = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, max(df_covariates$AGE)),\n",
    "#                         labels = c(\"0-10\", \"10-20\", \"20-30\", \"30-40\", \"40-50\", \"50-60\", \"60-70\", \"70-80\", \"80-90\", \">90\"))\n",
    "# table(df_covariates$AGE_cat, useNA = \"ifany\")\n",
    "\n",
    "df_covariates <- df_covariates %>%\n",
    "    rename(SEX = sex_at_birth) %>%\n",
    "    filter(SEX %in% c(\"Female\",\"Male\"))\n",
    "\n",
    "table(df_covariates$SEX)\n",
    "\n",
    "dim(df_covariates)\n",
    "head(df_covariates)\n",
    "\n",
    "#df_covariates$SEX <- factor(df_covariates$SEX, levels = c(0, 1), labels = c(\"Female\", \"Male\"))\n",
    "\n",
    "#levels(df_covariates$SEX)\n",
    "\n",
    "write.table(df_covariates, file.path(data_path, \"dataframes/df_covariates_1.2.txt\"), sep=\"\\t\", quote=F, row.names=F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMI, Height, Weight, Waist, BP (Via GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# This query represents dataset \"df_Covariates\" for domain \"measurement\" and was generated for All of Us Controlled Tier Dataset v8\n",
    "dataset_35873912_measurement_sql <- paste(\"\n",
    "    SELECT\n",
    "        measurement.person_id,\n",
    "        measurement.measurement_concept_id,\n",
    "        m_standard_concept.concept_name as standard_concept_name,\n",
    "        m_standard_concept.concept_code as standard_concept_code,\n",
    "        m_standard_concept.vocabulary_id as standard_vocabulary,\n",
    "        measurement.measurement_datetime,\n",
    "        measurement.measurement_type_concept_id,\n",
    "        m_type.concept_name as measurement_type_concept_name,\n",
    "        measurement.operator_concept_id,\n",
    "        m_operator.concept_name as operator_concept_name,\n",
    "        measurement.value_as_number,\n",
    "        measurement.value_as_concept_id,\n",
    "        m_value.concept_name as value_as_concept_name,\n",
    "        measurement.unit_concept_id,\n",
    "        m_unit.concept_name as unit_concept_name,\n",
    "        measurement.range_low,\n",
    "        measurement.range_high,\n",
    "        measurement.visit_occurrence_id,\n",
    "        m_visit.concept_name as visit_occurrence_concept_name,\n",
    "        measurement.measurement_source_value,\n",
    "        measurement.measurement_source_concept_id,\n",
    "        m_source_concept.concept_name as source_concept_name,\n",
    "        m_source_concept.concept_code as source_concept_code,\n",
    "        m_source_concept.vocabulary_id as source_vocabulary,\n",
    "        measurement.unit_source_value,\n",
    "        measurement.value_source_value \n",
    "    FROM\n",
    "        ( SELECT\n",
    "            * \n",
    "        FROM\n",
    "            `measurement` measurement \n",
    "        WHERE\n",
    "            (\n",
    "                measurement_source_concept_id IN (SELECT\n",
    "                    DISTINCT c.concept_id \n",
    "                FROM\n",
    "                    `cb_criteria` c \n",
    "                JOIN\n",
    "                    (SELECT\n",
    "                        CAST(cr.id as string) AS id       \n",
    "                    FROM\n",
    "                        `cb_criteria` cr       \n",
    "                    WHERE\n",
    "                        concept_id IN (903107, 903115, 903118, 903121, 903124, 903133, 903135)       \n",
    "                        AND full_text LIKE '%_rank1]%'      ) a \n",
    "                        ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                        OR c.path LIKE CONCAT('%.', a.id) \n",
    "                        OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                        OR c.path = a.id) \n",
    "                WHERE\n",
    "                    is_standard = 0 \n",
    "                    AND is_selectable = 1)\n",
    "            )) measurement \n",
    "    LEFT JOIN\n",
    "        `concept` m_standard_concept \n",
    "            ON measurement.measurement_concept_id = m_standard_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_type \n",
    "            ON measurement.measurement_type_concept_id = m_type.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_operator \n",
    "            ON measurement.operator_concept_id = m_operator.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_value \n",
    "            ON measurement.value_as_concept_id = m_value.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_unit \n",
    "            ON measurement.unit_concept_id = m_unit.concept_id \n",
    "    LEFT JOIn\n",
    "        `visit_occurrence` v \n",
    "            ON measurement.visit_occurrence_id = v.visit_occurrence_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_visit \n",
    "            ON v.visit_concept_id = m_visit.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_source_concept \n",
    "            ON measurement.measurement_source_concept_id = m_source_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "measurement_35873912_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"measurement_35873912\",\n",
    "  \"measurement_35873912_*.csv\")\n",
    "message(str_glue('The data will be written to {measurement_35873912_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_35873912_measurement_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  measurement_35873912_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {measurement_35873912_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), measurement_type_concept_name = col_character(), operator_concept_name = col_character(), value_as_concept_name = col_character(), unit_concept_name = col_character(), visit_occurrence_concept_name = col_character(), measurement_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), unit_source_value = col_character(), value_source_value = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "\n",
    "#measurement_35873912_path <- \"gs://fc-secure-b96fb036-3379-4be0-8834-e7e486f2b76e/bq_exports/davidz1@researchallofus.org/20240517/measurement_35873912/measurement_35873912_*.csv\"\n",
    "measurement_35873912_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250701/measurement_35873912/measurement_35873912_*.csv\"\n",
    "\n",
    "dataset_35873912_measurement_df <- read_bq_export_from_workspace_bucket(measurement_35873912_path)\n",
    "\n",
    "dim(dataset_35873912_measurement_df)\n",
    "\n",
    "head(dataset_35873912_measurement_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df_covariates <- data.table::fread(file.path(data_path, \"dataframes/df_covariates_1.2.txt\"), sep=\"\\t\")\n",
    "dim(df_covariates)\n",
    "\n",
    "measurement_df <- dataset_35873912_measurement_df %>%\n",
    "    select(person_id, standard_concept_name, value_as_number, measurement_datetime) %>%\n",
    "    filter(standard_concept_name != \"Computed blood pressure systolic and diastolic, mean of 2nd and 3rd measures\") %>%\n",
    "    mutate(obs_date = as.Date(measurement_datetime))\n",
    "\n",
    "measurement_df$standard_concept_name[measurement_df$standard_concept_name == \"Body height\"] <- \"Standing height\"\n",
    "measurement_df$standard_concept_name[measurement_df$standard_concept_name == \"Body mass index (BMI) [Ratio]\"] <- \"BMI\"\n",
    "measurement_df$standard_concept_name[measurement_df$standard_concept_name == \"Computed systolic blood pressure, mean of 2nd and 3rd measures\"] <- \"SBP\"\n",
    "measurement_df$standard_concept_name[measurement_df$standard_concept_name == \"Computed diastolic blood pressure, mean of 2nd and 3rd measures\"] <- \"DBP\"\n",
    "measurement_df$standard_concept_name[measurement_df$standard_concept_name == \"Computed waist circumference, mean of closest two measures\"] <- \"Waist circumference\"\n",
    "measurement_df$standard_concept_name[measurement_df$standard_concept_name == \"Body weight\"] <- \"Weight\"\n",
    "\n",
    "measurement_df_wide <- measurement_df %>%\n",
    "  pivot_wider(\n",
    "    id_cols     = c(person_id, obs_date),\n",
    "    names_from  = standard_concept_name,\n",
    "    values_from = value_as_number,\n",
    "    values_fn   = mean,     # take the average of duplicates in one day\n",
    "    values_fill = NA        # fill in truly missing labs\n",
    "  )\n",
    "\n",
    "df_covariates <- merge(df_covariates, measurement_df_wide, by=\"person_id\")\n",
    "\n",
    "dim(df_covariates)\n",
    "head(df_covariates)\n",
    "\n",
    "### Impute by calculation\n",
    "bmi_NA <- sum(is.na(df_covariates$BMI))\n",
    "print(paste(\"Number of NA values in BMI before imputation:\", bmi_NA))\n",
    "\n",
    "df_covariates$BMI[is.na(df_covariates$`BMI`) & !is.na(df_covariates$Weight) & !is.na(df_covariates$`Standing height`)] <- df_covariates$Weight[is.na(df_covariates$`BMI`) & !is.na(df_covariates$Weight) & !is.na(df_covariates$`Standing height`)] / (df_covariates$`Standing height`[is.na(df_covariates$`BMI`) & !is.na(df_covariates$Weight) & !is.na(df_covariates$`Standing height`)]/100)^2\n",
    "bmi_NA <- sum(is.na(df_covariates$BMI))\n",
    "print(paste(\"Number of NA values in BMI after manual calculation:\", bmi_NA))\n",
    "\n",
    "### Impute by waist circumference in categories\n",
    "df_covariates$BMI_cat <- cut(df_covariates$BMI,\n",
    "                        breaks = c(0, 18.5, 24.9, 29.9, max(df_covariates$BMI, na.rm=T)),\n",
    "                        labels = c(\"Underweight\", \"Normal weight\", \"Overweight\", \"Obese\"))\n",
    "bmi_NA <- sum(is.na(df_covariates$BMI) & !is.na(df_covariates$`Waist circumference`))\n",
    "print(paste(\"Number of NA values that could be imputed by estimation from Waist circumference:\", bmi_NA))\n",
    "\n",
    "### ********** CHANGED SEX *************** ###\n",
    "#Label as obese or normal according to WHO definition\n",
    "df_covariates$BMI_cat[is.na(df_covariates$BMI_cat) & (df_covariates$`Waist circumference` >= 80) & (df_covariates$SEX == \"Female\")] <- \"Obese\"\n",
    "df_covariates$BMI_cat[is.na(df_covariates$BMI_cat) & (df_covariates$`Waist circumference` < 80) & (df_covariates$SEX == \"Female\")] <- \"Normal weight\"\n",
    "df_covariates$BMI_cat[is.na(df_covariates$BMI_cat) & (df_covariates$`Waist circumference` >= 94) & (df_covariates$SEX == \"Male\")] <- \"Obese\"\n",
    "df_covariates$BMI_cat[is.na(df_covariates$BMI_cat) & (df_covariates$`Waist circumference` < 94) & (df_covariates$SEX == \"Male\")] <- \"Normal weight\"\n",
    "\n",
    "# Calculate mean for Group \"Normal weight\" and \"Obese\" according to waist circumference and Sex\n",
    "normal_men <- mean(df_covariates$BMI[df_covariates$BMI_cat == \"Normal weight\" & df_covariates$SEX==\"Male\"], na.rm=TRUE)\n",
    "normal_women <- mean(df_covariates$BMI[df_covariates$BMI_cat == \"Normal weight\" & df_covariates$SEX==\"Female\"], na.rm=TRUE)\n",
    "obese_men <- mean(df_covariates$BMI[df_covariates$BMI_cat == \"Obese\" & df_covariates$SEX==\"Male\"], na.rm=TRUE)\n",
    "obese_women <- mean(df_covariates$BMI[df_covariates$BMI_cat == \"Obese\" & df_covariates$SEX==\"Female\"], na.rm=TRUE)\n",
    "\n",
    "#Store mean of groups normal/obese for men/women (Not great but better than just mean imputing)\n",
    "df_covariates$BMI[is.na(df_covariates$BMI) & !is.na(df_covariates$`Waist circumference`) & df_covariates$SEX== \"Male\"] <- ifelse(df_covariates$BMI_cat[is.na(df_covariates$BMI) & !is.na(df_covariates$`Waist circumference`) & df_covariates$SEX== \"Male\"] == \"Normal weight\", normal_men, obese_men) \n",
    "df_covariates$BMI[is.na(df_covariates$BMI) & !is.na(df_covariates$`Waist circumference`) & df_covariates$SEX== \"Female\"] <- ifelse(df_covariates$BMI_cat[is.na(df_covariates$BMI) & !is.na(df_covariates$`Waist circumference`) & df_covariates$SEX== \"Female\"] == \"Normal weight\", normal_women, obese_women) \n",
    "\n",
    "bmi_NA <- sum(is.na(df_covariates$BMI))\n",
    "print(paste(\"Number of NA values after imputation from Waist circumference:\", bmi_NA))\n",
    "\n",
    "dim(df_covariates)\n",
    "head(df_covariates)\n",
    "\n",
    "#dim(df_covariates[complete.cases(df_covariates[,c(\"BMI\", \"Waist circumference\", \"Weight\", \"Standing height\")]), ])\n",
    "#df_covariates <- df_covariates[complete.cases(df_covariates[,c(\"BMI\", \"Waist circumference\", \"Weight\", \"Standing height\")]), ]\n",
    "\n",
    "write.table(df_covariates, file.path(data_path, \"dataframes/df_covariates_1.3.txt\"), sep=\"\\t\", quote=F, row.names=F)\n",
    "print(\"Writing successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoking (via GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# This query represents dataset \"df_df_Smoking\" for domain \"survey\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_22980624_survey_sql <- paste(\"\n",
    "    SELECT\n",
    "        answer.person_id,\n",
    "        answer.survey_datetime,\n",
    "        answer.survey,\n",
    "        answer.question_concept_id,\n",
    "        answer.question,\n",
    "        answer.answer_concept_id,\n",
    "        answer.answer,\n",
    "        answer.survey_version_concept_id,\n",
    "        answer.survey_version_name  \n",
    "    FROM\n",
    "        `ds_survey` answer   \n",
    "    WHERE\n",
    "        (\n",
    "            question_concept_id IN (1585857, 1585860, 1585873, 1586159, 1586162)\n",
    "        )\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "survey_22980624_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"survey_22980624\",\n",
    "  \"survey_22980624_*.csv\")\n",
    "message(str_glue('The data will be written to {survey_22980624_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_22980624_survey_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  survey_22980624_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {survey_22980624_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(survey = col_character(), question = col_character(), answer = col_character(), survey_version_name = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "survey_22980624_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250701/survey_22980624/survey_22980624_*.csv\"\n",
    "dataset_22980624_survey_df <- read_bq_export_from_workspace_bucket(survey_22980624_path)\n",
    "\n",
    "dim(dataset_22980624_survey_df)\n",
    "\n",
    "head(dataset_22980624_survey_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df_covariates <- data.table::fread(file.path(data_path, \"dataframes/df_covariates_1.3.txt\"), sep=\"\\t\")\n",
    "head(df_covariates)\n",
    "\n",
    "survey_df <- dataset_22980624_survey_df %>%\n",
    "    select(person_id, question, answer)\n",
    "\n",
    "survey_df$question[survey_df$question == \"Smoking: 100 Cigs Lifetime\"] <- \"Ever smoked\"\n",
    "survey_df$question[survey_df$question == \"Smoking: Smoke Frequency\"] <- \"Smoking status\"\n",
    "survey_df$question[survey_df$question == \"Smoking: Average Daily Cigarette Number\"] <- \"Avg daily\"\n",
    "survey_df$question[survey_df$question == \"Smoking: Current Daily Cigarette Number\"] <- \"Current daily\"\n",
    "survey_df$question[survey_df$question == \"Smoking: Number Of Years\"] <- \"Years\"\n",
    "\n",
    "survey_df <- pivot_wider(survey_df, names_from = question, values_from = answer)\n",
    "\n",
    "df_covariates <- merge(df_covariates, survey_df, by=\"person_id\")\n",
    "\n",
    "# Change ever smoked to yes, no, NA (we don't know)\n",
    "df_covariates$`Ever smoked`[df_covariates$`Ever smoked` == \"100 Cigs Lifetime: Yes\"] <- \"Yes\"\n",
    "df_covariates$`Ever smoked`[df_covariates$`Ever smoked` == \"100 Cigs Lifetime: No\"] <- \"No\"\n",
    "df_covariates$`Ever smoked`[grepl(\"PMI\", df_covariates$`Ever smoked`)] <- NA\n",
    "\n",
    "# Change avg daily and years to NA if no answer (there's no way to impute)\n",
    "df_covariates$`Avg daily`[grepl(\"PMI\", df_covariates$`Avg daily`)] <- NA\n",
    "df_covariates$`Years`[grepl(\"PMI\", df_covariates$`Years`)] <- NA\n",
    "\n",
    "# Compute pack years\n",
    "df_covariates$`Pack years` <- NA\n",
    "df_covariates$`Pack years`[!is.na(df_covariates$`Avg daily`) & !is.na(df_covariates$`Years`)] <- (as.numeric(df_covariates$`Avg daily`[!is.na(df_covariates$`Avg daily`) & !is.na(df_covariates$`Years`)]) / 20) * \n",
    "                                                                                                           as.numeric(df_covariates$`Years`[!is.na(df_covariates$`Avg daily`) & !is.na(df_covariates$`Years`)])\n",
    "quantile(df_covariates$`Pack years`, na.rm=T)\n",
    "\n",
    "#Impute ever smoked (is NA - no way of knowing)\n",
    "df_covariates$`Ever smoked`[is.na(df_covariates$`Ever smoked`) & df_covariates$`Pack years` >= 0] <- \"Yes\"\n",
    "df_covariates$`Ever smoked`[is.na(df_covariates$`Ever smoked`) & df_covariates$`Smoking status` != \"Never\"] <- \"Yes\"\n",
    "df_covariates$`Ever smoked`[is.na(df_covariates$`Ever smoked`) & df_covariates$`Current daily` > 0] <- \"Yes\"\n",
    "#df_covariates$`Ever smoked`[is.na(df_covariates$`Ever smoked`)] <- \"No\"\n",
    "#table(df_covariates$`Ever smoked`, useNA = \"ifany\")\n",
    "\n",
    "# Impute smoking status\n",
    "df_covariates$`Smoking status`[df_covariates$`Smoking status` == \"Smoke Frequency: Every Day\"] <- \"Current\"\n",
    "df_covariates$`Smoking status`[df_covariates$`Smoking status` == \"Smoke Frequency: Some Days\"] <- \"Current\"\n",
    "df_covariates$`Smoking status`[df_covariates$`Smoking status` == \"Smoke Frequency: Not At All\"] <- \"Previous\"\n",
    "df_covariates$`Smoking status`[grepl(\"PMI\", df_covariates$`Smoking status`)] <- \"Previous\"\n",
    "df_covariates$`Smoking status`[df_covariates$`Ever smoked` == \"Yes\" & is.na(df_covariates$`Smoking status`)] <- \"Previous\"\n",
    "df_covariates$`Smoking status`[df_covariates$`Ever smoked` == \"No\" & is.na(df_covariates$`Smoking status`)] <- \"Never\"\n",
    "\n",
    "#Pack years = 0 imputed from Smoking status=never or Ever smoked=No\n",
    "df_covariates$`Pack years`[df_covariates$`Ever smoked` == \"No\" | df_covariates$`Smoking status` == \"Never\"] <- 0 \n",
    "\n",
    "#All others have smoking = yes and will get the mean Pack years\n",
    "print(paste0(\"Mean pack years: \", mean(df_covariates$`Pack years`, na.rm=T)))\n",
    "df_covariates$`Pack years`[is.na(df_covariates$`Pack years`) & !is.na(df_covariates$`Ever smoked`)] <- mean(df_covariates$`Pack years`, na.rm=T)\n",
    "quantile(df_covariates$`Pack years`, na.rm=T)\n",
    "\n",
    "#############################\n",
    "### Should missing smoking also get mean pack years? for now no - can add later or just remove pts\n",
    "#############################\n",
    "\n",
    "df_covariates <- df_covariates %>%\n",
    "    select(-`Avg daily`, -`Current daily`, -`Years`)\n",
    "\n",
    "sum(is.na(df_covariates$`Smoking status`))\n",
    "sum(is.na(df_covariates$`Ever smoked`))\n",
    "sum(is.na(df_covariates$`Pack years`))\n",
    "\n",
    "#set factors with labels/levels\n",
    "#df_covariates[\"Ever smoked\"] <- factor(df_covariates$`Ever smoked`, levels=c(0, 1), labels = c(\"No\", \"Yes\"))\n",
    "#df_covariates[\"Smoking status\"] <- factor(df_covariates$`Smoking status`, levels=c(0, 1, 2), labels = c(\"Never\", \"Previous\", \"Current\"))\n",
    "\n",
    "dim(df_covariates)\n",
    "head(df_covariates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df_covariates <- adjust_outliers(df_covariates, 'Pack years')\n",
    "\n",
    "quantile(df_covariates$`Pack years`, na.rm=T)\n",
    "\n",
    "dim(df_covariates)\n",
    "head(df_covariates)\n",
    "\n",
    "write.table(df_covariates, file.path(data_path, \"dataframes/df_covariates_1.4.txt\"), sep=\"\\t\", quote=F, row.names=F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alcohol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# This query represents dataset \"df_df_Alcohol\" for domain \"survey\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_05380700_survey_sql <- paste(\"\n",
    "    SELECT\n",
    "        answer.person_id,\n",
    "        answer.survey_datetime,\n",
    "        answer.survey,\n",
    "        answer.question_concept_id,\n",
    "        answer.question,\n",
    "        answer.answer_concept_id,\n",
    "        answer.answer,\n",
    "        answer.survey_version_concept_id,\n",
    "        answer.survey_version_name  \n",
    "    FROM\n",
    "        `ds_survey` answer   \n",
    "    WHERE\n",
    "        (\n",
    "            question_concept_id IN (1586198, 1586201, 1586207, 1586213)\n",
    "        )\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "survey_05380700_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"survey_05380700\",\n",
    "  \"survey_05380700_*.csv\")\n",
    "message(str_glue('The data will be written to {survey_05380700_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_05380700_survey_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  survey_05380700_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {survey_05380700_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(survey = col_character(), question = col_character(), answer = col_character(), survey_version_name = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "survey_05380700_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250701/survey_05380700/survey_05380700_*.csv\"\n",
    "dataset_05380700_survey_df <- read_bq_export_from_workspace_bucket(survey_05380700_path)\n",
    "\n",
    "dim(dataset_05380700_survey_df)\n",
    "\n",
    "head(dataset_05380700_survey_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df_covariates <- data.table::fread(file.path(data_path, \"dataframes/df_covariates_1.4.txt\"), sep=\"\\t\")\n",
    "head(df_covariates)\n",
    "\n",
    "survey_df <- dataset_05380700_survey_df %>%\n",
    "    select(person_id, question, answer)\n",
    "\n",
    "survey_df$question[survey_df$question == \"Alcohol: Alcohol Participant\"] <- \"Ever drank\"\n",
    "survey_df$question[survey_df$question == \"Alcohol: Drink Frequency Past Year\"] <- \"Past year\"\n",
    "survey_df$question[survey_df$question == \"Alcohol: Average Daily Drink Count\"] <- \"Avg daily\"\n",
    "survey_df$question[survey_df$question == \"Alcohol: 6 or More Drinks Occurrence\"] <- \"More six\"\n",
    "\n",
    "survey_df <- pivot_wider(survey_df, names_from = question, values_from = answer)\n",
    "\n",
    "df_covariates <- merge(df_covariates, survey_df, by=\"person_id\")\n",
    "\n",
    "# Rename `Past year`\n",
    "df_covariates$`Past year`[df_covariates$`Past year` == \"Drink Frequency Past Year: Never\"] <- 0\n",
    "df_covariates$`Past year`[df_covariates$`Past year` == \"Drink Frequency Past Year: Monthly Or Less\"] <- 1/4.345\n",
    "df_covariates$`Past year`[df_covariates$`Past year` == \"Drink Frequency Past Year: 2 to 4 Per Month\"] <- 3/4.345\n",
    "df_covariates$`Past year`[df_covariates$`Past year` == \"Drink Frequency Past Year: 2 to 3 Per Week\"] <- 2.5\n",
    "df_covariates$`Past year`[df_covariates$`Past year` == \"Drink Frequency Past Year: 4 or More Per Week\"] <- 4\n",
    "df_covariates$`Past year`[grepl(\"PMI\", df_covariates$`Past year`)] <- NA\n",
    "\n",
    "# Rename `Avg daily`\n",
    "df_covariates$`Avg daily`[df_covariates$`Avg daily` == \"Average Daily Drink Count: 1 or 2\"] <- 1.5\n",
    "df_covariates$`Avg daily`[df_covariates$`Avg daily` == \"Average Daily Drink Count: 3 or 4\"] <- 3.5\n",
    "df_covariates$`Avg daily`[df_covariates$`Avg daily` == \"Average Daily Drink Count: 5 or 6\"] <- 5.5\n",
    "df_covariates$`Avg daily`[df_covariates$`Avg daily` == \"Average Daily Drink Count: 7 to 9\"] <- 8\n",
    "df_covariates$`Avg daily`[df_covariates$`Avg daily` == \"Average Daily Drink Count: 10 or More\"] <- 10\n",
    "df_covariates$`Avg daily`[grepl(\"PMI\", df_covariates$`Avg daily`)] <- NA\n",
    "\n",
    "# Compute alk_g_d\n",
    "df_covariates$`Alk_g_d` <- NA\n",
    "df_covariates$`Alk_g_d`[!is.na(df_covariates$`Past year`) & !is.na(df_covariates$`Avg daily`)] <- \n",
    "    as.numeric(df_covariates$`Past year`[!is.na(df_covariates$`Past year`) & !is.na(df_covariates$`Avg daily`)]) * \n",
    "    as.numeric(df_covariates$`Avg daily`[!is.na(df_covariates$`Past year`) & !is.na(df_covariates$`Avg daily`)]) *\n",
    "    14 / 7\n",
    "df_covariates$`Alk_g_d`[df_covariates$`Ever drank` == \"Alcohol Participant: No\"] <- 0\n",
    "df_covariates$`Alk_g_d`[df_covariates$`Past year` == 0] <- 0\n",
    "# Otherwise keep NA if missing - if they prefer not to answer, we cannot make assumption\n",
    "\n",
    "df_covariates <- df_covariates %>%\n",
    "    select(-`Ever drank`, -`Past year`, -`Avg daily`, -`More six`)\n",
    "\n",
    "quantile(df_covariates$`Alk_g_d`, na.rm=T)\n",
    "\n",
    "dim(df_covariates)\n",
    "head(df_covariates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df_covariates <- adjust_outliers(df_covariates, 'Alk_g_d')\n",
    "\n",
    "# WHO's gender-specific limits\n",
    "df_covariates$Path_Alk <- ifelse(df_covariates$SEX == \"Male\", df_covariates$`Alk_g_d` / 60, df_covariates$`Alk_g_d` / 40)\n",
    "df_covariates$Path_Alk <- ifelse(df_covariates$Path_Alk >= 1, 1, ifelse(df_covariates$Path_Alk < 0.9999999999, 0, df_covariates$Path_Alk))\n",
    "df_covariates$High_Alk <- ifelse(df_covariates$SEX == \"Male\", df_covariates$`Alk_g_d` / 24, df_covariates$`Alk_g_d` / 12)\n",
    "df_covariates$High_Alk <- ifelse(df_covariates$High_Alk >= 1, 1, ifelse(df_covariates$High_Alk < 0.9999999999, 0, df_covariates$High_Alk))\n",
    "\n",
    "dim(df_covariates)\n",
    "head(df_covariates)\n",
    "\n",
    "write.table(df_covariates, file.path(data_path, \"dataframes/df_covariates_1.5.txt\"), sep=\"\\t\", quote=F, row.names=F)\n",
    "\n",
    "sum(!is.na(df_covariates$`Alk_g_d`) & df_covariates$`Alk_g_d` > 0)\n",
    "quantile(df_covariates$`Alk_g_d`, na.rm=T)\n",
    "mean(df_covariates$`Alk_g_d`, na.rm=T)\n",
    "sd(df_covariates$`Alk_g_d`, na.rm=T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprivation index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# This query represents dataset \"df_Deprivation\" for domain \"zip_code_socioeconomic\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_14505310_zip_code_socioeconomic_sql <- paste(\"\n",
    "    SELECT\n",
    "        observation.person_id,\n",
    "        observation.observation_datetime,\n",
    "        zip_code.zip3_as_string as zip_code,\n",
    "        zip_code.fraction_assisted_income as assisted_income,\n",
    "        zip_code.fraction_high_school_edu as high_school_education,\n",
    "        zip_code.median_income,\n",
    "        zip_code.fraction_no_health_ins as no_health_insurance,\n",
    "        zip_code.fraction_poverty as poverty,\n",
    "        zip_code.fraction_vacant_housing as vacant_housing,\n",
    "        zip_code.deprivation_index,\n",
    "        zip_code.acs as american_community_survey_year \n",
    "    FROM\n",
    "        `zip3_ses_map` zip_code \n",
    "    JOIN\n",
    "        `observation` observation \n",
    "            ON CAST(SUBSTR(observation.value_as_string, 0, STRPOS(observation.value_as_string, '*') - 1) AS INT64) = zip_code.zip3 \n",
    "            AND observation_source_concept_id = 1585250 \n",
    "            AND observation.value_as_string NOT LIKE 'Res%'\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "zip_code_socioeconomic_14505310_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"zip_code_socioeconomic_14505310\",\n",
    "  \"zip_code_socioeconomic_14505310_*.csv\")\n",
    "message(str_glue('The data will be written to {zip_code_socioeconomic_14505310_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_14505310_zip_code_socioeconomic_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  zip_code_socioeconomic_14505310_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(bigrquery)\n",
    "\n",
    "# 1. Identify the project from the WORKSPACE_CDR env var:\n",
    "#    This is usually of the form \"fc-aou-cdr-prod.C2024Q3R4\"\n",
    "workspace_cdr <- Sys.getenv(\"WORKSPACE_CDR\")\n",
    "parts <- strsplit(workspace_cdr, \"\\\\.\")[[1]]\n",
    "project_id <- parts[1]       # \"fc-aou-cdr-prod\"\n",
    "dataset_id <- parts[2]       # \"C2024Q3R4\"\n",
    "\n",
    "# 2. List all datasets in the Controlled-Tier project (to verify names):\n",
    "all_datasets <- bq_project_datasets(project_id)\n",
    "print(all_datasets)          # shows e.g. \"C2024Q3R4\", \"C2024Q3R4_base\", etc.\n",
    "\n",
    "# 3. List all tables in your specific CDR v8 dataset:\n",
    "cdr_dataset <- bq_dataset(project_id, dataset_id)\n",
    "tables <- bq_dataset_tables(cdr_dataset)\n",
    "print(tables)                # e.g. tables$table_id includes \"observation\", \"zip3_ses_map\"…\n",
    "\n",
    "# 4. Inspect the schema of a specific table:\n",
    "zip3_table <- bq_table(project_id, dataset_id, \"zip3_ses_map\")\n",
    "bq_table_meta(zip3_table)$schema$fields  # shows each column and its type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(bigrquery)\n",
    "\n",
    "# 1. get your CDR project\n",
    "cdr_proj <- strsplit(Sys.getenv(\"WORKSPACE_CDR\"), \"\\\\.\")[[1]][1]\n",
    "\n",
    "# 2. list every dataset in that project\n",
    "all_ds <- bq_project_datasets(cdr_proj)\n",
    "ds_ids <- all_ds$dataset # vector of names\n",
    "\n",
    "# 3. filter to Controlled-Tier (starts with \"C\")\n",
    "ct_ds <- grep(\"^C\", ds_ids, value = TRUE)\n",
    "print(ct_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "\n",
    "Sys.getenv(\"WORKSPACE_CDR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {zip_code_socioeconomic_14505310_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(zip3_as_string = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "zip_code_socioeconomic_14505310_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250701/zip_code_socioeconomic_14505310/zip_code_socioeconomic_14505310_*.csv\"\n",
    "dataset_14505310_zip_code_socioeconomic_df <- read_bq_export_from_workspace_bucket(zip_code_socioeconomic_14505310_path)\n",
    "\n",
    "dim(dataset_14505310_zip_code_socioeconomic_df)\n",
    "\n",
    "head(dataset_14505310_zip_code_socioeconomic_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df_covariates <- data.table::fread(file.path(data_path, \"dataframes/df_covariates_1.5.txt\"), sep=\"\\t\")\n",
    "head(df_covariates)\n",
    "\n",
    "dep_idx <- dataset_14505310_zip_code_socioeconomic_df %>%\n",
    "    select(person_id, deprivation_index)\n",
    "\n",
    "df_covariates <- merge(df_covariates, dep_idx, by=\"person_id\")\n",
    "\n",
    "dim(df_covariates)\n",
    "head(df_covariates)\n",
    "\n",
    "write.table(df_covariates, file.path(data_path, \"dataframes/df_covariates_1.6.txt\"), sep=\"\\t\", quote=F, row.names=F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# This query represents dataset \"df_df_Medications\" for domain \"survey\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_00748392_survey_sql <- paste(\"\n",
    "    SELECT\n",
    "        answer.person_id,\n",
    "        answer.survey_datetime,\n",
    "        answer.survey,\n",
    "        answer.question_concept_id,\n",
    "        answer.question,\n",
    "        answer.answer_concept_id,\n",
    "        answer.answer,\n",
    "        answer.survey_version_concept_id,\n",
    "        answer.survey_version_name  \n",
    "    FROM\n",
    "        `ds_survey` answer   \n",
    "    WHERE\n",
    "        (\n",
    "            question_concept_id IN (1384437, 1384641, 43528793, 43528819, 43528820, 836799, 836800)\n",
    "        )\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "survey_00748392_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"survey_00748392\",\n",
    "  \"survey_00748392_*.csv\")\n",
    "message(str_glue('The data will be written to {survey_00748392_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_00748392_survey_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  survey_00748392_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {survey_00748392_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(survey = col_character(), question = col_character(), answer = col_character(), survey_version_name = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "survey_00748392_path <- \"gs://fc-secure-cde9a0f0-7d5a-4045-98bb-fb1d394a535b/bq_exports/janclusmann@researchallofus.org/20250701/survey_00748392/survey_00748392_*.csv\"\n",
    "dataset_00748392_survey_df <- read_bq_export_from_workspace_bucket(survey_00748392_path)\n",
    "\n",
    "dim(dataset_00748392_survey_df)\n",
    "\n",
    "head(dataset_00748392_survey_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df_covariates <- data.table::fread(file.path(data_path, \"dataframes/df_covariates_1.6.txt\"), sep=\"\\t\")\n",
    "head(df_covariates)\n",
    "\n",
    "survey_df <- dataset_00748392_survey_df %>%\n",
    "    select(person_id, question, answer)\n",
    "\n",
    "survey_df$question[survey_df$question == \"Are you currently prescribed medications and/or receiving treatment for high blood pressure (hypertension)?\"] <- \"HTN\"\n",
    "survey_df$question[survey_df$question == \"Are you currently prescribed medications and/or receiving treatment for high cholesterol?\"] <- \"High Chol\"\n",
    "survey_df$question[survey_df$question == \"Are you currently prescribed medications and/or receiving treatment for other hormone/endocrine condition(s)?\"] <- \"Hormone\"\n",
    "survey_df$question[survey_df$question == \"Are you currently prescribed medications and/or receiving treatment for type 1 diabetes?\"] <- \"T1DM\"\n",
    "survey_df$question[survey_df$question == \"Are you currently prescribed medications and/or receiving treatment for type 2 diabetes?\"] <- \"T2DM\"\n",
    "survey_df$question[survey_df$question == \"Including yourself, who in your family has had type 1 diabetes? Select all that apply.\"] <- \"T1DM Fam\"\n",
    "survey_df$question[survey_df$question == \"Including yourself, who in your family has had type 2 diabetes? Select all that apply.\"] <- \"T2DM Fam\"\n",
    "\n",
    "# Set NA responses to 0 as well\n",
    "htn_ids <- survey_df$person_id[survey_df$question == \"HTN\" & survey_df$answer == \"Are you currently prescribed medications and/or receiving treatment for high blood pressure (hypertension)? - Yes\"]\n",
    "chol_ids <- survey_df$person_id[survey_df$question == \"High Chol\" & survey_df$answer == \"Are you currently prescribed medications and/or receiving treatment for high cholesterol? - Yes\"]\n",
    "t1dm_ids <- survey_df$person_id[survey_df$question == \"T1DM\" & survey_df$answer == \"Are you currently prescribed medications and/or receiving treatment for type 1 diabetes? - Yes\"]\n",
    "t2dm_ids <- survey_df$person_id[survey_df$question == \"T2DM\" & survey_df$answer == \"Are you currently prescribed medications and/or receiving treatment for type 2 diabetes? - Yes\"]\n",
    "metabolic_ids <- unique(c(htn_ids, chol_ids, t1dm_ids, t2dm_ids))\n",
    "hormone_ids <- survey_df$person_id[survey_df$question == \"Hormone\" & survey_df$answer == \"Are you currently prescribed medications and/or receiving treatment for other hormone/endocrine condition(s)? - Yes\"]\n",
    "\n",
    "df_covariates$Medication <- 0\n",
    "df_covariates$Medication[df_covariates$person_id %in% metabolic_ids] <- 1\n",
    "df_covariates$Medication[df_covariates$person_id %in% hormone_ids] <- 2\n",
    "df_covariates$Medication <- factor(df_covariates$Medication, levels=c(0,1,2), labels= c(\"No Medication\", \"Metabolic\", \"Hormones\"))\n",
    "\n",
    "table(df_covariates$Medication)\n",
    "\n",
    "# Family diabetes\n",
    "t1dm_ids <- survey_df$person_id[survey_df$question == \"T1DM Fam\" &\n",
    "                                (survey_df$answer == \"Including yourself, who in your family has had type 1 diabetes? - Father\" | \n",
    "                                 survey_df$answer == \"Including yourself, who in your family has had type 1 diabetes? - Mother\" |\n",
    "                                 survey_df$answer == \"Including yourself, who in your family has had type 1 diabetes? - Sibling\")]\n",
    "t2dm_ids <- survey_df$person_id[survey_df$question == \"T2DM Fam\" &\n",
    "                                (survey_df$answer == \"Including yourself, who in your family has had type 2 diabetes? - Father\" | \n",
    "                                 survey_df$answer == \"Including yourself, who in your family has had type 2 diabetes? - Mother\" |\n",
    "                                 survey_df$answer == \"Including yourself, who in your family has had type 2 diabetes? - Sibling\")]\n",
    "dm_ids <- unique(c(t1dm_ids, t2dm_ids))\n",
    "\n",
    "df_covariates$Family_diabetes <- 0\n",
    "df_covariates$Family_diabetes[df_covariates$person_id %in% dm_ids] <- 1\n",
    "df_covariates$Family_diabetes <- as.factor(df_covariates$Family_diabetes)\n",
    "\n",
    "table(df_covariates$Family_diabetes)\n",
    "\n",
    "dim(df_covariates)\n",
    "head(df_covariates)\n",
    "\n",
    "write.table(df_covariates, file.path(data_path, \"dataframes/df_covariates_1.7.txt\"), sep=\"\\t\", quote=F, row.names=F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df_covariates <- data.table::fread(file.path(data_path, \"dataframes/df_covariates_1.7.txt\"), sep=\"\\t\")\n",
    "df_covariates <- df_covariates %>% select(-(c(\"AGE_cat\")))\n",
    "\n",
    "\n",
    "\n",
    "#write.table(df_covariates, \"data/df_covariates_1.7.txt\", sep=\"\\t\", quote=F, row.names=F) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICD codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EHR conditions SQL query (ONLY essential columns to reduce memory)\n",
    "dataset_ehr_conditions_sql <- paste(\"\n",
    "    SELECT\n",
    "        condition_occurrence.person_id,\n",
    "        c_source_concept.concept_code as source_concept_code,\n",
    "        condition_occurrence.condition_start_datetime,\n",
    "        c_source_concept.concept_name as source_concept_name\n",
    "    FROM\n",
    "        `condition_occurrence` condition_occurrence\n",
    "    LEFT JOIN\n",
    "        `concept` c_source_concept \n",
    "            ON condition_occurrence.condition_source_concept_id = c_source_concept.concept_id\n",
    "    WHERE\n",
    "        condition_occurrence.condition_start_datetime IS NOT NULL\n",
    "        AND condition_occurrence.person_id IS NOT NULL\n",
    "        AND c_source_concept.concept_code IS NOT NULL\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for EHR conditions data\n",
    "ehr_conditions_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),\n",
    "  \"ehr_conditions_comprehensive\",\n",
    "  \"ehr_conditions_comprehensive_*.csv\")\n",
    "\n",
    "message(str_glue('EHR conditions data will be written to {ehr_conditions_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_ehr_conditions_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  ehr_conditions_path,\n",
    "  destination_format = \"CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# READ DATA FROM CLOUD STORAGE\n",
    "# =============================================================================\n",
    "\n",
    "read_ehr_conditions_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(\n",
    "    person_id = col_character(),\n",
    "    source_concept_code = col_character(),\n",
    "    condition_start_datetime = col_character(),\n",
    "    source_concept_name = col_character()\n",
    "  )\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "\n",
    "\n",
    "# Read EHR conditions data from Cloud Storage  \n",
    "message(\"Reading comprehensive EHR conditions data from Cloud Storage...\")\n",
    "df_diagnosis_raw <- read_bq_export_from_workspace_bucket(ehr_conditions_path)\n",
    "\n",
    "message(str_glue(\"EHR conditions data loaded: {nrow(df_diagnosis_raw)} rows, {ncol(df_diagnosis_raw)} columns\"))\n",
    "head(df_diagnosis_raw, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process EHR conditions data  \n",
    "message(\"Processing EHR conditions data...\")\n",
    "df_diagnosis <- df_diagnosis_raw %>%\n",
    "    select(person_id, source_concept_code, condition_start_datetime, source_concept_name) %>%\n",
    "    filter(!is.na(source_concept_code), !is.na(condition_start_datetime))\n",
    "\n",
    "# Clean ICD codes (following your existing pattern)\n",
    "df_diagnosis$source_concept_code <- gsub(\"\\\\.\", \"\", df_diagnosis$source_concept_code)\n",
    "df_diagnosis$source_concept_code <- substr(df_diagnosis$source_concept_code, 1, 4)\n",
    "\n",
    "message(str_glue(\"Processed diagnosis data: {nrow(df_diagnosis)} conditions\"))\n",
    "\n",
    "# =============================================================================\n",
    "# DATA SUMMARIES\n",
    "# =============================================================================\n",
    "\n",
    "# EHR conditions summary\n",
    "message(\"=== EHR CONDITIONS SUMMARY ===\")  \n",
    "message(str_glue(\"Total conditions: {nrow(df_diagnosis)}\"))\n",
    "message(str_glue(\"Unique persons: {n_distinct(df_diagnosis$person_id)}\"))\n",
    "\n",
    "if(nrow(df_diagnosis) > 0) {\n",
    "    message(str_glue(\"Date range: {min(as.Date(df_diagnosis$condition_start_datetime), na.rm=TRUE)} to {max(as.Date(df_diagnosis$condition_start_datetime), na.rm=TRUE)}\"))\n",
    "    \n",
    "    # Top condition codes\n",
    "    condition_counts <- df_diagnosis %>% \n",
    "        count(source_concept_code, source_concept_name, sort = TRUE) %>% \n",
    "        head(20)\n",
    "    message(\"Top 20 condition codes:\")\n",
    "    print(condition_counts)\n",
    "    \n",
    "    # ICD chapter distribution\n",
    "    icd_distribution <- df_diagnosis %>%\n",
    "        mutate(icd_chapter = str_sub(source_concept_code, 1, 1)) %>%\n",
    "        count(icd_chapter, sort = TRUE)\n",
    "    message(\"ICD code chapter distribution:\")\n",
    "    print(icd_distribution)\n",
    "}\n",
    "\n",
    "\n",
    "# Save processed diagnosis data (you can inspect before final storage)  \n",
    "message(\"Saving processed diagnosis data...\")\n",
    "write.table(df_diagnosis, \"data/df_diagnosis_processed.txt\", sep=\"\\t\", quote=FALSE, row.names=FALSE)\n",
    "\n",
    "message(\"=== EXTRACTION AND PROCESSING COMPLETE ===\")\n",
    "message(\"Files created for your inspection:\")\n",
    "message(\"  - data/df_blood_processed.txt\")\n",
    "message(\"  - data/df_diagnosis_processed.txt\")\n",
    "message(\"\")\n",
    "message(\"After inspection, you can rename these to:\")\n",
    "message(\"  - data/df_blood.txt\") \n",
    "message(\"  - data/df_diagnosis.txt\")\n",
    "message(\"To integrate with your existing processing pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "icd_codes_subset <- data.table::fread(\"data/df_ICD_codes_subset.txt\", sep=\"\\t\")\n",
    "\n",
    "dim(icd_codes_subset)\n",
    "head(icd_codes_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "icd_codes_subset$source_concept_code <- gsub(\"\\\\.\", \"\", icd_codes_subset$source_concept_code)\n",
    "icd_codes_subset$source_concept_code <- substr(icd_codes_subset$source_concept_code, 1, 4)\n",
    "\n",
    "head(icd_codes_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ICD Groups\n",
    "ICD_Groups <- data.table::fread(\"data/ICD_Groups.txt\", sep=\"\\t\")\n",
    "\n",
    "pat_icds_labelled <- left_join(x = ICD_Groups, y = icd_codes_subset %>% select(person_id, source_concept_code) %>% distinct(), \n",
    "                               by = c(\"ICD10\" = \"source_concept_code\")) %>% \n",
    "                     distinct()\n",
    "\n",
    "dim(pat_icds_labelled)\n",
    "#head(pat_icds_labelled)\n",
    "\n",
    "#Check for amount of diagnosis per actual ICD, instead of group. Likely here will be the correct amounts, while summarized will be less due to parallel coded diagnosis\n",
    "df_icd_groups_before <- pat_icds_labelled %>% group_by(Diagnosis, person_id) %>% summarise(occurence = n()) %>% spread(Diagnosis, occurence) \n",
    "sum_groups_before <- as.data.frame(colSums(df_icd_groups_before, na.rm=TRUE))\n",
    "\n",
    "head(sum_groups_before)\n",
    "\n",
    "#Groupby + summarises\n",
    "df_icd_groups <- pat_icds_labelled %>% group_by(Group, person_id) %>% summarise(occurence = n()) %>% spread(Group, occurence) \n",
    "df_icd_groups[df_icd_groups > 1 & df_icd_groups < 100]<- 1 \n",
    "df_icd_groups[is.na(df_icd_groups)] <- 0\n",
    "sum_groups_after <- as.data.frame(colSums(df_icd_groups))\n",
    "\n",
    "head(sum_groups_before)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ICD Singles\n",
    "ICD_Singles <- data.table::fread(\"data/ICD_Singles.txt\", sep=\"\\t\")\n",
    "\n",
    "pat_icds_labelled <- left_join(x = ICD_Singles, y = icd_codes_subset %>% select(person_id, source_concept_code) %>% distinct(), \n",
    "                               by = c(\"ICD10\" = \"source_concept_code\"))%>% \n",
    "                     distinct() \n",
    "\n",
    "dim(pat_icds_labelled)\n",
    "\n",
    "#Groupby + summarises\n",
    "df_icd_singles <- pat_icds_labelled %>% group_by(Diagnosis, person_id) %>% summarise(occurence = n()) %>% spread(Diagnosis, occurence) \n",
    "df_icd_singles[df_icd_singles > 1 & df_icd_singles < 10000]<- 1 \n",
    "df_icd_singles[is.na(df_icd_singles)] <- 0\n",
    "sum_singles <- as.data.frame(colSums(df_icd_singles))\n",
    "\n",
    "head(sum_singles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# DM ICD codes\n",
    "icd_dm_codes <- data.table::fread(\"data/df_ICD_codes_DM.txt\", sep=\"\\t\")\n",
    "\n",
    "icd_dm <- icd_dm_codes %>%\n",
    "    select(person_id) %>%\n",
    "    distinct()\n",
    "icd_dm$DM <- 1\n",
    "icd_dm <- select(icd_dm, c(person_id, DM))\n",
    "\n",
    "dim(icd_dm)\n",
    "head(icd_dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Merging groups and singles\n",
    "df_icd <- full_join(df_icd_singles, df_icd_groups, by = \"person_id\")\n",
    "df_icd <- merge(df_icd, icd_dm, by=\"person_id\", all=TRUE)\n",
    "df_icd <- replace(df_icd, is.na(df_icd), 0)\n",
    "\n",
    "dim(df_icd)\n",
    "head(df_icd)\n",
    "\n",
    "### Note:\n",
    "### - person_id '0' represents all codes that had NO patients diagnosed\n",
    "\n",
    "\n",
    "### Include 'control' patients - patients with no diagnoses\n",
    "icd_person_ids <- read.table(\"data/AllofUs_v7_phenotype_icd_091223_person_ids.txt\", sep=\"\\t\", header=T)\n",
    "\n",
    "df_icd <- merge(df_icd, icd_person_ids, by=\"person_id\", all=TRUE)\n",
    "df_icd[is.na(df_icd)] <- 0\n",
    "\n",
    "dim(df_icd)\n",
    "head(df_icd)\n",
    "\n",
    "write.table(df_icd, \"data/df_diagnosis.txt\", sep=\"\\t\", quote=F, row.names=F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Summarize ICD codes\n",
    "create_summary <- function(df) {\n",
    "  total_rows <- nrow(df)\n",
    "  summary_df <- df %>% \n",
    "    summarise(across(-person_id, ~sum(. == \"1\", na.rm = TRUE))) %>% \n",
    "    pivot_longer(everything(), names_to = \"Diagnosis\", values_to = \"Occurrence\") %>% \n",
    "    mutate(Percentage = (Occurrence / total_rows) * 100) %>%\n",
    "    arrange(desc(Occurrence))\n",
    "  \n",
    "  as.data.frame(summary_df)\n",
    "}\n",
    "\n",
    "df_icd <- data.table::fread(\"data/df_diagnosis.txt\", sep=\"\\t\")\n",
    "\n",
    "sum_diagnosis <- create_summary(df_icd)\n",
    "sum_diagnosis\n",
    "sum_diagnosis_sorted <- sum_diagnosis[order(sum_diagnosis$Diagnosis), ]\n",
    "sum_diagnosis_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract DOI cases/controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "icd_codes_subset <- data.table::fread(\"data/HCC_ICD_codes_subset.txt\", sep=\"\\t\")\n",
    "\n",
    "dim(icd_codes_subset)\n",
    "head(icd_codes_subset)\n",
    "\n",
    "icd_codes_subset$source_concept_code <- gsub(\"\\\\.\", \"\", icd_codes_subset$source_concept_code)\n",
    "icd_codes_subset$source_concept_code <- substr(icd_codes_subset$source_concept_code, 1, 4)\n",
    "\n",
    "head(icd_codes_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process DOI cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "##### DOI Filters from project configs #####\n",
    "print(IOIs)\n",
    "\n",
    "# Build regex pattern: ^ ensures \"starts with\"\n",
    "pattern <- paste0(\"^\", IOIs, collapse = \"|\")\n",
    "\n",
    "# Apply filter\n",
    "cases <- icd_codes_subset[grepl(pattern, icd_codes_subset$source_concept_code), ]\n",
    "\n",
    "\n",
    "cases <- icd_codes_subset[icd_codes_subset$source_concept_code == \"C220\",]\n",
    "cases <- cases %>%\n",
    "  mutate(condition_start_datetime = ymd_hms(condition_start_datetime))\n",
    "\n",
    "# Filter for the first visit per unique person_id\n",
    "cases_first_visit <- cases %>%\n",
    "  group_by(person_id) %>%\n",
    "  arrange(condition_start_datetime) %>%\n",
    "  slice(1) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(year_of_diag = sapply(condition_start_datetime, extract_year))\n",
    "\n",
    "# Check the result\n",
    "print(paste(\"Original number of rows:\", nrow(cases)))\n",
    "print(paste(\"Number of rows after filtering:\", nrow(cases_first_visit)))\n",
    "print(paste(\"Number of unique person_ids:\", n_distinct(cases_first_visit$person_id)))\n",
    "\n",
    "#Prepare a df_y that includes the \n",
    "df_y <- cases_first_visit %>% select(c(\"person_id\", \"year_of_diag\")) %>% mutate(status = 1) %>% rename(eid = person_id)\n",
    "       \n",
    "df_y\n",
    "\n",
    "write.csv(df_y, \"data/df_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot DOI Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "if (!requireNamespace(\"systemfonts\", quietly = TRUE)) install.packages(\"systemfonts\")\n",
    "if (!requireNamespace(\"showtext\", quietly = TRUE)) install.packages(\"showtext\")\n",
    "\n",
    "n_total <- nrow(df_y) #Assess absolute number\n",
    "\n",
    "plot_included_discarded_cases(cases_first_visit, base_size=30, n_total = n_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HCC Only: Explore diseases underlying DOI cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank priority of diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "priority_order <- c(\"Cirrhosis\", \"Viral Hepatitis\", \"CLD\", \"No Liver disease\")\n",
    "\n",
    "pat_cld <- pat_icds[pat_icds$diag_icd10 %in% par_icd_codes | pat_icds$diag_icd9 %in% par_icd_codes, ] %>%\n",
    "  select(c(\"eid\", \"diag_icd9\", \"diag_icd10\", \"epistart\")) %>%\n",
    "  left_join(Patients_at_risk, by = c(\"diag_icd10\" = \"ICD10\")) %>%\n",
    "  right_join(df_y, by = \"eid\") %>%\n",
    "  subset(status==1) %>%\n",
    "  select(-c(\"location_name\", \"location_code\", \"location_nr\", \"location_country\", \"country_code\", \"split_ext\", \"split_int\"))\n",
    "\n",
    "pat_cld$epistart[is.na(pat_cld$Group)] <- as.Date(pat_cld$date_of_diag)\n",
    "\n",
    "pat_cld$Group[is.na(pat_cld$Group)] <- \"No Liver disease\"\n",
    "pat_cld$Group[!pat_cld$Group %in% par_subset] <- \"No Liver disease\" #Replace all non-matching groups with \"No LD\"\n",
    "pat_cld$Group <- factor(pat_cld$Group, levels=priority_order)\n",
    "\n",
    "summary(pat_cld$Group)\n",
    "\n",
    "priority <- function(diagnosis) {\n",
    "  case_when(\n",
    "    diagnosis == \"Cirrhosis\" ~ 1,\n",
    "    diagnosis == \"Viral Hepatitis\" ~ 2,\n",
    "    diagnosis == \"CLD\" ~ 3,\n",
    "    diagnosis == \"No Liver disease\" ~ 4,\n",
    "    TRUE ~ 5  # Assign a lower priority to other diagnoses\n",
    "  )\n",
    "}\n",
    "\n",
    "\n",
    "pat_cld <- pat_cld %>%\n",
    "  group_by(eid)\n",
    "\n",
    "# Node 0 represents first visit to hospital after assessment\n",
    "pat_cld_node0 <- pat_cld %>%\n",
    "  mutate(Priority = priority(Group)) %>%\n",
    "  group_by(eid) %>%\n",
    "  filter(epistart == min(epistart)) %>%\n",
    "  arrange(eid, Priority) %>%\n",
    "  filter(row_number() == 1) %>%\n",
    "  ungroup() %>%\n",
    "  select(-Priority)\n",
    "\n",
    "summary_node0 <- pat_cld_node0 %>%\n",
    "  group_by(Group) %>%\n",
    "  summarize(Count = n(), .groups = 'drop') %>%\n",
    "  mutate(Time = \"First \\nEHR\") %>%\n",
    "  mutate(Order = 1 ) %>%\n",
    "  mutate(Priority = priority(Group)) %>%\n",
    "  arrange(Priority) %>%\n",
    "  mutate(Percentage = round(Count / sum(Count) * 100)) \n",
    "\n",
    "\n",
    "\n",
    "pat_cld_node1 <- pat_cld %>%\n",
    "  mutate(Priority = priority(Group)) %>%\n",
    "  group_by(eid) %>%\n",
    "  #filter(epistart == max(epistart)) %>%   #better to take all incidents than just the last, as not all diags get coded everytime\n",
    "  arrange(eid, Priority) %>%\n",
    "  filter(row_number() == 1) %>%\n",
    "  ungroup() %>%\n",
    "  select(-Priority)\n",
    "\n",
    "summary_node1 <- pat_cld_node1 %>%\n",
    "  group_by(Group) %>%\n",
    "  summarize(Count = n(), .groups = 'drop') %>%\n",
    "  mutate(Time = paste0(\"Prior to\\n\", DOI)) %>%\n",
    "  mutate(Order = 2 ) %>%\n",
    "  mutate(Priority = priority(Group)) %>%\n",
    "  arrange(Priority) %>%\n",
    "  mutate(Percentage = round(Count / sum(Count) * 100))\n",
    "\n",
    "\n",
    "# View the summaries\n",
    "print(summary_node0)\n",
    "print(summary_node1)\n",
    "\n",
    "\n",
    "# Merge timepoints\n",
    "combined_data <- rbind(summary_node0, summary_node1)\n",
    "\n",
    "\n",
    "stacked_bars_time_comparison(combined_data, base_size=22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Cirrhosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "check_unique_participants <- function(df, name) {\n",
    "  total_rows <- nrow(df)\n",
    "  unique_participants <- n_distinct(df$person_id)\n",
    "  print(paste(\"Checking\", name))\n",
    "  print(paste(\"Total rows:\", total_rows))\n",
    "  print(paste(\"Unique participants:\", unique_participants))\n",
    "}\n",
    "\n",
    "\n",
    "# Check cirrhosis_cases and HCC_cases\n",
    "check_unique_participants(cirrhosis_cases, \"Cirrhosis Cases\")\n",
    "check_unique_participants(HCC_cases, \"HCC Cases\")\n",
    "\n",
    "\n",
    "process_icd_codes <- function(icd_codes_subset, codes, disease_name, select_visits = \"first\") {\n",
    "  # Filter for the specified ICD codes\n",
    "  cases <- icd_codes_subset[icd_codes_subset$source_concept_code %in% codes, ]\n",
    "  \n",
    "  # Convert condition_start_datetime to datetime format\n",
    "  cases <- cases %>%\n",
    "    mutate(condition_start_datetime = ymd_hms(condition_start_datetime))\n",
    "  \n",
    "  # Function to extract year from datetime\n",
    "  extract_year <- function(date) {\n",
    "    return(year(date))\n",
    "  }\n",
    "  \n",
    "  # Process based on select_visits option\n",
    "  if (select_visits == \"first\") {\n",
    "    # Filter for the first visit per unique person_id\n",
    "    processed_cases <- cases %>%\n",
    "      group_by(person_id) %>%\n",
    "      arrange(condition_start_datetime) %>%\n",
    "      slice(1) %>%\n",
    "      ungroup()\n",
    "  } else if (select_visits == \"all\") {\n",
    "    # Keep all visits\n",
    "    processed_cases <- cases\n",
    "  } else {\n",
    "    stop(\"Invalid select_visits option. Use 'first' or 'all'.\")\n",
    "  }\n",
    "  \n",
    "  # Add year and date_of_diag columns\n",
    "  processed_cases <- processed_cases %>%\n",
    "    mutate(\n",
    "      year = sapply(condition_start_datetime, extract_year),\n",
    "      date_of_diag = as.Date(condition_start_datetime)\n",
    "    )\n",
    "  \n",
    "  # Add disease name column\n",
    "  processed_cases$disease <- disease_name\n",
    "  \n",
    "  # Print summary information\n",
    "  print(paste(\"Processing\", disease_name, \"cases:\"))\n",
    "  print(paste(\"ICD codes used:\", paste(codes, collapse = \", \")))\n",
    "  print(paste(\"Original number of rows:\", nrow(cases)))\n",
    "  print(paste(\"Number of rows after processing:\", nrow(processed_cases)))\n",
    "  print(paste(\"Number of unique person_ids:\", n_distinct(processed_cases$person_id)))\n",
    "  \n",
    "  # Optional: Count of cases per ICD code\n",
    "  if (length(codes) > 1) {\n",
    "    code_counts <- processed_cases %>%\n",
    "      group_by(source_concept_code) %>%\n",
    "      summarise(count = n()) %>%\n",
    "      arrange(desc(count))\n",
    "    print(\"Cases per ICD code:\")\n",
    "    print(code_counts)\n",
    "  }\n",
    "    \n",
    "  processed_cases <- processed_cases %>% select(c(\"person_id\", \"source_concept_code\", \"date_of_diag\", \"disease\"))\n",
    "  return(processed_cases)\n",
    "}\n",
    "\n",
    "cirrhosis_codes <- c(\"K703\", \"K743\", \"K745\", \"K746\", \"K767\", \"I850\", \"I859\", \"R18\")\n",
    "\n",
    "\n",
    "cirrhosis_cases <- process_icd_codes(icd_codes_subset, cirrhosis_codes, \"Cirrhosis\", \"first\")\n",
    "\n",
    "HCC_cases <- process_icd_codes(icd_codes_subset, \"C220\", DOI, \"first\")\n",
    "\n",
    "\n",
    "check_unique_participants(cirrhosis_cases, \"Cirrhosis Cases\")\n",
    "check_unique_participants(HCC_cases, paste0(\"Cases with\", DOI))\n",
    "\n",
    "\n",
    "head(cirrhosis_cases)\n",
    "head(HCC_cases)\n",
    "dim(cirrhosis_cases)\n",
    "dim(HCC_cases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "time_threshold = 90\n",
    "print(time_threshold)\n",
    "\n",
    "cirrhosis_cases <- cirrhosis_cases %>%\n",
    "    mutate(date_of_diag = as.Date(date_of_diag))\n",
    "  \n",
    "HCC_cases <- HCC_cases %>%\n",
    "    mutate(date_of_diag = as.Date(date_of_diag))\n",
    "\n",
    "early_cirrhosis_cases <- merge(cirrhosis_cases, HCC_cases, by=\"person_id\", suffix = c(\"_cirrhosis\", \"_HCC\"), all=TRUE)\n",
    "\n",
    "sum(is.na(early_cirrhosis_cases$date_of_diag_HCC))\n",
    "\n",
    "early_cirrhosis_cases <- early_cirrhosis_cases %>%\n",
    "  mutate(\n",
    "    time_to_hcc = case_when(\n",
    "      !is.na(date_of_diag_HCC) & !is.na(date_of_diag_cirrhosis) ~ \n",
    "        as.numeric(difftime(date_of_diag_HCC, date_of_diag_cirrhosis, units = \"days\")),\n",
    "      TRUE ~ NA_real_\n",
    "    ),\n",
    "    cirrhosis_status = case_when(\n",
    "      is.na(date_of_diag_cirrhosis) & is.na(date_of_diag_HCC) ~ \"Neither Cirrhosis nor HCC\",\n",
    "      is.na(date_of_diag_HCC) ~ \"Cirrhosis but No HCC\",\n",
    "      is.na(date_of_diag_cirrhosis) ~ \"HCC but No Cirrhosis\",\n",
    "      time_to_hcc > time_threshold ~ \"Cirrhosis prior to HCC\",\n",
    "      time_to_hcc >= -time_threshold & time_to_hcc <= time_threshold ~ \"Simultaneous Cirrhosis + HCC\",\n",
    "      time_to_hcc < -time_threshold ~ \"Cirrhosis after HCC\",\n",
    "      TRUE ~ \"Error in date calculation\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "\n",
    "early_cirrhosis_cases\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "case_analysis <- early_cirrhosis_cases %>%\n",
    "    group_by(cirrhosis_status) %>%\n",
    "    summarize(count = n()) %>%\n",
    "    mutate(percentage = count / sum(count) * 100)\n",
    "  \n",
    "  print(\"Cirrhosis cases analysis:\")\n",
    "  print(case_analysis)\n",
    "\n",
    "early_cirrhosis_only <- early_cirrhosis_cases %>%\n",
    "  filter(cirrhosis_status %in% c(\"Cirrhosis but No HCC\", \"Cirrhosis prior to HCC\"))\n",
    "\n",
    "df_early_cirrhosis <- early_cirrhosis_only %>%\n",
    "  select(person_id) %>%\n",
    "  mutate(cirrhosis = 1) %>%\n",
    "  distinct()  \n",
    "\n",
    "write_csv(df_early_cirrhosis, \"data/df_early_cirrhosis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "early_cirrhosis_only\n",
    "\n",
    "df_cirrhosis <- early_cirrhosis_only %>% select(\"person_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood: Not functional yet, use \"Extract quantitative phenotypes\"\n",
    "\n",
    "https://support.researchallofus.org/hc/en-us/articles/30125602539284-Introduction-to-All-of-Us-Electronic-Health-Record-EHR-Collection-and-Data-Transformation-Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract (all blood data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_blood_labs_sql <- paste(\"\n",
    "    SELECT\n",
    "        measurement.person_id,\n",
    "        m_standard_concept.concept_name as standard_concept_name,\n",
    "        measurement.value_as_number,\n",
    "        measurement.measurement_datetime\n",
    "    FROM\n",
    "        `measurement` measurement \n",
    "    LEFT JOIN\n",
    "        `concept` m_standard_concept \n",
    "            ON measurement.measurement_concept_id = m_standard_concept.concept_id \n",
    "    WHERE\n",
    "        measurement.value_as_number IS NOT NULL\n",
    "        AND measurement.measurement_datetime IS NOT NULL\n",
    "        AND measurement.person_id IS NOT NULL\n",
    "        AND m_standard_concept.concept_name IS NOT NULL\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for blood labs data\n",
    "blood_labs_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),\n",
    "  \"blood_labs_comprehensive\",\n",
    "  \"blood_labs_comprehensive_*.csv\")\n",
    "\n",
    "message(str_glue('Blood labs data will be written to {blood_labs_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_blood_labs_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  blood_labs_path,\n",
    "  destination_format = \"CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from GCS\n",
    "\n",
    "#TODO: Runs into timeouts as memory is overloaded (330MB * 86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_blood_labs_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(\n",
    "    person_id = col_character(),\n",
    "    standard_concept_name = col_character(), \n",
    "    value_as_number = col_double(),\n",
    "    measurement_datetime = col_character()\n",
    "  )\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "\n",
    "blood_labs_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),\n",
    "  \"blood_labs_comprehensive\",\n",
    "  \"blood_labs_comprehensive_*.csv\")\n",
    "\n",
    "\n",
    "# Read blood labs data from Cloud Storage\n",
    "message(\"Reading comprehensive blood labs data from Cloud Storage...\")\n",
    "df_blood_raw <- read_blood_labs_from_workspace_bucket(blood_labs_path)\n",
    "\n",
    "message(str_glue(\"Blood labs data loaded: {nrow(df_blood_raw)} rows, {ncol(df_blood_raw)} columns\"))\n",
    "head(df_blood_raw, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process/merge on person_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process blood labs in batches\n",
    "process_blood_labs_in_batches <- function(gcs_path, \n",
    "                                         output_file = \"data/df_blood_batched.txt\",\n",
    "                                         batch_size = 5,  # Process 5 files at a time\n",
    "                                         filters = NULL) {\n",
    "  \n",
    "  # Get list of all files\n",
    "  file_list <- system2('gsutil', args = c('ls', gcs_path), stdout = TRUE, stderr = TRUE)\n",
    "  n_files <- length(file_list)\n",
    "  n_batches <- ceiling(n_files / batch_size)\n",
    "  \n",
    "  message(str_glue('Found {n_files} files. Processing in {n_batches} batches of {batch_size} files each.'))\n",
    "  \n",
    "  # Initialize output file (remove if exists)\n",
    "  if(file.exists(output_file)) file.remove(output_file)\n",
    "  \n",
    "  # Process each batch\n",
    "  for(batch in 1:n_batches) {\n",
    "    start_idx <- (batch - 1) * batch_size + 1\n",
    "    end_idx <- min(batch * batch_size, n_files)\n",
    "    batch_files <- file_list[start_idx:end_idx]\n",
    "    \n",
    "    message(str_glue('Processing batch {batch}/{n_batches}: files {start_idx} to {end_idx}'))\n",
    "    \n",
    "    # Process this batch\n",
    "    batch_data <- map_dfr(batch_files, function(csv_file) {\n",
    "      tryCatch({\n",
    "        message(str_glue('  Loading {basename(csv_file)}'))\n",
    "        \n",
    "        # Read with data.table for speed\n",
    "        dt <- fread(\n",
    "          pipe(str_glue('gsutil cat {csv_file}')),\n",
    "          select = c(\"person_id\", \"standard_concept_name\", \"value_as_number\", \"measurement_datetime\"),\n",
    "          colClasses = list(character = c(\"person_id\", \"standard_concept_name\", \"measurement_datetime\"),\n",
    "                           numeric = \"value_as_number\")\n",
    "        )\n",
    "        \n",
    "        # Apply filters if provided\n",
    "        if(!is.null(filters)) {\n",
    "          # Example filters - customize as needed\n",
    "          dt <- dt[!is.na(value_as_number) & value_as_number > 0]\n",
    "          \n",
    "          # Filter by date if specified\n",
    "          if(\"date_filter\" %in% names(filters)) {\n",
    "            dt[, measurement_date := as.Date(measurement_datetime)]\n",
    "            dt <- dt[measurement_date >= as.Date(filters$date_filter)]\n",
    "          }\n",
    "          \n",
    "          # Filter by lab types if specified\n",
    "          if(\"lab_types\" %in% names(filters)) {\n",
    "            dt <- dt[standard_concept_name %in% filters$lab_types]\n",
    "          }\n",
    "        }\n",
    "        \n",
    "        return(as_tibble(dt))\n",
    "        \n",
    "      }, error = function(e) {\n",
    "        message(str_glue('  ERROR loading {basename(csv_file)}: {e$message}'))\n",
    "        return(tibble())\n",
    "      })\n",
    "    })\n",
    "    \n",
    "    # Save this batch to file (append mode)\n",
    "    if(nrow(batch_data) > 0) {\n",
    "      fwrite(batch_data, output_file, append = (batch > 1), sep = \"\\t\")\n",
    "      message(str_glue('  Batch {batch} processed: {nrow(batch_data)} rows'))\n",
    "    }\n",
    "    \n",
    "    # Memory cleanup\n",
    "    rm(batch_data)\n",
    "    gc()\n",
    "    \n",
    "    # Optional: pause between batches to let system recover\n",
    "    if(batch < n_batches) Sys.sleep(2)\n",
    "  }\n",
    "  \n",
    "  message(str_glue('All batches processed. Final data saved to: {output_file}'))\n",
    "  \n",
    "  # Read and return final result\n",
    "  return(fread(output_file, sep = \"\\t\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "# Example 1: Process with basic filters\n",
    "blood_labs_path <- \"gs://your-bucket/bq_exports/.../blood_labs_comprehensive/blood_labs_comprehensive_*.csv\"\n",
    "\n",
    "# Define filters to reduce data size\n",
    "my_filters <- list(\n",
    "  date_filter = \"2015-01-01\",  # Only data from 2015 onwards\n",
    "  lab_types = c(\n",
    "    \"Alanine aminotransferase\",\n",
    "    \"Aspartate aminotransferase\", \n",
    "    \"Platelet count\",\n",
    "    \"Glucose\",\n",
    "    \"Albumin\",\n",
    "    \"Hemoglobin\",\n",
    "    \"Hematocrit\"\n",
    "  )\n",
    ")\n",
    "\n",
    "# Process in small batches\n",
    "df_blood <- process_blood_labs_in_batches(\n",
    "  gcs_path = blood_labs_path,\n",
    "  output_file = \"data/df_blood_batched.txt\",\n",
    "  batch_size = 3,  # Very small batches for safety\n",
    "  filters = my_filters\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# MONITOR MEMORY USAGE\n",
    "# =============================================================================\n",
    "\n",
    "# Function to monitor memory during processing\n",
    "monitor_memory <- function() {\n",
    "  mem_info <- gc()\n",
    "  used_mb <- sum(mem_info[, 2]) * 8 / 1024  # Convert to MB\n",
    "  message(str_glue('Memory used: {round(used_mb)} MB'))\n",
    "  return(used_mb)\n",
    "}\n",
    "\n",
    "# Call this periodically during processing\n",
    "monitor_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blood Concept Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# APPROACH 1: EXPLORE AVAILABLE LAB CONCEPTS\n",
    "# =============================================================================\n",
    "\n",
    "# First, run the lab concepts explorer query (ALL CONCEPTS - no filtering)\n",
    "lab_concepts_explorer_sql <- paste(\"\n",
    "SELECT \n",
    "    m_standard_concept.concept_id,\n",
    "    m_standard_concept.concept_name as standard_concept_name,\n",
    "    m_standard_concept.concept_code as standard_concept_code,\n",
    "    m_standard_concept.vocabulary_id as vocabulary,\n",
    "    COUNT(*) as measurement_count,\n",
    "    COUNT(DISTINCT measurement.person_id) as unique_persons,\n",
    "    MIN(measurement.measurement_datetime) as earliest_date,\n",
    "    MAX(measurement.measurement_datetime) as latest_date,\n",
    "    APPROX_QUANTILES(measurement.value_as_number, 4)[OFFSET(2)] as median_value,\n",
    "    MIN(measurement.value_as_number) as min_value,\n",
    "    MAX(measurement.value_as_number) as max_value\n",
    "FROM \n",
    "    `\", Sys.getenv(\"WORKSPACE_CDR\"), \".measurement` measurement\n",
    "LEFT JOIN\n",
    "    `\", Sys.getenv(\"WORKSPACE_CDR\"), \".concept` m_standard_concept \n",
    "        ON measurement.measurement_concept_id = m_standard_concept.concept_id \n",
    "WHERE\n",
    "    measurement.value_as_number IS NOT NULL\n",
    "    AND measurement.measurement_datetime IS NOT NULL\n",
    "    AND m_standard_concept.concept_name IS NOT NULL\n",
    "GROUP BY \n",
    "    m_standard_concept.concept_id,\n",
    "    m_standard_concept.concept_name,\n",
    "    m_standard_concept.concept_code,\n",
    "    m_standard_concept.vocabulary_id\n",
    "HAVING \n",
    "    COUNT(*) >= 100\n",
    "ORDER BY \n",
    "    measurement_count DESC\", sep=\"\")\n",
    "\n",
    "# Run the explorer query\n",
    "message(\"Running lab concepts explorer query...\")\n",
    "lab_concepts_df <- bq_table_download(bq_project_query(\n",
    "    Sys.getenv(\"GOOGLE_PROJECT\"), \n",
    "    query = str_glue(lab_concepts_explorer_sql)\n",
    "))\n",
    "\n",
    "# Save the lab concepts for inspection\n",
    "write_xlsx(lab_concepts_df, \"data/aou_lab_concepts_available.xlsx\")\n",
    "message(\"Available lab concepts saved to: data/aou_lab_concepts_available.xlsx\")\n",
    "\n",
    "# Display top 20 for quick inspection\n",
    "message(\"Top 20 most common lab measurements:\")\n",
    "print(head(lab_concepts_df, 20))\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION TO MATCH UKB BLOOD VALUES\n",
    "# =============================================================================\n",
    "\n",
    "match_ukb_blood_values <- function(ukb_file_path, aou_concepts_df) {\n",
    "  # Load your UKB blood values file\n",
    "  if(file.exists(ukb_file_path)) {\n",
    "    ukb_blood <- read_excel(ukb_file_path)  # Adjust based on your file format\n",
    "    \n",
    "    # Perform fuzzy matching between UKB and AOU lab names\n",
    "    # You can customize this matching logic\n",
    "    matched_concepts <- aou_concepts_df %>%\n",
    "      filter(\n",
    "        str_detect(tolower(standard_concept_name), \n",
    "                   paste(tolower(ukb_blood$lab_name), collapse = \"|\"))  # Adjust column name\n",
    "      )\n",
    "    \n",
    "    return(matched_concepts)\n",
    "  } else {\n",
    "    message(\"UKB file not found. Please provide the correct path.\")\n",
    "    return(aou_concepts_df)  # Return all concepts if no UKB file\n",
    "  }\n",
    "}\n",
    "\n",
    "# Example usage (adjust the file path to your UKB blood values file)\n",
    "# matched_labs <- match_ukb_blood_values(\"data/ukb_blood_values.xlsx\", lab_concepts_df)\n",
    "\n",
    "# =============================================================================\n",
    "# APPROACH 2: HARMONIZED EXTRACTION WITH YEARLY MEANS (RECOMMENDED)\n",
    "# =============================================================================\n",
    "\n",
    "create_harmonized_blood_extraction <- function(selected_concept_ids = NULL) {\n",
    "  \n",
    "  # Base query for harmonized extraction\n",
    "  base_harmonized_sql <- \"\n",
    "  SELECT\n",
    "      measurement.person_id,\n",
    "      m_standard_concept.concept_name as standard_concept_name,\n",
    "      EXTRACT(YEAR FROM measurement.measurement_datetime) as year,\n",
    "      CONCAT(CAST(measurement.person_id AS STRING), '_', CAST(EXTRACT(YEAR FROM measurement.measurement_datetime) AS STRING)) as person_id_year,\n",
    "      AVG(measurement.value_as_number) as mean_value,\n",
    "      COUNT(measurement.value_as_number) as measurement_count,\n",
    "      MIN(measurement.measurement_datetime) as first_measurement_date,\n",
    "      MAX(measurement.measurement_datetime) as last_measurement_date\n",
    "  FROM \n",
    "      `measurement` measurement\n",
    "  LEFT JOIN\n",
    "      `concept` m_standard_concept \n",
    "          ON measurement.measurement_concept_id = m_standard_concept.concept_id \n",
    "  WHERE\n",
    "      measurement.value_as_number IS NOT NULL\n",
    "      AND measurement.measurement_datetime IS NOT NULL\n",
    "      AND measurement.person_id IS NOT NULL\n",
    "      AND m_standard_concept.concept_name IS NOT NULL\n",
    "      AND measurement.measurement_datetime >= '2010-01-01'\n",
    "      AND measurement.value_as_number > 0\n",
    "      AND measurement.value_as_number < 1000000\"\n",
    "  \n",
    "  # Add concept filtering if provided\n",
    "  if(!is.null(selected_concept_ids)) {\n",
    "    concept_filter <- paste0(\" AND measurement.measurement_concept_id IN (\", \n",
    "                            paste(selected_concept_ids, collapse = \",\"), \")\")\n",
    "    base_harmonized_sql <- paste0(base_harmonized_sql, concept_filter)\n",
    "  }\n",
    "  \n",
    "  # Add GROUP BY and ORDER BY\n",
    "  base_harmonized_sql <- paste0(base_harmonized_sql, \"\n",
    "  GROUP BY \n",
    "      measurement.person_id,\n",
    "      m_standard_concept.concept_name,\n",
    "      EXTRACT(YEAR FROM measurement.measurement_datetime)\n",
    "  HAVING \n",
    "      COUNT(measurement.value_as_number) >= 1\n",
    "  ORDER BY \n",
    "      measurement.person_id,\n",
    "      standard_concept_name,\n",
    "      year\")\n",
    "  \n",
    "  return(base_harmonized_sql)\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE EXAMPLES FOR ALL THREE APPROACHES\n",
    "# =============================================================================\n",
    "\n",
    "message(\"=== THREE EXTRACTION APPROACHES ===\")\n",
    "message(\"1. Lab Concepts Explorer: Complete ✓\")\n",
    "message(\"2. Harmonized Extraction: Ready to run\")\n",
    "message(\"3. Batch Processing: Ready to run\")\n",
    "\n",
    "message(\"\\nNext steps:\")\n",
    "message(\"1. Inspect: data/aou_lab_concepts_available.xlsx\")\n",
    "message(\"2. Match against your UKB blood values\")\n",
    "message(\"3. Choose your preferred extraction approach\")\n",
    "\n",
    "# Example: Run harmonized extraction with top 20 concepts\n",
    "# top_20_concepts <- head(lab_concepts_df$concept_id, 20)\n",
    "# harmonized_sql <- create_harmonized_blood_extraction(top_20_concepts)\n",
    "# message(\"Harmonized extraction SQL ready. Use bq_table_save() to run it.\")\n",
    "\n",
    "# For batch processing, use the previous batch_processor functions with:\n",
    "# my_filters <- NULL  # No filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmonized Blood Data yearly means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HARMONIZED BLOOD DATA EXTRACTION WITH YEARLY MEANS - R SCRIPT\n",
    "# =============================================================================\n",
    "# This creates aggregated blood data (mean per person-year) in R using BigQuery\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HARMONIZED EXTRACTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "create_harmonized_blood_extraction <- function(selected_concept_ids = NULL, \n",
    "                                               start_year = 2010,\n",
    "                                               min_measurements = 1) {\n",
    "  \n",
    "  # Base query for harmonized extraction\n",
    "  base_harmonized_sql <- paste(\"\n",
    "  SELECT\n",
    "      measurement.person_id,\n",
    "      m_standard_concept.concept_name as standard_concept_name,\n",
    "      EXTRACT(YEAR FROM measurement.measurement_datetime) as year,\n",
    "      CONCAT(CAST(measurement.person_id AS STRING), '_', CAST(EXTRACT(YEAR FROM measurement.measurement_datetime) AS STRING)) as person_id_year,\n",
    "      AVG(measurement.value_as_number) as mean_value,\n",
    "      COUNT(measurement.value_as_number) as measurement_count,\n",
    "      MIN(measurement.measurement_datetime) as first_measurement_date,\n",
    "      MAX(measurement.measurement_datetime) as last_measurement_date\n",
    "  FROM \n",
    "      `\", Sys.getenv(\"WORKSPACE_CDR\"), \".measurement` measurement\n",
    "  LEFT JOIN\n",
    "      `\", Sys.getenv(\"WORKSPACE_CDR\"), \".concept` m_standard_concept \n",
    "          ON measurement.measurement_concept_id = m_standard_concept.concept_id \n",
    "  WHERE\n",
    "      measurement.value_as_number IS NOT NULL\n",
    "      AND measurement.measurement_datetime IS NOT NULL\n",
    "      AND measurement.person_id IS NOT NULL\n",
    "      AND m_standard_concept.concept_name IS NOT NULL\n",
    "      AND measurement.measurement_datetime >= '\", start_year, \"-01-01'\n",
    "      AND measurement.value_as_number > 0\n",
    "      AND measurement.value_as_number < 1000000\", sep=\"\")\n",
    "  \n",
    "  # Add concept filtering if provided\n",
    "  if(!is.null(selected_concept_ids)) {\n",
    "    concept_filter <- paste0(\" AND measurement.measurement_concept_id IN (\", \n",
    "                            paste(selected_concept_ids, collapse = \",\"), \")\")\n",
    "    base_harmonized_sql <- paste0(base_harmonized_sql, concept_filter)\n",
    "  }\n",
    "  \n",
    "  # Add GROUP BY and ORDER BY\n",
    "  base_harmonized_sql <- paste0(base_harmonized_sql, \"\n",
    "  GROUP BY \n",
    "      measurement.person_id,\n",
    "      m_standard_concept.concept_name,\n",
    "      EXTRACT(YEAR FROM measurement.measurement_datetime)\n",
    "  HAVING \n",
    "      COUNT(measurement.value_as_number) >= \", min_measurements, \"\n",
    "  ORDER BY \n",
    "      measurement.person_id,\n",
    "      standard_concept_name,\n",
    "      year\")\n",
    "  \n",
    "  return(base_harmonized_sql)\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# RUN HARMONIZED EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "run_harmonized_blood_extraction <- function(selected_concept_ids = NULL,\n",
    "                                           export_name = \"blood_labs_harmonized\",\n",
    "                                           start_year = 2010) {\n",
    "  \n",
    "  # Create the SQL query\n",
    "  harmonized_sql <- create_harmonized_blood_extraction(\n",
    "    selected_concept_ids = selected_concept_ids,\n",
    "    start_year = start_year\n",
    "  )\n",
    "  \n",
    "  # Set up GCS export path\n",
    "  harmonized_path <- file.path(\n",
    "    Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "    \"bq_exports\",\n",
    "    Sys.getenv(\"OWNER_EMAIL\"),\n",
    "    strftime(lubridate::now(), \"%Y%m%d\"),\n",
    "    export_name,\n",
    "    paste0(export_name, \"_*.csv\")\n",
    "  )\n",
    "  \n",
    "  message(str_glue('Harmonized blood data will be written to {harmonized_path}'))\n",
    "  \n",
    "  # Export to GCS\n",
    "  bq_table_save(\n",
    "    bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), harmonized_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "    harmonized_path,\n",
    "    destination_format = \"CSV\"\n",
    "  )\n",
    "  \n",
    "  return(harmonized_path)\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# READ HARMONIZED DATA FROM GCS\n",
    "# =============================================================================\n",
    "\n",
    "read_harmonized_blood_from_gcs <- function(export_path) {\n",
    "  \n",
    "  # Function to read harmonized data\n",
    "  read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "    col_types <- cols(\n",
    "      person_id = col_character(),\n",
    "      standard_concept_name = col_character(),\n",
    "      year = col_integer(),\n",
    "      person_id_year = col_character(),\n",
    "      mean_value = col_double(),\n",
    "      measurement_count = col_integer(),\n",
    "      first_measurement_date = col_character(),\n",
    "      last_measurement_date = col_character()\n",
    "    )\n",
    "    \n",
    "    bind_rows(\n",
    "      map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "          function(csv) {\n",
    "            message(str_glue('Loading {csv}.'))\n",
    "            chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "            chunk\n",
    "          }))\n",
    "  }\n",
    "  \n",
    "  # Read the data\n",
    "  df_harmonized <- read_bq_export_from_workspace_bucket(export_path)\n",
    "  \n",
    "  message(str_glue(\"Harmonized blood data loaded: {nrow(df_harmonized)} rows, {ncol(df_harmonized)} columns\"))\n",
    "  \n",
    "  return(df_harmonized)\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# CONVERT TO WIDE FORMAT (Like your existing pipeline)\n",
    "# =============================================================================\n",
    "\n",
    "convert_to_wide_format <- function(df_harmonized) {\n",
    "  \n",
    "  # Convert to wide format (one row per person_id_year with lab columns)\n",
    "  df_blood_wide <- df_harmonized %>%\n",
    "    select(person_id, person_id_year, year, standard_concept_name, mean_value) %>%\n",
    "    pivot_wider(\n",
    "      names_from = standard_concept_name,\n",
    "      values_from = mean_value,\n",
    "      id_cols = c(person_id, person_id_year, year)\n",
    "    )\n",
    "  \n",
    "  message(str_glue(\"Wide format data: {nrow(df_blood_wide)} rows, {ncol(df_blood_wide)} columns\"))\n",
    "  \n",
    "  return(df_blood_wide)\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "# Example 1: Extract ALL lab measurements (harmonized)\n",
    "run_all_labs_harmonized <- function() {\n",
    "  \n",
    "  message(\"Running harmonized extraction for ALL lab measurements...\")\n",
    "  \n",
    "  # Run without concept filtering (gets everything)\n",
    "  export_path <- run_harmonized_blood_extraction(\n",
    "    selected_concept_ids = NULL,  # NULL = all concepts\n",
    "    export_name = \"blood_labs_all_harmonized\",\n",
    "    start_year = 2010\n",
    "  )\n",
    "  \n",
    "  # Read the data\n",
    "  df_harmonized <- read_harmonized_blood_from_gcs(export_path)\n",
    "  \n",
    "  # Convert to wide format\n",
    "  df_blood_wide <- convert_to_wide_format(df_harmonized)\n",
    "  \n",
    "  # Save processed data\n",
    "  write.table(df_blood_wide, \"data/df_blood_harmonized.txt\", sep=\"\\t\", quote=FALSE, row.names=FALSE)\n",
    "  \n",
    "  message(\"Harmonized blood data saved to: data/df_blood_harmonized.txt\")\n",
    "  \n",
    "  return(df_blood_wide)\n",
    "}\n",
    "\n",
    "# Example 2: Extract specific concepts (after you choose them)\n",
    "run_selected_labs_harmonized <- function(concept_ids) {\n",
    "  \n",
    "  message(str_glue(\"Running harmonized extraction for {length(concept_ids)} selected concepts...\"))\n",
    "  \n",
    "  # Run with concept filtering\n",
    "  export_path <- run_harmonized_blood_extraction(\n",
    "    selected_concept_ids = concept_ids,\n",
    "    export_name = \"blood_labs_selected_harmonized\",\n",
    "    start_year = 2010\n",
    "  )\n",
    "  \n",
    "  # Read and process\n",
    "  df_harmonized <- read_harmonized_blood_from_gcs(export_path)\n",
    "  df_blood_wide <- convert_to_wide_format(df_harmonized)\n",
    "  \n",
    "  # Save\n",
    "  write.table(df_blood_wide, \"data/df_blood_selected_harmonized.txt\", sep=\"\\t\", quote=FALSE, row.names=FALSE)\n",
    "  \n",
    "  return(df_blood_wide)\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# READY TO USE\n",
    "# =============================================================================\n",
    "\n",
    "message(\"=== HARMONIZED BLOOD EXTRACTION READY ===\")\n",
    "message(\"Usage:\")\n",
    "message(\"1. For ALL labs: df_blood <- run_all_labs_harmonized()\")\n",
    "message(\"2. For specific labs: df_blood <- run_selected_labs_harmonized(c(3013682, 3000963, ...))\")\n",
    "message(\"\")\n",
    "message(\"This will create data with person_id_year structure matching your existing pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "493px",
    "width": "274px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
