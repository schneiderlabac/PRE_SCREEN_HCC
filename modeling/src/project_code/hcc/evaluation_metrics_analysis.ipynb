{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Plot Relationships between your metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This ipynb is not receiving your raw pipeline data, but its purpose is to provide visuals based on the summary model metrics that are automatically exported during validation/testing. In principle, you can plot any correlation between two metrics (or a third one plotted as color). \n",
    "\n",
    "Requirement: Scenarios_colors, scenarios_list and, most importantly, \"all_evaluation_results.xlsx\" in your project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "sys.path.append(\"../../modeling_pipeline\") #Because the project is in a different folder (two levels up), we need to add the path to the sys path\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pipeline import * #Load our package with classes pipeline, models, pp (preprocessing), plot, and more\n",
    "from wrapper_eval_metrics import * #Load our wrapper for the ROC analysis\n",
    "\n",
    "#This allows us to automatically reload the packages we are working on in the background, no \"Restart Kernel\" needed\n",
    "\n",
    "\n",
    "\n",
    "############### CHANGE THIS ############\n",
    "path= pp.userpath(os.environ.get(\"USER\", os.environ.get(\"USERNAME\")), project=\"hcc\") # Choose your own project here, only works if you added specific project in user_settings.json\n",
    "############### CHANGE THIS ############\n",
    "\n",
    "\n",
    "fig_path = f\"{path}/visuals\"\n",
    "auroc_path = f\"{fig_path}/AUROCs\"\n",
    "if not os.path.exists(auroc_path):\n",
    "    os.makedirs(auroc_path)\n",
    "\n",
    "# Load the default color dictionary\n",
    "yaml_colors_path = \"custom_colors.yaml\"\n",
    "with open(yaml_colors_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "scenarios_colors = config.get(\"scenarios_colors\", {}) # Extract the color dictionary\n",
    "print(\"Successfully loaded color dictionary with\", len(scenarios_colors), \"entries\")\n",
    "\n",
    "scenario_lists = config.get(\"scenario_lists\", {}) # Extract the color dictionary\n",
    "print(\"Successfully loaded scenario list with\", len(scenario_lists), \"entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type=\"RFC\"\n",
    "\n",
    "# Load the threshold-dependent results\n",
    "results_path = \"path/to/all_evaluation_results.xlsx\"\n",
    "threshold_results_UKB = pd.read_excel(path+'/Models/Pipelines/'+model_type+'/combined_output/val/all_evaluation_results.xlsx', sheet_name=\"Threshold metrics_all_threshold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7eed9",
   "metadata": {},
   "source": [
    "### Supplementary Figure 10 a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For comparing multiple models\n",
    "models = [\"Model_TOP15\"]\n",
    "datasets = [\"par\", \"all\"]\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        plot_metric_tradeoff(\n",
    "            threshold_results_UKB,\n",
    "            model_filter=model,\n",
    "            dataset_filter=dataset,\n",
    "            font_size=18,\n",
    "            dot_size=180,\n",
    "            fig_path=fig_path,\n",
    "            figsize=(8, 6),\n",
    "            name_suffix=\"UKB\",\n",
    "            title=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type=\"RFC\"\n",
    "\n",
    "# Load the threshold-dependent results\n",
    "results_path = \"path/to/all_evaluation_results.xlsx\"\n",
    "threshold_results_AOU = pd.read_excel(path+'/ext_val_tables/all_evaluation_results_AOU.xlsx', sheet_name=\"Threshold metrics_all_threshold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030b855",
   "metadata": {},
   "source": [
    "### Supplementary Figure 10 c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For comparing multiple models\n",
    "models = [\"Model_TOP15\"]\n",
    "datasets = [\"par\", \"all\"]\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        plot_metric_tradeoff(\n",
    "            threshold_results_AOU,\n",
    "            model_filter=model,\n",
    "            dataset_filter=dataset,\n",
    "            font_size=18,\n",
    "            dot_size=180,\n",
    "            fig_path=fig_path,\n",
    "            figsize=(8, 6),\n",
    "            name_suffix=\"AOU\",\n",
    "            title=False\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
