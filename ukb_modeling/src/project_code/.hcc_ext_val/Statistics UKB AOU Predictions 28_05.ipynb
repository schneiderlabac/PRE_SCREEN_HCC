{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Statistics Prediction Values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Functions and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "import time\n",
    "from scipy.stats import sem, t\n",
    "from scipy import stats\n",
    "import matplotlib.font_manager as fm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Parameters for the plotting:\n",
    "plt.rcParams['font.family'] = 'sans-serif' \n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans'] #change font to a known standard font\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"axes.labelsize\"] = 16\n",
    "plt.rcParams[\"xtick.labelsize\"] = 16\n",
    "plt.rcParams[\"ytick.labelsize\"] = 16\n",
    "plt.rcParams[\"legend.fontsize\"] = 16\n",
    "plt.rcParams[\"figure.titlesize\"] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(df_aou, df_ukb, sheet_name):\n",
    "    results = {}\n",
    "    \n",
    "    # Check for required columns\n",
    "    aou_valid = check_required_columns(df_aou, sheet_name, \"AOU\")\n",
    "    ukb_valid = check_required_columns(df_ukb, sheet_name, \"UKB\")\n",
    "    \n",
    "    if not (aou_valid and ukb_valid):\n",
    "        return None  # Skip this comparison if required columns are missing\n",
    "\n",
    "    for condition, mask_aou, mask_ukb in [\n",
    "        ('All', slice(None), slice(None)),\n",
    "        ('No HCC', df_aou['status'] == 1, df_ukb['status'] == 1),\n",
    "        ('HCC', df_aou['status'] == 0, df_ukb['status'] == 0)\n",
    "    ]:\n",
    "        # Perform Mann-Whitney U test\n",
    "        statistic, p_value = stats.mannwhitneyu(df_aou.loc[mask_aou, 'y_pred'], \n",
    "                                                df_ukb.loc[mask_ukb, 'y_pred'])\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        aou_mean = df_aou.loc[mask_aou, 'y_pred'].mean()\n",
    "        aou_std = df_aou.loc[mask_aou, 'y_pred'].std()\n",
    "        ukb_mean = df_ukb.loc[mask_ukb, 'y_pred'].mean()\n",
    "        ukb_std = df_ukb.loc[mask_ukb, 'y_pred'].std()\n",
    "        \n",
    "        results[condition] = {\n",
    "            'AOU Mean': aou_mean,\n",
    "            'AOU Std': aou_std,\n",
    "            'UKB Mean': ukb_mean,\n",
    "            'UKB Std': ukb_std,\n",
    "            'P-value': p_value\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def check_required_columns(df, sheet_name, dataset_name):\n",
    "    required_columns = ['status', 'y_pred']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Warning: {dataset_name} dataset, sheet '{sheet_name}' is missing columns: {', '.join(missing_columns)}\")\n",
    "    return len(missing_columns) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/jupyter/workspaces/machinelearningforlivercancerriskprediction\"\n",
    "fig_path = f\"{path}/HCC/visuals\"\n",
    "\n",
    "\n",
    "# get the data from the combined\n",
    "excel_file = pd.ExcelFile(path+'/combined_output/val/Prediction_values_combined.xlsx')\n",
    "sheet_names = excel_file.sheet_names\n",
    "\n",
    "# Create DataFrames\n",
    "dataframes = {}\n",
    "for sheet_name in sheet_names:\n",
    "    dataframes[sheet_name] = pd.read_excel(excel_file, sheet_name)\n",
    "    print(f\"Sheet {sheet_name} read and saved to dataframes dictionary\")\n",
    "dataframes.pop('Sheet')\n",
    "# dataframes.pop([i for i in list(dataframes.keys()) if str(i).endswith('Model_D') or str(i).endswith('Demographics')][0])\n",
    "dataframes.keys()\n",
    "\n",
    "\n",
    "# Load UKB prediction data\n",
    "excel_file_ukb = pd.ExcelFile(path+'/HCC/prediction_values_ukb.xlsx')\n",
    "sheet_names_ukb = excel_file_ukb.sheet_names\n",
    "dataframes_ukb = {}\n",
    "for sheet_name in sheet_names_ukb:\n",
    "    dataframes_ukb[sheet_name] = pd.read_excel(excel_file_ukb, sheet_name)\n",
    "    print(f\"Sheet {sheet_name} read and saved to dataframes dictionary\")\n",
    "\n",
    "# dataframes.pop([i for i in list(dataframes.keys()) if str(i).endswith('Model_D') or str(i).endswith('Demographics')][0])\n",
    "dataframes_ukb.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[\"all_Model_TOP30\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Perform comparisons for all matching sheets\n",
    "all_results = {}\n",
    "for sheet_name in set(dataframes.keys()) & set(dataframes_ukb.keys()):\n",
    "    result = compare_predictions(dataframes[sheet_name], dataframes_ukb[sheet_name], sheet_name)\n",
    "    if result is not None:\n",
    "        all_results[sheet_name] = result\n",
    "\n",
    "if not all_results:\n",
    "    print(\"No valid comparisons could be made due to missing columns.\")\n",
    "else:\n",
    "    # Apply Bonferroni correction\n",
    "    all_p_values = [result[condition]['P-value'] \n",
    "                    for result in all_results.values() \n",
    "                    for condition in result.keys()]\n",
    "    rejected, corrected_p_values, _, _ = multipletests(all_p_values, method='bonferroni')\n",
    "\n",
    "    # Update results with corrected p-values\n",
    "    p_value_index = 0\n",
    "    for sheet_name, result in all_results.items():\n",
    "        for condition in result.keys():\n",
    "            all_results[sheet_name][condition]['Corrected P-value'] = corrected_p_values[p_value_index]\n",
    "            p_value_index += 1\n",
    "\n",
    "    # Create a DataFrame to display the results\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Sheet': sheet_name,\n",
    "            'Condition': condition,\n",
    "            'AOU Mean (SD)': f\"{result[condition]['AOU Mean']:.4f} ({result[condition]['AOU Std']:.4f})\",\n",
    "            'UKB Mean (SD)': f\"{result[condition]['UKB Mean']:.4f} ({result[condition]['UKB Std']:.4f})\",\n",
    "            'P-value': result[condition]['P-value'],\n",
    "            'Corrected P-value': result[condition]['Corrected P-value']\n",
    "        }\n",
    "        for sheet_name, result in all_results.items()\n",
    "        for condition in result.keys()\n",
    "    ])\n",
    "\n",
    "    # Sort the DataFrame\n",
    "    results_df = results_df.sort_values(['Sheet', 'Condition'])\n",
    "\n",
    "    # Display the results\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    # Optionally, save to Excel\n",
    "    results_df.to_excel(f\"{fig_path}/prediction_comparison_results.xlsx\", index=False)\n",
    "    print(f\"\\nResults saved to {fig_path}/prediction_comparison_results.xlsx\")\n",
    "\n",
    "# Print column names for each dataframe\n",
    "print(\"\\nColumn names in each dataframe:\")\n",
    "for dataset_name, dataset in [(\"AOU\", dataframes), (\"UKB\", dataframes_ukb)]:\n",
    "    print(f\"\\n{dataset_name} dataset:\")\n",
    "    for sheet_name, df in dataset.items():\n",
    "        print(f\"  Sheet '{sheet_name}': {', '.join(df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
