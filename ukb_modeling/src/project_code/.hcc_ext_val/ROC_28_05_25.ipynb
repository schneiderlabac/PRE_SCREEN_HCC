{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.font_manager as fm\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif' \n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans'] #change font to a known standard font"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the env\n",
    "\n",
    "path = \"/home/jupyter/workspaces/machinelearningforlivercancerriskprediction\"\n",
    "fig_path = f\"{path}/visuals\"\n",
    "auroc_path = f\"{fig_path}/AUROCs\"\n",
    "date = \"30_04_2025\"\n",
    "\n",
    "if not os.path.exists(auroc_path):\n",
    "    os.makedirs(auroc_path)\n",
    "\n",
    "\n",
    "#Define all possible scenarios and colors\n",
    "all_scenarios = {\n",
    "    'A': '#4995AD',\n",
    "    'B': '#385579',\n",
    "    'C': '#C13617',\n",
    "    'D': '#F0903E',\n",
    "    'E': '#F0C872',\n",
    "    'Demographics': '#4995AD',\n",
    "    'Diagnosis': '#385579',\n",
    "    'Blood': '#C13617',\n",
    "    'SNP': '#F0903E',\n",
    "    'Metabolomics': '#F0C872',\n",
    "    'Csmall': '#402155',\n",
    "    'AMAP-RFC': '#c9c9c9',\n",
    "    'TOP75' : '#cb6043',\n",
    "    'TOP30' : '#d1846e',\n",
    "    'TOP15' : '#d0a79a'\n",
    "}\n",
    "\n",
    "\n",
    "# 'Demographics': '#402155',\n",
    "# 'Diagnosis': '#1E477C',\n",
    "# 'Blood': '#21968C',\n",
    "# 'SNP': '#74E980',\n",
    "# 'Metabolomics': '#F8E61E'\n",
    "\n",
    "# Define different scenario lists (combinations of scenarios plotted together)\n",
    "scenario_lists = {\n",
    "    'incremental': ['A', 'B', 'C', 'D', 'E'],\n",
    "    'separate': ['Demographics', 'Diagnosis', 'Blood', 'SNP', 'Metabolomics'],\n",
    "    'small_prev': ['C', 'Csmall', 'AMAP-RFC'],\n",
    "    'small': ['AMAP-RFC', 'TOP30', 'TOP15'],\n",
    "    'small_par' : ['TOP15'],\n",
    "    'c': ['TOP75', 'TOP15'],\n",
    "    'all': list(all_scenarios.keys()),\n",
    "    'benchmarks': ['aMAP', 'APRI', 'FIB4', 'NFS', 'Liver cirrhosis']\n",
    "}\n",
    "\n",
    "def get_colors(scenario_list):\n",
    "    return {scenario: all_scenarios[scenario] for scenario in scenario_list}\n",
    "\n",
    "\n",
    "def plot_colorbar(scenarios):\n",
    "    \"\"\"\n",
    "    Plots a colorbar based on the given scenarios.\n",
    "\n",
    "    Parameters:\n",
    "    - scenarios (list): A list of scenario labels.\n",
    "    \"\"\"\n",
    "    colors = get_colors(scenarios)\n",
    "    fig, ax = plt.subplots(figsize=(5.5, 1.1))\n",
    "    for i, (label, color) in enumerate(colors.items()):\n",
    "        rect = plt.Rectangle((i * 55, 0), 55, 55, linewidth=2, edgecolor='white', facecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(i * 55 + 27.5, -10, label, ha='center', va='top', fontsize=10, color='black')\n",
    "    ax.set_xlim(0, len(scenarios) * 55)\n",
    "    ax.set_ylim(-20, 55)\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve(test_scores, true_labels, ax=False, label=None, color='#c9c9c9', lw=2.5, linestyle=\"--\", fig_path=None, font_size=16):\n",
    "    \"\"\"\n",
    "    Plots a ROC curve given test scores and true labels.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    test_scores : array-like\n",
    "        Test scores for the positive class.\n",
    "        true_labels : array-like\n",
    "            True labels for the positive class.\n",
    "            ax : matplotlib.axes, optional\n",
    "            Axes object to plot on. If None, creates a new figure.\n",
    "            label : str, optional\n",
    "            Label for the ROC curve. If None, uses the AUC value.\n",
    "            color : str, optional\"\"\"\n",
    "\n",
    "    # Calculate ROC curve and AUC\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, test_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_auc = round(roc_auc, 2)\n",
    "    base_fpr = np.linspace(0, 1, 100)\n",
    "    tpr = np.interp(base_fpr, fpr, tpr)\n",
    "\n",
    "    if label is None:\n",
    "        plot_label = 'aMAP ({:.2f})'.format(roc_auc)\n",
    "    else:\n",
    "        plot_label = '{} ({:.2f})'.format(label, roc_auc)\n",
    "\n",
    "    # Create the ROC curve plot\n",
    "    if ax == False:\n",
    "        plt.plot(base_fpr, tpr, color=color, lw=lw, label=plot_label, alpha=1, linestyle=linestyle)\n",
    "    else:\n",
    "        ax.plot(base_fpr, tpr, color=color, lw=lw, label=plot_label, alpha=1, linestyle=linestyle)\n",
    "    return thresholds, fpr, tpr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_rocs(tprs, fig, ax, plot_all=True, y_amap=None, col_line=None, scenario='',\n",
    "              fill_bet=True, title='', fig_type='', n_splits=5, line_style='-', save_fig=True,\n",
    "              # Add new parameters for customization\n",
    "              individual_alpha=0.15,       # Lower alpha for individual lines\n",
    "              individual_lw=1,             # Thinner individual lines\n",
    "              individual_color=None,       # Optional different color for individual lines\n",
    "              mean_lw=2.5,                 # Keep mean line width the same\n",
    "              mean_alpha=1.0,               # Full opacity for mean line\n",
    "              fig_path=None,\n",
    "              font_size=None):\n",
    "    \"\"\"\n",
    "    Creates multiple ROC curves on the same plot. Gets called by plot_rocs_flexible and plot_rocs_multi_estimator.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    mapped_tprs : DataFrame\n",
    "        Multi-indexed DataFrame containing TPR values.\n",
    "    fig : matplotlib.figure\n",
    "        Figure object to plot on.\n",
    "    ax : matplotlib.axes\n",
    "        Axes object to plot on.\n",
    "    scenarios : list\n",
    "        List of scenario identifiers to plot.\n",
    "    cohort : str\n",
    "        Cohort identifier.\n",
    "    scenarios_colors : dict, optional\n",
    "        Dictionary mapping scenario names to colors.\n",
    "        If None, uses the global default colors.\n",
    "    plot_all : bool, optional\n",
    "        Whether to plot individual fold curves.\n",
    "    fill_bet : bool, optional\n",
    "        Whether to fill the area between standard deviation bounds.\n",
    "    title : str, optional\n",
    "        Title for the plot.\n",
    "    fig_type : str, optional\n",
    "        Figure type identifier for saving.\n",
    "    n_splits : int, optional\n",
    "        Number of cross-validation splits.\n",
    "    \"\"\"\n",
    "    if font_size == None:\n",
    "        font_size = 20\n",
    "\n",
    "    # Compute mean ROC curve and AUC\n",
    "\n",
    "    tprs = np.array(tprs)\n",
    "    mean_tprs = tprs.mean(axis=0)\n",
    "    std = tprs.std(axis=0)\n",
    "    base_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "    tprs_lower = mean_tprs - std\n",
    "\n",
    "    # Plot ROC curves for each fold and mean ROC curve\n",
    "    if plot_all:\n",
    "        individual_col = individual_color if individual_color is not None else col_line\n",
    "        for i in range(n_splits):\n",
    "            ax.plot(base_fpr, tprs[i], color=individual_col, alpha=individual_alpha, lw=individual_lw)\n",
    "\n",
    "    # Plot mean line with original parameters\n",
    "    ax.plot(base_fpr, mean_tprs, col_line, linestyle=line_style,\n",
    "            label=f'{scenario} ({round(auc(base_fpr,mean_tprs),ndigits=2)})',\n",
    "            lw=mean_lw, alpha=mean_alpha)\n",
    "\n",
    "    if fill_bet:\n",
    "        ax.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], color=\"grey\", linestyle=\"--\", lw=2.5)\n",
    "    if y_amap is not None:\n",
    "        plot_roc_curve(test_scores=y_amap.amap, true_labels=y_amap.status, ax=ax)\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.0])\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=font_size)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=font_size)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=font_size, pad=0)\n",
    "    ax.set_title(title, fontsize=font_size+2, pad=10)\n",
    "    condensed_font = fm.FontProperties(family='DejaVu Sans', style='normal', weight='normal', stretch='condensed')\n",
    "    ax.legend(loc=\"lower right\", bbox_to_anchor=(1.01, -0.02), fontsize=font_size, frameon=False, prop=condensed_font)\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "\n",
    "    # Export\n",
    "    if save_fig & (fig_path is not None):\n",
    "        save_figure(fig, title, fig_type, fig_path)\n",
    "\n",
    "\n",
    "def plot_rocs_flexible(mapped_tprs, fig, ax, scenarios, cohort, plot_all=False, fill_bet=False, fig_path=None, title='', fig_type='', n_splits=5, font_size=20):\n",
    "    colors = get_colors(scenarios)\n",
    "    for scenario in scenarios:\n",
    "        color = colors[scenario]\n",
    "        scenario_tprs = mapped_tprs.loc[(cohort, scenario), :]\n",
    "        plot_rocs(tprs=scenario_tprs.values, fig=fig, ax=ax, plot_all=plot_all,\n",
    "                  fill_bet=fill_bet, col_line=color, scenario=scenario,\n",
    "                  title=title, fig_type=fig_type, n_splits=n_splits, fig_path=fig_path, font_size=font_size, save_fig=True)\n",
    "\n",
    "\n",
    "\n",
    "def plot_combined_roc(tprs1, tprs2, label1='amap_cld', label2='amap_all'):\n",
    "    \"\"\"\n",
    "    Plots two or more ROC curves in the same figure, currently used to compare the capacity of AMAP for different cohorts\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    mean_tpr1 = np.mean(tprs1, axis=0)\n",
    "    mean_tpr2 = np.mean(tprs2, axis=0)\n",
    "\n",
    "    ax.plot(mean_tpr1, label=label1)\n",
    "    ax.plot(mean_tpr2, label=label2)\n",
    "\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Combined ROC Curve')\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# def plot_rocs(tprs,fig, ax, plot_all=True,y_amap=None,col_line='b',scenario='',fill_bet=True, title='', fig_type='', n_splits=5, line_style='-'):\n",
    "#     # Compute mean ROC curve and AUC\n",
    "#     tprs = np.array(tprs)\n",
    "#     mean_tprs = tprs.mean(axis=0)\n",
    "#     std = tprs.std(axis=0)\n",
    "#     base_fpr=np.linspace(0, 1, 100)  # Create a range of fprs for the x\n",
    "\n",
    "#     tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "#     tprs_lower = mean_tprs - std\n",
    "\n",
    "#     # Plot ROC curves for each fold and mean ROC curve\n",
    "#     if plot_all==True:\n",
    "#         for i in range(n_splits):\n",
    "#             plt.plot(base_fpr, tprs[i], 'b', alpha=0.3, lw=3)\n",
    "#     plt.plot(base_fpr, mean_tprs, col_line, linestyle=line_style, label=f'{scenario} (AUC = {round(auc(base_fpr,mean_tprs),ndigits=3)})', lw=2.5)\n",
    "#     if fill_bet:\n",
    "#         plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "\n",
    "\n",
    "#     plt.plot([0, 1], [0, 1],color=\"grey\", linestyle=\"--\", lw=2.5)\n",
    "#     if y_amap is not None:\n",
    "#         plot_roc_curve(test_scores=y_amap.amap,true_labels=y_amap.status)\n",
    "\n",
    "#     ax.set_xlim([0.0, 1.0])\n",
    "#     ax.set_ylim([0.0, 1.0])\n",
    "#     ax.set_xlabel('False Positive Rate', fontsize=14)\n",
    "#     ax.set_ylabel('True Positive Rate', fontsize=14)\n",
    "#     ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "#     ax.set_title(title, fontsize=16, pad=10)\n",
    "#     ax.legend(loc=\"lower right\", fontsize=12)\n",
    "#     plt.rcParams.update({'font.size': 16})  # Set a default font size for all elements\n",
    "\n",
    "#     #Export\n",
    "#     name = \"ROCs\"\n",
    "#     if fig_path:\n",
    "#         save_figure(fig, title, fig_type, fig_path)\n",
    "\n",
    "\n",
    "# def plot_combined_roc(tprs1, tprs2, label1='amap_cld', label2='amap_all'):\n",
    "#     \"\"\"\n",
    "#     Plots two or more ROC curves in the same figure, currently used to compare the capacity of AMAP for different cohorts\n",
    "\n",
    "#     Parameters:\n",
    "#     - tprs1: DataFrame or array-like, TPRs for the first data.\n",
    "#     - tprs2: DataFrame or array-like, TPRs for the second data.\n",
    "#     - label1: str, label for the first data.\n",
    "#     - label2: str, label for the second data.\n",
    "#     \"\"\"\n",
    "#     fig, ax = plt.subplots()\n",
    "#     mean_tpr1 = np.mean(tprs1, axis=0)\n",
    "#     mean_tpr2 = np.mean(tprs2, axis=0)\n",
    "\n",
    "#     ax.plot(mean_tpr1, label=label1)\n",
    "#     ax.plot(mean_tpr2, label=label2)\n",
    "\n",
    "#     ax.set_xlabel('False Positive Rate')\n",
    "#     ax.set_ylabel('True Positive Rate')\n",
    "#     ax.set_title('Combined ROC Curve')\n",
    "#     ax.legend(loc='best')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Usage\n",
    "# # plot_combined_roc(tprs_amap_cld, tprs_amap_all)\n",
    "\n",
    "def save_figure(fig, title, fig_type, fig_path):\n",
    "    # Create necessary directories\n",
    "    os.makedirs(fig_path, exist_ok=True)\n",
    "\n",
    "    # Replace spaces and special characters in title for filename\n",
    "    file_name = title.replace(' ', '_').replace('/', '_')\n",
    "\n",
    "    # Construct file paths for PNG and SVG\n",
    "    #png_path = os.path.join(auroc_path, f\"{fig_type}_{file_name}_{model_type}.png\")\n",
    "    svg_path = os.path.join(auroc_path, f\"{fig_type}_{file_name}_{model_type}.svg\")\n",
    "\n",
    "\n",
    "    # Save the figure in both formats\n",
    "    #fig.savefig(png_path, format='png', dpi=300)\n",
    "    fig.savefig(svg_path, format='svg', transparent=True)\n",
    "\n",
    "    \n",
    "def add_benchmarks_to_mapped_tprs(mapped_tprs, benchmark_dict, benchmark_names=None, n_folds=5, status_col='status'):\n",
    "    \"\"\"\n",
    "    Add benchmark scores to mapped_tprs by calculating TPR values from test scores and true labels.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    mapped_tprs : pd.DataFrame\n",
    "        Existing multi-index DataFrame with TPR values\n",
    "        Index: (cohort, scenario, model, estimator)\n",
    "        Columns: 0-99 (representing TPR values at standardized FPR points)\n",
    "    benchmark_dict : dict\n",
    "        Dictionary mapping cohort names to benchmark dataframes\n",
    "        Each dataframe should have: 'eid', benchmark score columns, 'status'/'status_cancerreg'\n",
    "    benchmark_names : list, optional\n",
    "        List of benchmark column names to include. If None, auto-detects.\n",
    "    n_folds : int\n",
    "        Number of folds to replicate (to match CV structure). Default: 5\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Updated mapped_tprs with benchmark scores included\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    updated_tprs = mapped_tprs.copy()\n",
    "\n",
    "    # Get the number of FPR points from existing data (should be 100)\n",
    "    n_fpr_points = len(mapped_tprs.columns)\n",
    "    base_fpr = np.linspace(0, 1, n_fpr_points)\n",
    "\n",
    "    print(f\"Processing {len(benchmark_dict)} cohorts...\")\n",
    "    print(f\"Using {n_fpr_points} FPR points for interpolation\")\n",
    "\n",
    "    for cohort_name, cohort_df in benchmark_dict.items():\n",
    "        print(f\"\\nProcessing cohort: {cohort_name}\")\n",
    "\n",
    "        # Auto-detect benchmark columns if not provided\n",
    "        if benchmark_names is None:\n",
    "            detected_benchmarks = [col for col in cohort_df.columns\n",
    "                                 if col not in ['eid', 'status', 'status_cancerreg']]\n",
    "        else:\n",
    "            detected_benchmarks = benchmark_names\n",
    "\n",
    "        print(f\"Found benchmark columns: {detected_benchmarks}\")\n",
    "\n",
    "        # Determine which status column to use\n",
    "        print(f\"Using status column: {status_col}\")\n",
    "\n",
    "        for benchmark_name in detected_benchmarks:\n",
    "            print(f\"  Processing benchmark: {benchmark_name}\")\n",
    "\n",
    "            # Get valid data (non-null predictions and labels)\n",
    "            valid_mask = (cohort_df[benchmark_name].notna() &\n",
    "                         cohort_df[status_col].notna())\n",
    "\n",
    "            if valid_mask.sum() == 0:\n",
    "                print(f\"    Warning: No valid data for {benchmark_name}\")\n",
    "                continue\n",
    "\n",
    "            n_valid = valid_mask.sum()\n",
    "            n_cases = cohort_df.loc[valid_mask, status_col].sum()\n",
    "            print(f\"    Valid samples: {n_valid}, Cases: {n_cases}\")\n",
    "\n",
    "            try:\n",
    "                # Step 1: Calculate ROC curve\n",
    "                fpr, tpr, thresholds = roc_curve(\n",
    "                    cohort_df.loc[valid_mask, status_col],  # True labels\n",
    "                    cohort_df.loc[valid_mask, benchmark_name]  # Test scores\n",
    "                )\n",
    "\n",
    "\n",
    "                # Step 2: Calculate AUC for validation (BEFORE interpolation)\n",
    "                auc_score = auc(fpr, tpr)\n",
    "\n",
    "                # Step 3: Interpolate TPR values at standardized FPR points\n",
    "                tpr_interpolated = np.interp(base_fpr, fpr, tpr)\n",
    "                print(f\"    AUC: {auc_score:.3f}\")\n",
    "\n",
    "                # Step 4: Add to mapped_tprs with proper multi-index structure\n",
    "                # Since benchmarks don't have CV folds, replicate the same TPR curve\n",
    "                for fold in range(n_folds):\n",
    "                    # Create multi-index tuple: (cohort, scenario, model, estimator)\n",
    "                    index_tuple = (\n",
    "                        cohort_name,      # cohort (e.g., 'all', 'par')\n",
    "                        benchmark_name,   # scenario (e.g., 'aMAP', 'APRI')\n",
    "                        f\"_{fold}\",            # model (0-4 for CV folds)\n",
    "                        'linear'         # estimator (all benchmarks are linear)\n",
    "                    )\n",
    "\n",
    "                    # Add the interpolated TPR values\n",
    "                    updated_tprs.loc[index_tuple] = tpr_interpolated\n",
    "\n",
    "                print(f\"    Successfully added {benchmark_name} for cohort {cohort_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing {benchmark_name}: {e}\")\n",
    "\n",
    "    print(f\"\\nFinal mapped_tprs shape: {updated_tprs.shape}\")\n",
    "    return updated_tprs\n",
    "\n",
    "plot_colorbar(scenario_lists['incremental'])\n",
    "plot_colorbar(scenario_lists['separate'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import single TPRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"RFC\"\n",
    "# import the tprs\n",
    "#tprs=pd.read_excel(path+'/Models/Pipelines/'+model_type+'/combined_output/val/TPRS_combined.xlsx')\n",
    "tprs=pd.read_excel(path+'/combined_output/val/TPRS_combined.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "columns=tprs.columns.tolist()\n",
    "mapper=pd.DataFrame({'col_names':columns})\n",
    "mapper[\"estimator\"] = model_type\n",
    "mapper['cohort']=[i.split('_')[0] for i in mapper.col_names]\n",
    "mapper['scenario']=[i.split('_')[2] for i in mapper.col_names]\n",
    "mapper['model']=[i.split('_model')[1] for i in mapper.col_names]\n",
    "mapper.set_index('col_names',inplace=True)\n",
    "tprs.transpose()\n",
    "mapped_tprs=pd.concat([mapper,tprs.transpose()],axis=1).set_index(['cohort','scenario','model', 'estimator'])\n",
    "mapped_tprs.groupby(level=['cohort','scenario']).agg('mean').transpose()\n",
    "mapped_tprs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load benchmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the amap data\n",
    "#amap_cld=pd.read_excel(path+'/Models/amaps_cld_all_with_y.xlsx')\n",
    "#amap_all=pd.read_csv(path+'/HCC/df_amap.csv')\n",
    "benchmarks= pd.read_csv(path+'/data/df_benchmark.csv')\n",
    "# amap_cirrhosis=pd.read_csv(path+'/Models/df_amap_cirrhosis.csv')\n",
    "# amap_nafld=pd.read_csv(path+'/Models/df_amap_nafld.csv')\n",
    "# amap_par=pd.read_csv(path+'/Models/df_amap_par.csv')\n",
    "benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing benchmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_impute = ['aMAP', 'NFS', 'APRI', 'FIB4', 'cirrhosis']\n",
    "\n",
    "# Verify the imputation\n",
    "print(\"NA counts before imputation:\")\n",
    "print(benchmarks[columns_to_impute].isnull().sum())\n",
    "\n",
    "# Impute the specified columns with their respective means\n",
    "for column in columns_to_impute:\n",
    "    benchmarks[column].fillna(benchmarks[column].mean(), inplace=True)\n",
    "\n",
    "# Verify the imputation\n",
    "print(\"NA counts after imputation:\")\n",
    "print(benchmarks[columns_to_impute].isnull().sum())\n",
    "\n",
    "# Optional: Display summary statistics of imputed columns\n",
    "print(\"\\nSummary of imputed columns:\")\n",
    "print(benchmarks[columns_to_impute].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PAR requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par = pd.read_csv(path+f\"/data/{date}/df_par_outer_basic_par.csv\")\n",
    "\n",
    "# Filter benchmarks based on matching eids in df_par\n",
    "benchmarks_par = benchmarks[benchmarks[\"eid\"].isin(df_par[\"eid\"])]\n",
    "\n",
    "# Summary statistics\n",
    "n_before = benchmarks.shape[0]\n",
    "n_after = benchmarks_par.shape[0]\n",
    "\n",
    "print(f\"Number of entries before filtering: {n_before}\")\n",
    "print(f\"Number of entries after filtering: {n_after}\")\n",
    "print(f\"Filtered out {n_before - n_after} entries ({(n_before - n_after) / n_before * 100:.2f}% reduction).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dict = {}\n",
    "benchmark_dict[\"all\"] = benchmarks\n",
    "benchmark_dict[\"par\"] = benchmarks_par\n",
    "\n",
    "benchmark_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the scores you want to use (need to be represented as column names in the benchmarks df and, accordingly in the benchmark_dict dataframes)\n",
    "benchmark_names = [\"aMAP\", \"APRI\", \"FIB4\", \"NFS\", \"cirrhosis\"]\n",
    "\n",
    "# Add benchmarks to mapped_tprs\n",
    "mapped_tprs = add_benchmarks_to_mapped_tprs(\n",
    "    mapped_tprs=mapped_tprs,\n",
    "    benchmark_dict=benchmark_dict,\n",
    "    benchmark_names=benchmark_names,  # Optional: specify which benchmarks to include\n",
    "    n_folds=5\n",
    ")\n",
    "\n",
    "# Verify the structure\n",
    "print(\"\\nUpdated mapped_tprs index levels:\")\n",
    "for i, level in enumerate(mapped_tprs.index.names):\n",
    "    unique_values = mapped_tprs.index.get_level_values(i).unique()\n",
    "    print(f\"  Level {i} ({level}): {unique_values.tolist()}\")\n",
    "\n",
    "# Check that benchmarks were added correctly\n",
    "benchmark_indices = mapped_tprs.index[\n",
    "    mapped_tprs.index.get_level_values('estimator') == 'linear'\n",
    "]\n",
    "print(f\"\\nAdded {len(benchmark_indices)} benchmark entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_tprs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmarks[\"aMAP\"], true_labels=benchmarks.status, ax=ax, label=\"aMAP\", linestyle=\"-\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"APRI\"], true_labels=benchmarks.status, ax=ax, label=\"APRI\", linestyle=\"--\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"FIB4\"], true_labels=benchmarks.status, ax=ax, label=\"FIB4\", linestyle=\"-.\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"NFS\"], true_labels=benchmarks.status, ax=ax, label=\"NFS\", linestyle=\":\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"cirrhosis\"], true_labels=benchmarks.status, ax=ax, label=\"Cirrhosis\", linestyle=\"-\", color=\"#385579\")\n",
    "plot_rocs_flexible(mapped_tprs, fig, ax, scenario_lists['c'], 'all',\n",
    "                   title='Literature Benchmark (All Of Us)', fig_type=\"AUROCS_combined\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined AUROCs (Incremental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmarks_par[\"aMAP\"], true_labels=benchmarks_par.status, ax=ax, label=\"aMAP\", linestyle=\"-\")\n",
    "plot_rocs_flexible(mapped_tprs, fig, ax, scenario_lists['incremental'], 'par',\n",
    "                   title='Chronic Liver Disease', fig_type=\"AUROCS_combined\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=amap_all[\"aMAP\"], true_labels=amap_all.status, ax=ax)\n",
    "plot_rocs_flexible(mapped_tprs, fig, ax, scenario_lists['incremental'], 'all',\n",
    "                   title=\"All\", fig_type=\"AUROCS_combined\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separately trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=amap_par[\"aMAP\"], true_labels=amap_par.status, ax=ax)\n",
    "plot_rocs_flexible(mapped_tprs, fig, ax, scenario_lists['separate'], 'par',\n",
    "                   title='Chronic Liver Disease', fig_type=\"AUROCS_separately\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=amap_all[\"aMAP\"], true_labels=amap_all.status, ax=ax)\n",
    "plot_rocs_flexible(mapped_tprs, fig, ax, scenario_lists['separate'], 'all',\n",
    "                   title='All', fig_type=\"AUROCS_separately\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(6, 5))\n",
    "# plot_roc_curve(test_scores=amap_all[\"aMAP\"], true_labels=amap_all.status, ax=ax)\n",
    "# plot_rocs_flexible(mapped_tprs, fig, ax, scenario_lists['small'], 'all',\n",
    "#                    title=\"All Patients - Small Models\", fig_type=\"AUROCS_small_models\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmarks[\"aMAP\"], true_labels=benchmarks.status, ax=ax, label=\"aMAP\", linestyle=\"-\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"APRI\"], true_labels=benchmarks.status, ax=ax, label=\"APRI\", linestyle=\"--\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"FIB4\"], true_labels=benchmarks.status, ax=ax, label=\"FIB4\", linestyle=\"-.\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"NFS\"], true_labels=benchmarks.status, ax=ax, label=\"NFS\", linestyle=\":\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"cirrhosis\"], true_labels=benchmarks.status, ax=ax, label=\"Cirrhosis\", linestyle=\"-\", color=\"#385579\")\n",
    "plot_rocs_flexible(mapped_tprs, fig, ax, scenario_lists['small'], 'all',\n",
    "                   title=\"All Patients - Small Models\", fig_type=\"AUROCS_small_models\", font_size=16, fig_path=fig_path)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small models PAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmarks_par[\"aMAP\"], true_labels=benchmarks_par.status, ax=ax, label=\"aMAP\", linestyle=\"-\")\n",
    "plot_roc_curve(test_scores=benchmarks_par[\"APRI\"], true_labels=benchmarks_par.status, ax=ax, label=\"APRI\", linestyle=\"--\")\n",
    "plot_roc_curve(test_scores=benchmarks_par[\"FIB4\"], true_labels=benchmarks_par.status, ax=ax, label=\"FIB4\", linestyle=\"-.\")\n",
    "plot_roc_curve(test_scores=benchmarks_par[\"NFS\"], true_labels=benchmarks_par.status, ax=ax, label=\"NFS\", linestyle=\":\")\n",
    "plot_roc_curve(test_scores=benchmarks_par[\"cirrhosis\"], true_labels=benchmarks_par.status, ax=ax, label=\"Cirrhosis\", linestyle=\"-\", color=\"#385579\")\n",
    "plot_rocs_flexible(mapped_tprs, fig, ax, scenario_lists['small_par'], 'par',\n",
    "                   title=\"Patients at Risk - Small Models\", fig_type=\"AUROCS_small_models\", fig_path=fig_path)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For all 5 scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "\n",
    "# Loop through each scenario in the scenarios_colors dictionary\n",
    "for scenario, color in scenarios_colors.items():\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_rocs(tprs=mapped_tprs.transpose()['cld', scenario].transpose(), col_line=color, scenario=scenario, plot_all=True, fill_bet=True, title=f'Chronic liver disease - Scenario {scenario}', fig_type=\"AUROC_sep\")\n",
    "\n",
    "for scenario, color in scenarios_colors.items():\n",
    "    fig_all, ax_all =plt.subplots()\n",
    "    plot_rocs(tprs=mapped_tprs.transpose()['all',scenario].transpose(),col_line=color,scenario=scenario,plot_all=True,fill_bet=True, title=f\"All - Scenario {scenario}\", fig_type=\"AUROC_sep\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax =plt.subplots()\n",
    "n_splits=5\n",
    "\n",
    "for scenario,color in zip(['A'],['#36617B']): #y, g, r, b, brown\n",
    "    plot_rocs(tprs=mapped_tprs.transpose()['par',scenario].transpose(),col_line=color,scenario=scenario,plot_all=True,fill_bet=True, title=f'Patients at Risk', fig_type=\"AUROC sep\")\n",
    "\n",
    "fig_all, ax_all =plt.subplots()\n",
    "n_splits=5\n",
    "for scenario,color in zip(['A'],['#36617B']):\n",
    "   plot_rocs(tprs=mapped_tprs.transpose()['all',scenario].transpose(),col_line=color,scenario=scenario,plot_all=False,fill_bet=False, title=\"All\", fig_type=\"AUROC_sep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing AMAP Subcohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing AMAP Subcohorts\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "#plot_roc_curve(test_scores=amap_cld.amap, true_labels=amap_cld.status, ax=ax, label=\"Chronic Liver Disease\", color=\"blue\", lw=1.5)\n",
    "plot_roc_curve(test_scores=amap_all[\"aMAP\"], true_labels=amap_all.status, ax=ax, label=\"All\", color=\"green\", lw=1.5)\n",
    "plot_roc_curve(test_scores=amap_par[\"aMAP\"], true_labels=amap_par.status, ax=ax, label=\"PAR\", color=\"green\", lw=1.5)\n",
    "#plot_roc_curve(test_scores=amap_cirrhosis.aMAP, true_labels=amap_cirrhosis.status, ax=ax, label=\"Cirrhosis\", color=\"red\", lw=1.5)\n",
    "#plot_roc_curve(test_scores=amap_nafld.aMAP, true_labels=amap_nafld.status, ax=ax, label=\"MASLD\", color=\"purple\", lw=1.5)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.plot([0, 1], [0, 1], color=\"grey\", linestyle=\"--\", lw=1)\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=13)\n",
    "plt.title(\"AMAP Score For Different Groups at Risk\", fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax =plt.subplots()\n",
    "n_splits=5\n",
    "\n",
    "for scenario,color in zip(['A'],['#36617B']): #y, g, r, b, brown\n",
    "    plot_rocs(tprs=mapped_tprs.transpose()['par',scenario].transpose(),col_line=color,scenario=scenario,plot_all=True,fill_bet=True, title=f'Patients at Risk', fig_type=\"AUROC sep\")\n",
    "\n",
    "fig_all, ax_all =plt.subplots()\n",
    "n_splits=5\n",
    "for scenario,color in zip(['A'],['#36617B']):\n",
    "   plot_rocs(tprs=mapped_tprs.transpose()['all',scenario].transpose(),col_line=color,scenario=scenario,plot_all=False,fill_bet=False, title=\"All\", fig_type=\"AUROC_sep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Estimators Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import multiple TPRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [\"XGB\", \"RFC\"]\n",
    "base_path = path + '/Models/Pipelines/'\n",
    "all_tprs = pd.DataFrame()\n",
    "\n",
    "for model_type in model_types:\n",
    "    # Construct path to the TPR file\n",
    "    tprs_path = f'{base_path}{model_type}/combined_output/val/TPRS_combined.xlsx'\n",
    "\n",
    "    # Load the TPRs\n",
    "    if os.path.exists(tprs_path):\n",
    "        tprs = pd.read_excel(tprs_path)\n",
    "        print(tprs.head)\n",
    "\n",
    "        columns=tprs.columns.tolist()\n",
    "        mapper=pd.DataFrame({'col_names':columns})\n",
    "        mapper[\"estimator\"] = model_type\n",
    "        mapper['cohort']=[i.split('_')[0] for i in mapper.col_names]\n",
    "        mapper['scenario']=[i.split('_')[2] for i in mapper.col_names]\n",
    "        mapper['model']=[i.split('_model')[1] for i in mapper.col_names]\n",
    "        mapper.set_index('col_names',inplace=True)\n",
    "        tprs.transpose()\n",
    "        mapped_tprs=pd.concat([mapper,tprs.transpose()],axis=1).set_index(['cohort','scenario','model', 'estimator'])\n",
    "        mapped_tprs.groupby(level=['cohort','scenario']).agg('mean').transpose()\n",
    "        mapped_tprs\n",
    "\n",
    "        # # Concatenate to the main DataFrame\n",
    "        all_tprs = pd.concat([all_tprs, mapped_tprs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark distinct estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "# Define cohorts to loop through\n",
    "cohorts = ['par']\n",
    "\n",
    "# Loop through each cohort\n",
    "for cohort in cohorts:\n",
    "    # Set up the plot for the current cohort\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))  # Adjust size as needed\n",
    "\n",
    "    # Loop through each scenario\n",
    "    for scenario, color in scenarios_colors_incremental.items():\n",
    "        # Loop through each estimator within the current scenario\n",
    "        for estimator in ['XGB', 'RFC']:  # Add other estimators as needed\n",
    "            # Extract TPRs for the current scenario, cohort, and estimator\n",
    "            scenario_tprs = all_tprs.loc[(cohort, scenario, slice(None), estimator), :]\n",
    "\n",
    "            line_style = \"--\" if estimator == 'XGB' else '-'\n",
    "\n",
    "\n",
    "            # Check if the scenario and estimator data is not empty\n",
    "            if not scenario_tprs.empty:\n",
    "                # Call plot_rocs function to plot the ROC curve\n",
    "                plot_rocs(tprs=scenario_tprs.values, fig=fig, ax=ax, plot_all=False, fill_bet=True, col_line=color, scenario=f'{scenario} - {estimator}', line_style=line_style, title=f'Estimator Benchmark - {cohort}', fig_type='AUROCS_combined', n_splits=n_splits)\n",
    "\n",
    "    # Finalize the plot settings\n",
    "    ax.set_title(f'AUROC Comparison by Scenario and Estimator for {cohort.upper()} Cohort')\n",
    "    ax.legend(title='Scenarios')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_structure(all_tprs, cohorts, scenarios, estimators):\n",
    "    print(\"Checking data structure...\")\n",
    "    print(f\"all_tprs shape: {all_tprs.shape}\")\n",
    "    print(f\"all_tprs index levels: {all_tprs.index.names}\")\n",
    "    print(f\"all_tprs columns: {all_tprs.columns}\")\n",
    "\n",
    "    # Check if cohorts, scenarios, and estimators exist in all_tprs\n",
    "    missing_cohorts = [cohort for cohort in cohorts if cohort not in all_tprs.index.get_level_values('cohort')]\n",
    "    missing_scenarios = [scenario for scenario in scenarios if scenario not in all_tprs.index.get_level_values('scenario')]\n",
    "    missing_estimators = [estimator for estimator in estimators if estimator not in all_tprs.index.get_level_values('estimator')]\n",
    "\n",
    "    if missing_cohorts:\n",
    "        print(f\"Error: Missing cohorts in all_tprs: {missing_cohorts}\")\n",
    "    if missing_scenarios:\n",
    "        print(f\"Error: Missing scenarios in all_tprs: {missing_scenarios}\")\n",
    "    if missing_estimators:\n",
    "        print(f\"Error: Missing estimators in all_tprs: {missing_estimators}\")\n",
    "\n",
    "def delong_roc_variance(tpr1, tpr2):\n",
    "    \"\"\"\n",
    "    Computes the variance for DeLong test using TPRs.\n",
    "    \"\"\"\n",
    "    n = len(tpr1)\n",
    "    v10 = np.var(tpr1)\n",
    "    v11 = np.var(tpr2)\n",
    "\n",
    "    # Estimate covariance\n",
    "    cov = np.cov(tpr1, tpr2)[0, 1]\n",
    "\n",
    "    return (v10 + v11 - 2 * cov) / n\n",
    "\n",
    "def delong_roc_test(tpr1, tpr2):\n",
    "    \"\"\"\n",
    "    Performs DeLong test using TPRs, accounting for multiple folds.\n",
    "    \"\"\"\n",
    "    # Assuming tpr1 and tpr2 are 2D arrays where each row is a fold\n",
    "    auc1 = np.mean(tpr1, axis=1)  # AUC for each fold\n",
    "    auc2 = np.mean(tpr2, axis=1)  # AUC for each fold\n",
    "\n",
    "    # Compute the differences in AUC for each fold\n",
    "    auc_diffs = auc1 - auc2\n",
    "\n",
    "    # Compute mean and standard error of the differences\n",
    "    mean_diff = np.mean(auc_diffs)\n",
    "    se_diff = np.std(auc_diffs, ddof=1) / np.sqrt(len(auc_diffs))\n",
    "\n",
    "    z = mean_diff / se_diff\n",
    "    p = 2 * (1 - stats.norm.cdf(abs(z))) # Two-sided test\n",
    "\n",
    "    return z, p, mean_diff, se_diff\n",
    "\n",
    "def perform_delong_test(all_tprs, cohorts, scenarios, estimators, compare_all=False, reference_scenario=None, reference_estimator=None):\n",
    "    check_data_structure(all_tprs, cohorts, scenarios, estimators)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for cohort in cohorts:\n",
    "        cohort_results = []\n",
    "\n",
    "        if compare_all:\n",
    "            # Compare all scenarios with each other\n",
    "            scenario_pairs = list(combinations(scenarios, 2))\n",
    "        else:\n",
    "            # Compare only with the reference scenario\n",
    "            if reference_scenario is None or reference_estimator is None:\n",
    "                raise ValueError(\"reference_scenario and reference_estimator must be provided when compare_all is False\")\n",
    "            scenario_pairs = [(reference_scenario, scenario) for scenario in scenarios if scenario != reference_scenario]\n",
    "\n",
    "        for scenario1, scenario2 in scenario_pairs:\n",
    "            for estimator in estimators:\n",
    "                tpr1 = all_tprs.loc[(cohort, scenario1, slice(None), estimator), :].values\n",
    "                tpr2 = all_tprs.loc[(cohort, scenario2, slice(None), estimator), :].values\n",
    "\n",
    "                # Perform DeLong's test\n",
    "                z, p_value, mean_diff, se_diff = delong_roc_test(tpr1, tpr2)\n",
    "\n",
    "                cohort_results.append({\n",
    "                    'Estimator' : f\"{estimator}\",\n",
    "                    'Model1': f\"{scenario1}\",\n",
    "                    'Model2': f\"{scenario2}\",\n",
    "                    'Z-statistic': np.round(z, 4),\n",
    "                    'p-value': (p_value),\n",
    "                    'Mean AUC Difference': round(mean_diff, 4),\n",
    "                    'SE of Difference': round(se_diff, 4)\n",
    "                })\n",
    "\n",
    "        # Create DataFrame for the cohort\n",
    "        results[cohort] = pd.DataFrame(cohort_results)\n",
    "\n",
    "        # Apply Bonferroni correction\n",
    "        n_tests = len(results[cohort])\n",
    "        results[cohort]['Bonferroni-adjusted p-value'] = np.minimum(results[cohort]['p-value'] * n_tests, 1.0)\n",
    "\n",
    "        # Determine significance after Bonferroni correction\n",
    "        results[cohort]['Significant (α=0.05)'] = results[cohort]['Bonferroni-adjusted p-value'].apply(\n",
    "            lambda x: \"True\" if x < 0.05 else \"False\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def perform_delong_test_custom(all_tprs, cohorts, ref_combo, comparison_combos):\n",
    "    \"\"\"\n",
    "    Compare a reference (scenario, estimator) combination against specific other combinations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_tprs : DataFrame\n",
    "        Multi-indexed DataFrame with TPR values\n",
    "    cohorts : list\n",
    "        List of cohorts to analyze\n",
    "    ref_combo : tuple\n",
    "        Reference (scenario, estimator) combination\n",
    "    comparison_combos : list of tuples\n",
    "        List of (scenario, estimator) combinations to compare against reference\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Results dictionary similar to perform_delong_test\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for cohort in cohorts:\n",
    "        cohort_results = []\n",
    "        ref_scenario, ref_estimator = ref_combo\n",
    "\n",
    "        for comp_scenario, comp_estimator in comparison_combos:\n",
    "            try:\n",
    "                tpr1 = all_tprs.loc[(cohort, ref_scenario, slice(None), ref_estimator), :].values\n",
    "                tpr2 = all_tprs.loc[(cohort, comp_scenario, slice(None), comp_estimator), :].values\n",
    "\n",
    "                z, p_value, mean_diff, se_diff = delong_roc_test(tpr1, tpr2)\n",
    "\n",
    "                cohort_results.append({\n",
    "                    'Reference': f\"{ref_scenario}-{ref_estimator}\",\n",
    "                    'Comparison': f\"{comp_scenario}-{comp_estimator}\",\n",
    "                    'Z-statistic': np.round(z, 4),\n",
    "                    'p-value': p_value,\n",
    "                    'Mean AUC Difference': round(mean_diff, 4),\n",
    "                    'SE of Difference': round(se_diff, 4)\n",
    "                })\n",
    "            except KeyError as e:\n",
    "                print(f\"Skipping {comp_scenario}-{comp_estimator} for {cohort}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Create DataFrame and apply Bonferroni correction\n",
    "        results[cohort] = pd.DataFrame(cohort_results)\n",
    "        if len(results[cohort]) > 0:\n",
    "            n_tests = len(results[cohort])\n",
    "            results[cohort]['Bonferroni-adjusted p-value'] = np.minimum(results[cohort]['p-value'] * n_tests, 1.0)\n",
    "            results[cohort]['Significant (α=0.05)'] = results[cohort]['Bonferroni-adjusted p-value'].apply(\n",
    "                lambda x: \"True\" if x < 0.05 else \"False\"\n",
    "            )\n",
    "    print(\"✅ DeLong tests completed successfully!\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delong_results_all = perform_delong_test(\n",
    "    all_tprs=mapped_tprs,\n",
    "    cohorts=['all'],\n",
    "    scenarios=['TOP75', 'TOP30', 'TOP15', 'AMAP-RFC'],\n",
    "    estimators=['RFC'],\n",
    "    compare_all=True\n",
    ")\n",
    "\n",
    "#Compare TOP15-RFC specifically against linear models\n",
    "linear_scenarios = ['aMAP', 'APRI', 'FIB4', 'NFS', 'Liver cirrhosis']\n",
    "linear_combos = [(scenario, 'linear') for scenario in linear_scenarios]\n",
    "\n",
    "delong_results_top15_vs_linear = perform_delong_test_custom(\n",
    "    all_tprs=mapped_tprs,\n",
    "    cohorts=['all', 'par'],\n",
    "    ref_combo=('TOP15', 'RFC'),\n",
    "    comparison_combos=linear_combos\n",
    ")\n",
    "\n",
    "\n",
    "def save_results_to_excel(results, file_name):\n",
    "    with pd.ExcelWriter(file_name) as writer:\n",
    "        for cohort, df in results.items():\n",
    "            df.to_excel(writer, sheet_name=cohort, index=False)\n",
    "\n",
    "save_results_to_excel(delong_results_all, f\"{path}/HCC/tables/delong_test_results_all.xlsx\")\n",
    "save_results_to_excel(delong_results_top15_vs_linear, f\"{path}/HCC/tables/delong_test_results_top15_vs_linear.xlsx\")\n",
    "\n",
    "\n",
    "# Print results\n",
    "for result_type, delong_results in [(\"All Comparisons\", delong_results_all), (\"Benchmark Comparisons\", delong_results_top15_vs_linear)]:\n",
    "    print(f\"\\n--- {result_type} ---\")\n",
    "    for cohort, results in delong_results.items():\n",
    "        print(f\"\\nDeLong Test Results for {cohort}:\")\n",
    "        print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
