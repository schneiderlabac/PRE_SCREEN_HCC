{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[![](https://mermaid.ink/img/pako:eNp9V9tu4zYQ_RVWTy3gGI5zN4o8tNlegO6m2EWBoDBA0NLI4q5EqiTlrBPk3ztDyhapKDECR5pDzo1nZujnLNcFZKssr4W1d1JsjWjWiuHHS9jfsoVaKngOQsaUaGDFrDMHQYMKau72bSpu-41cd67tHG-Fq1aMvtfqsKazYLhUCK-CuaNO0bZgRsJCODES6QqN3iv4Q7sPiiIxrNSGuQpYLhxstZG5qP1OctAOpttaO_JXG7BM1LXfQ0LLZMn2umOPQjnmNLNiB4Q27FG6yq_Tm6-Quxnb7FkBpehqx6Bp3f6gHHaiXvnvTjipVZ9LXQYrYNDJRqgcNSnmwKIqZ4RUTKiC4S5ZhG3ktwU3eM3YppN1wRthHabu82-__vjTgA0GE-l3XKpEzQfFMVyCcJ0BLpuWb4ShFMQwvfNcGwO132pj0Oiciy7nFAL3EcQoJY4f-PMmwDFmNNBseqLYVyspKrAcc-bX4Z6Cj90cL5yO1VaiRXui3luZ2PG-S7WNZVMqai2KKCS2zkiCBAoaoDjS3h-lAUytssxCXa6zoOQl_FPagefqQRvqIjav1-obQMuQDI1EFa4SyMJKIi0tK6VzaINIgzx6CCnHHQeizA7crYizcQXshJFiU4OdBY3CAO3rlPyvg3qPJGmxECwql0F5xEJv_VFijWwwLNYajYqaHw4BTUXykXqCperJK60t4kY3DOk6Yw-__zJjCjqjFdZfC4UsybzIyam8kg7yigjpE9hgebIQoo_VM23uO4492A-1NbSS56hY9rhZOomxPOHphurrvZz5N9rG_DbKLx0mpoAs-85wzEawIfDPOSM3nQM7WKHP3f2fqeDzP5_4x_u7D399SeWiKPgGVF41wnxLoVzX3HYbX_AjOTJ6a3TXWo6t6h10JzXGli4gbvDQd2Mxnrg2jiMK9CVStJTbiT3YI2jPW46GSfB627SEh2xzosYINZr6ayrE0-hyINtdo0bZN_px0qFBjsbUdgR2CrWpEpspnmk-PlGCo974Ggy59p0oBac8ccJsX8moa-a6wpSOgL4bpdKdtB128XBuLGkmgZ5haEb0H003wlGr9cWKZVeCARxxgTqe9J5LFkddnIyYYNNSbrabN5CYkC-HORbcJdJFzj5Ejxw7IS_KsQQDiEXYohKr_VFbrls_qRIQSdO7FOtNgJSmPdG409xAo3cwCeFRKYuJHMBC5o6_RVZvifZht6WF0SgNO2LnhpY2yPbRY5qAp-gxIIMAz_Zrh2TDvsMf_MDFVMazzs-11Olkzk8io_P0l6foQMONMI2SqNhfsvCyMwL9ReuIVkCtv6TxQo3XAnJb0PQrZQ0T7KfLz_PktSZyCSsufQsj1B_H6DaQXATiZGId8waoZSQ5InG7e-8qFN37-otMcrtIounvQs_r6drBOx0P8UZweDxO4Z9PTl5RKLSI96D5_KhhpO_k5JYNgyI2c-uTH-T05Jf2EUwoOSIjbD6_DcefzbIGMFGywB8m_kzXGV3AYZ2t8LGgyYnD_wXXic7pL3uVZytnOphlWF_b6vDStTTd-h81ByHWHbbFj_2vHuz_cot6WqGy1XP2PVudnC4v52fLs8VycbNYXp9fLq5n2T5bLS-X88XFzenl8uLsfHm1vDp7mWVPWqPi0_nZ1fXV9enN6fnV6eLm5vrCK_zXg6WoLbz8D4O3OsQ?type=png)](https://mermaid.live/edit#pako:eNp9V9tu4zYQ_RVWTy3gGI5zN4o8tNlegO6m2EWBoDBA0NLI4q5EqiTlrBPk3ztDyhapKDECR5pDzo1nZujnLNcFZKssr4W1d1JsjWjWiuHHS9jfsoVaKngOQsaUaGDFrDMHQYMKau72bSpu-41cd67tHG-Fq1aMvtfqsKazYLhUCK-CuaNO0bZgRsJCODES6QqN3iv4Q7sPiiIxrNSGuQpYLhxstZG5qP1OctAOpttaO_JXG7BM1LXfQ0LLZMn2umOPQjnmNLNiB4Q27FG6yq_Tm6-Quxnb7FkBpehqx6Bp3f6gHHaiXvnvTjipVZ9LXQYrYNDJRqgcNSnmwKIqZ4RUTKiC4S5ZhG3ktwU3eM3YppN1wRthHabu82-__vjTgA0GE-l3XKpEzQfFMVyCcJ0BLpuWb4ShFMQwvfNcGwO132pj0Oiciy7nFAL3EcQoJY4f-PMmwDFmNNBseqLYVyspKrAcc-bX4Z6Cj90cL5yO1VaiRXui3luZ2PG-S7WNZVMqai2KKCS2zkiCBAoaoDjS3h-lAUytssxCXa6zoOQl_FPagefqQRvqIjav1-obQMuQDI1EFa4SyMJKIi0tK6VzaINIgzx6CCnHHQeizA7crYizcQXshJFiU4OdBY3CAO3rlPyvg3qPJGmxECwql0F5xEJv_VFijWwwLNYajYqaHw4BTUXykXqCperJK60t4kY3DOk6Yw-__zJjCjqjFdZfC4UsybzIyam8kg7yigjpE9hgebIQoo_VM23uO4492A-1NbSS56hY9rhZOomxPOHphurrvZz5N9rG_DbKLx0mpoAs-85wzEawIfDPOSM3nQM7WKHP3f2fqeDzP5_4x_u7D399SeWiKPgGVF41wnxLoVzX3HYbX_AjOTJ6a3TXWo6t6h10JzXGli4gbvDQd2Mxnrg2jiMK9CVStJTbiT3YI2jPW46GSfB627SEh2xzosYINZr6ayrE0-hyINtdo0bZN_px0qFBjsbUdgR2CrWpEpspnmk-PlGCo974Ggy59p0oBac8ccJsX8moa-a6wpSOgL4bpdKdtB128XBuLGkmgZ5haEb0H003wlGr9cWKZVeCARxxgTqe9J5LFkddnIyYYNNSbrabN5CYkC-HORbcJdJFzj5Ejxw7IS_KsQQDiEXYohKr_VFbrls_qRIQSdO7FOtNgJSmPdG409xAo3cwCeFRKYuJHMBC5o6_RVZvifZht6WF0SgNO2LnhpY2yPbRY5qAp-gxIIMAz_Zrh2TDvsMf_MDFVMazzs-11Olkzk8io_P0l6foQMONMI2SqNhfsvCyMwL9ReuIVkCtv6TxQo3XAnJb0PQrZQ0T7KfLz_PktSZyCSsufQsj1B_H6DaQXATiZGId8waoZSQ5InG7e-8qFN37-otMcrtIounvQs_r6drBOx0P8UZweDxO4Z9PTl5RKLSI96D5_KhhpO_k5JYNgyI2c-uTH-T05Jf2EUwoOSIjbD6_DcefzbIGMFGywB8m_kzXGV3AYZ2t8LGgyYnD_wXXic7pL3uVZytnOphlWF_b6vDStTTd-h81ByHWHbbFj_2vHuz_cot6WqGy1XP2PVudnC4v52fLs8VycbNYXp9fLq5n2T5bLS-X88XFzenl8uLsfHm1vDp7mWVPWqPi0_nZ1fXV9enN6fnV6eLm5vrCK_zXg6WoLbz8D4O3OsQ)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Introduction\n",
            "Welcome to commando central! This is the central resource for  one model execution, from training to evaluation, as illustrated on the image above. \n",
            "Every project will have this resource as one of a couple of major concepts. These concepts include the following\n",
            "-  A .ipynb file to create, load, save, evaluate pipelines (This file)\n",
            "- A .ipynb file that can be seen as the \"collector\" or \"wrapper\" file. This one allows for inter-pipeline comparisons, e.g. to create graphs such as AUROC curves, but also allows to let multiple pipeline instances run simultaneous\n",
            "- A user_input.yaml file to store variables like the name of the current project, the colors of your graphs etc.\n",
            "\n",
            "To start:\n",
            "- Copy the project_template folder and paste it (in the project_code directory). Then rename it as you like. This is your copy of the pipeline code at the time. Be aware:\n",
            "    - The functions are centralized, meaning they will be updated/fixed. Implications: Do not tread lightly when manipulating the functions\n",
            "    - The notebooks are decentralized. Here, you can play around, adapt function calls etc. as you like. While that is cool, it means that you will not necessarily learn isntantly about new features that others might have implemented. So check out the template/other peoples projects from time to time\n",
            "\n",
            "- Initialize a Virtual environment (venv). We have a looot of packages needed for the pipeline. Version conflicts are always an issue. So take the requirements.txt file, choose your preferred venv manager (for non-terminal natives: Anaconda navigator) and configure a venv based on the requirements we provide. \n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "![alt text](<../../modeling_pipeline/Single Model Pipeline.jpg>)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "%pip install -e \"C:\\Users\\janni\\OneDrive\\Dokumente\\GitHub\\modeling_pipeline\\src\""
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Libraries, packages, loading"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Setup pipeline package w/o loading model\n",
            "This chunk is perfect for starters. Load your packages, set up your path (in your user_input.yaml) and skip over the optional steps below"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import sys\n",
            "sys.path.append(\"../../modeling_pipeline\") #Because the project is in a different folder (two levels up), we need to add the path to the sys path\n",
            "sys.path.append(\"../..\")                    #Basically just to reduce error messages rn\n",
            "%load_ext autoreload\n",
            "%autoreload 2\n",
            "\n",
            "from modeling_pipeline.pipeline import *\n",
            "\n",
            "# import the user inputs\n",
            "with open('user_input.yaml') as file:\n",
            "    user_input = yaml.load(file, Loader=yaml.FullLoader)\n",
            "path = pp.userpath(os.environ.get(\"USER\", os.environ.get(\"USERNAME\")), project=user_input[\"project\"]) #You HAVE TO define your project user input etc.\n",
            "user_input[\"path\"] = path # Choose your own project here, only works if you added specific project in user_settings.json\n",
            "\n",
            "pd.options.display.max_rows=100\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Optional: Load preexisting model\n",
            "This chunk sets everything up for you AND loads a pipeline object (from previous work). Will not work if you have never trained a model in the specific project"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import sys\n",
            "sys.path.append(\"../../modeling_pipeline\") #Because the project is in a different folder (two levels up), we need to add the path to the sys path\n",
            "sys.path.append(\"../..\")\n",
            "%load_ext autoreload\n",
            "%autoreload 2\n",
            "\n",
            "#Load our package with classes pipeline, models, pp (preprocessing), plot, and more\n",
            "from pipeline import *\n",
            "import warnings\n",
            "warnings.filterwarnings(\"ignore\", message=\"Trying to unpickle\")\n",
            "\n",
            "# import the user inputs\n",
            "with open('user_input.yaml') as file:\n",
            "    user_input = yaml.load(file, Loader=yaml.FullLoader)\n",
            "user_input[\"path\"] = pp.userpath(\n",
            "    os.environ.get(\"USER\", os.environ.get(\"USERNAME\")), project=user_input[\"project\"]) #You HAVE TO define your project user input etc.\n",
            "path = user_input[\"path\"]\n",
            "  # Choose your own project here, only works if you added specific project in user_settings.json\n",
            "\n",
            "pl=load_Pipeline(user_input[\"path\"] + \"/Models/Pipelines/RFC/Pipeline_HCC_all_Model_TOP15_RFC.joblib\") #Load the pipeline object of your dreams"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Overview of pl object attributes"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ext_val=ExportExtVal(pl) # this inherits from the original class and copies all attributes of the pipeline class, it removes all patient level information for privacy reasons; assure this manually afterwards!\n",
            "\n",
            "\n",
            "ext_val.save()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl = joblib.load(\"D:\\OneDrive - Uniklinik RWTH Aachen\\drive\\public\\projects\\hcc\\Models\\Validation_Objects\\Pipeline_HCC_all_Model_TOP15_RFC_external_val.joblib\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.name"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "analyze_object(ext_val)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.data.X"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.summarize_data()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.trained_model.models"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.trained_model.model_with_info.get(\"model_0\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.calibration.plot_calibration_curve(title=False)\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.save_Pipeline_and_comb_outputs()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.save_Pipeline()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.ohe"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.eval.test_train_pred['val']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.user_input.DOI"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.eval.test_train_pred['val']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.user_input.target"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.data.X"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.user_input[\"target\"] = \"status\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "path"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.model_type"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for chosen_subset in [\"all\", \"par\"]: #Patients at risk or all patients\n",
            "    for chosen_model in [\"Model_A\", \"Model_B\", \"Model_C\", \"Model_D\", \"Model_E\",\"Model_TOP15\" ]:   #\n",
            "        pl = {}\n",
            "        user_input={}\n",
            "        #Set metadata of prediction model\n",
            "        ##################################################################################################################################\n",
            "        user_input[\"export_date_data\"]=\"09_09_2024\"\n",
            "        user_input[\"DOI\"] = \"HCC\"\n",
            "        user_input[\"row_subset\"] = chosen_subset  # \"par\" or \"all\" or \"par_Cirrhosis\"\n",
            "        user_input[\"col_subset\"] = chosen_model\n",
            "        pl=load_Pipeline(path + f\"/Models/Pipelines/RFC/Pipeline_HCC_{chosen_subset}_{chosen_model}_RFC.joblib\") #Change for pipeline you want\n",
            "        pl.evaluation_summary_threshold_dependent(thresholds=np.arange(0.7, 0.29, -0.02), beta=10)\n",
            "        pl.evaluation_summary_independent() #AUROCS and AUPRCS\n",
            "        #pl.save_Pipeline_and_comb_outputs()\n",
            "        #ext_val=export_ext_val(pl)\n",
            "        #ext_val.save('.')\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Loop to Update all pipeline objects"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for chosen_subset in [\"all\", \"par\"]: #Patients at risk or all patients\n",
            "    for chosen_model in [\"Model_A\", \"Model_B\", \"Model_C\", \"Model_D\", \"Model_E\", \"Model_TOP75\", \"Model_TOP30\", \"Model_TOP15\"]:   #\n",
            "        pl = {}\n",
            "        user_input={}\n",
            "        #Set metadata of prediction model\n",
            "        ##################################################################################################################################\n",
            "        user_input[\"export_date_data\"]=\"09_09_2024\"\n",
            "        user_input[\"DOI\"] = \"HCC\"\n",
            "        user_input[\"row_subset\"] = chosen_subset  # \"par\" or \"all\" or \"par_Cirrhosis\"\n",
            "        user_input[\"col_subset\"] = chosen_model\n",
            "        pl=load_Pipeline(path + f\"/Models/Pipelines/RFC/Pipeline_HCC_{chosen_subset}_{chosen_model}_RFC.joblib\") #Change for pipeline you want\n",
            "        pl.save_values_for_combined_plot()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Optional: Load an Ext.val export object \n",
            "Only relevant if you want to use this script on an external validation dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ext_val = load(\"C:/Users/janni/Uniklinik RWTH Aachen/CRC-1382-A11 - public/projects/hcc/Models/Validation_Objects/Pipeline_HCC_all_Model_TOP15_RFC_external_val.joblib\")\n",
            "pl_ext=Pipeline(ext_val_obj=ext_val)\n",
            "pl_ext.columngroups_df\n",
            "pl_ext.external_validation(X_val=pl.data.X_val,y_val=pl.data.y_val)\n",
            "pl_ext.ohe"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# New Pipeline"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Create new user input (for new model)\n",
            "With addition of the user_input.yaml, there are two options to add user-input. The yaml file (preferred), or change it directly in here... Just make sure you are aware that this one overrides the yaml file as both refer to the dictionary \"user_input\"\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Initialize Pipeline, training, models and eval\n",
            "\n",
            "There are multiple things happening upon initialization of the pipeline. Mostly, this is where the data loading happens \"behind closed doors\". To get a better idea, you can look e.g. at the load_data function in pp.py (which stands for preprocessing.py)\n",
            "\n",
            "Data Structure:\n",
            "- Make sure you have a folder with your data in the directory defined with \"userpath\" and user_input.yaml[export_date_data], so the pipeline knows where to look for.\n",
            "- You need 4 dataframes: \n",
            "    1. X_inner -> Training data variables including a unique identifier (eid)\n",
            "    2. y_inner -> Corresponding target information for the training data\n",
            "    3. X_outer -> Validation data variables\n",
            "    4. y_outer -> Validation data target info \n",
            "- And one extra files:\n",
            "    1. columngroups.csv -> Provides some metadata onto which column-ids of your dataset belong to which data source (necessary for modality-wise feature importance later on.) If e.g. you have metabolomics and demographics, pass this info in columngroups.csv.\n",
            "    2. Summary_reports (not actually needed)\n",
            "\n",
            "\n",
            "\n",
            "- Relevant columns:\n",
            "    - split_int: Part of the \"inner\" loop, which is a X-fold cross validation. split_int gives the info how the folds will be organized (best practice is to assign data from one center to one fold only, see TRIPOD Statement). Second best would be time-data separation: Data from 2019 into fold1, data from 2020 into fold2.\n",
            "    - status: Is your target column. You can also assign the target label to any column with the user_input: user_input[\"target\"] = \"status\" #status, #status_cancerreg or whatever you want to call the relevant column\n",
            "    - assessment: Date of UKB visit/data acquisition. Is relevant for time-based metrics as Timepoint 1\n",
            "    - date_of_diag: Date of the first diagnosis of your target. Is relevant for time-based metrics like Kaplan Meier Curves as Timepoint 2\n",
            "\n",
            "    - OPTIONAL: status_cancerreg: Is a relic feature that can come handy to differentiate between ground truth = Electronic health records and ground truth = confirmed cancer register\n",
            "\n",
            "For current convenience, we don't have a complex metadata processing system, but have just added the column subset and row subset information into the naming system. \n",
            "So, a dataframe can be called \n",
            "\n",
            "X_outer_basic_all.csv, with X_outer defining the dataframe function, \"basic\" describing the column subset (yes, this can be improved upon. Feel free!) and \"all\" (or \"par\") to differentiate between dataframes for all UKB participants or a pre-defined subset (Patients at risk, par)\n",
            "\n",
            "All of this is prepared automatically if you use our preprocessing pipeline. If not, you will either have to adapt the pipeline back-end to your needs or manually adjust the dataframes. For an example dataframe setup, look at projects/project_template/data/09_09_2024"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl=Pipeline(user_input)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.data.X"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#pl=Pipeline(user_input)\n",
            "pl.training('RFC') # Starts the training process (Select \"RFC\" or \"XGB\" or \"CatBoost\" or  \"neuronMLP\")\n",
            "pl.build_master_RFC() #Summarizes the 5 models in the master model\n",
            "pl.master_RFC.get_best_params() #Returns the best hyperparameters for the model\n",
            "pl.evaluation()    #Initializes the eval class, making it possible to call the evaluation functions\n",
            "pl.save_Pipeline_and_comb_outputs() #Saves the pipeline and the outputs"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Training Loop\n",
            "This can be used to quickly train multiple instances of pipelines, e.g. a model with the top75, 30, 15 features. The core idea is to put a) the different model options, b) the user input and c) the different pipeline steps seen directly in the cell above, all together. Less understandable, but definitely handy"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for chosen_subset in [\"par\"]:\n",
            "    #for chosen_model in [\"Model_A\", \"Model_B\", \"Model_C\", \"Model_D\", 'Model_E']:   #danach par Model SNP, Model Metabolomics\n",
            "    #for chosen_model in [\"Model_Demographics\", \"Model_Diagnosis\", \"Model_Blood\", \"Model_SNP\", \"Model_Metabolomics\"]:\n",
            "    #for chosen_model in [\"Model_TOP75\", 'Model_TOP30', 'Model_TOP15', \"Model_AMAP-RFC\"]:\n",
            "    for chosen_model in ['Model_A', 'Model_Metabolomics']:   #danach par Model SNP, Model Metabolomics\n",
            "        with open('user_input.yaml') as file:\n",
            "            user_input = yaml.load(file, Loader=yaml.FullLoader)\n",
            "            user_input[\"path\"] = pp.userpath(os.environ.get(\"USER\", os.environ.get(\"USERNAME\")), project=user_input[\"project\"])\n",
            "\n",
            "\n",
            "\n",
            "        #Set metadata of prediction model\n",
            "        ##################################################################################################################################\n",
            "        user_input[\"export_date_data\"]=\"09_09_2024\"\n",
            "        user_input[\"DOI\"] = \"HCC\"\n",
            "        user_input[\"project\"] = \"hcc\"\n",
            "        user_input[\"row_subset\"] = chosen_subset  # \"par\" or \"all\" or \"par_Cirrhosis\"\n",
            "        user_input[\"col_subset\"] = chosen_model\n",
            "\n",
            "\n",
            "        pl=Pipeline(user_input)\n",
            "\n",
            "        pl.training('RFC')\n",
            "        pl.build_master_RFC()\n",
            "        pl.master_RFC.get_best_params()\n",
            "        pl.evaluation()\n",
            "        pl.save_Pipeline_and_comb_outputs(only_val=True)\n",
            "        #ext_val=export_ext_val(pl)\n",
            "        #ext_val.save_to('.')\n",
            "\n",
            "        # Threshold independent metrics (AUCs, AUPRCs, both per model and for the average)\n",
            "        #pl.evaluation_summary_independent()\n",
            "        #Threshold-dependent metrics ('Precision, Recall, Accuracy, F1 Score, F-beta, Balanced accuracy, PPV, NPV\n",
            "        #pl.evaluation_summary_threshold_dependent(thresholds=np.arange(0.7, 0.29, -0.02), beta=10)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for chosen_subset in [\"par\"]:\n",
            "    #for chosen_model in [\"Model_A\", \"Model_B\", \"Model_C\", \"Model_D\", 'Model_E']:   #danach par Model SNP, Model Metabolomics\n",
            "    #for chosen_model in [\"Model_Demographics\", \"Model_Diagnosis\", \"Model_Blood\", \"Model_SNP\", \"Model_Metabolomics\"]:\n",
            "    #for chosen_model in [\"Model_TOP75\", 'Model_TOP30', 'Model_TOP15', \"Model_AMAP-RFC\"]:\n",
            "    for chosen_model in [\"Model_E\", 'Model_A', 'Model_Metabolomics']:   #danach par Model SNP, Model Metabolomics\n",
            "        user_input={}\n",
            "        #Set metadata of prediction model\n",
            "        ##################################################################################################################################\n",
            "        user_input[\"export_date_data\"]=\"09_09_2024\"\n",
            "        user_input[\"DOI\"] = \"HCC\"\n",
            "        user_input[\"project\"] = \"hcc\"\n",
            "        user_input[\"row_subset\"] = chosen_subset  # \"par\" or \"all\" or \"par_Cirrhosis\"\n",
            "        user_input[\"col_subset\"] = chosen_model\n",
            "        user_input[\"test_cohort\"] = \"val\" #applying model to either \"test\" or \"val\" data\n",
            "        user_input[\"visual_export\"] = True #Decide whether you just want to play around or export your visuals\n",
            "        user_input[\"run_conf_matrices\"]=True\n",
            "        user_input[\"run_violin_plot\"]=True\n",
            "        user_input[\"run_feature_imp\"] = True\n",
            "        user_input[\"RUN_MODELS\"] = True  # True Or False, Set on False if just evaluating\n",
            "        user_input[\"add_benchmark\"] = True\n",
            "        user_input[\"target\"] = \"status\" #status, #status_cancerreg or whatever you want to call the relevant column\n",
            "        user_input[\"target_to_validate_on\"] = \"status_cancerreg\"\n",
            "\n",
            "        # define a color scheme:\n",
            "        user_input[\"color_groups_all\"]={\"df_covariates\": '#4995AD', \"df_diagnosis\": '#385579', \"df_blood\": '#C13617', \"df_snp\": '#F0903E', \"df_metabolomics\": '#F0C872', \"TOP75\": '#cb664b',\n",
            "                \"TOP30\": '#ce907f', \"TOP15\": '#c7b8b4'}\n",
            "        user_input[\"color_groups_violin\"]={'Model_A': (0.28627450980392155, 0.5843137254901961, 0.6784313725490196),\n",
            "        'Model_B': (0.2196078431372549, 0.3333333333333333, 0.4745098039215686),\n",
            "        'Model_C': (0.7568627450980392, 0.21176470588235294, 0.09019607843137255),\n",
            "        'Model_Csmall': (0.7568627450980392, 0.21176470588235294, 0.09019607843137255),\n",
            "        'Model_AMAP-RFC': (0.7568627450980392, 0.21176470588235294, 0.09019607843137255),\n",
            "        'Model_D': (0.9411764705882353, 0.5647058823529412, 0.24313725490196078),\n",
            "        'Model_E': (0.9411764705882353, 0.7843137254901961, 0.4470588235294118),\n",
            "        'Model_TOP75': (0.796078431372549, 0.4, 0.29411764705882354),\n",
            "        'Model_TOP30': (0.807843137254902, 0.5647058823529412, 0.4980392156862745),\n",
            "        'Model_TOP15': (0.7803921568627451, 0.7215686274509804, 0.7058823529411765)}\n",
            "\n",
            "        user_input['training']={\"random_state\":1, # set random state to eval. variance in analysis?\n",
            "            \"cross_validation_method\":\"grouped\", #\"stratified\" or \"grouped\"\n",
            "            \"n_splits\":5,\n",
            "            \"n_jobs_indiv\":8,  # choose depending on your machine...\n",
            "            # adjust for the different models\n",
            "            \"hyper_para_options\":{\n",
            "            #'ccp_alpha': 0.0,\n",
            "            #'class_weight': 'balanced_subsample',\n",
            "            #'criterion': 'gini', -> options???\n",
            "            'max_depth':[3],\n",
            "            #'max_features': 'sqrt',\n",
            "            #'max_leaf_nodes': None,\n",
            "            #'max_samples': None,\n",
            "            #'min_impurity_decrease': 0.0,\n",
            "            #'min_samples_leaf': 1,\n",
            "            #'min_samples_split': [2,3,4,5],\n",
            "            #'min_weight_fraction_leaf': 0.0,\n",
            "            'n_estimators':[50],\n",
            "            #'n_jobs': 2,\n",
            "            #'oob_score': False,\n",
            "            #'random_state':np.random.randint(low=0,high=100,size=n_splits).tolist(), #### iterate here\n",
            "            #'verbose': [1],\n",
            "            },\n",
            "            \"scoring_grid_search\":\"balanced_accuracy\",#'roc_auc_ovo_weighted', #'balanced_accuracy', 'roc_auc_ovo_weighted', 'roc_auc_ovr_weighted','f1_weighted', #sklearn.metrics.get_scorer_names()\n",
            "            'verbose':1,\n",
            "        }\n",
            "\n",
            "        pl=Pipeline(user_input)\n",
            "\n",
            "        pl.training('RFC')\n",
            "        pl.build_master_RFC()\n",
            "        pl.master_RFC.get_best_params()\n",
            "        pl.evaluation()\n",
            "        pl.save_Pipeline_and_comb_outputs(only_val=True)\n",
            "        #ext_val=export_ext_val(pl)\n",
            "        #ext_val.save_to('.')\n",
            "\n",
            "        # Threshold independent metrics (AUCs, AUPRCs, both per model and for the average)\n",
            "        pl.evaluation_summary_independent()\n",
            "        #Threshold-dependent metrics ('Precision, Recall, Accuracy, F1 Score, F-beta, Balanced accuracy, PPV, NPV\n",
            "        pl.evaluation_summary_threshold_dependent(thresholds=np.arange(0.7, 0.29, -0.02), beta=10)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Visualizations"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Optional: Load pipeline from here \n",
            "(redundant to pipeline call at start of the script, just for the sake of less scrolling time)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import sys\n",
            "sys.path.append(\"../../modeling_pipeline\") #Because the project is in a different folder (two levels up), we need to add the path to the sys path\n",
            "sys.path.append(\"../..\")\n",
            "%load_ext autoreload\n",
            "%autoreload 2\n",
            "\n",
            "# Shortcut to load new model instance\n",
            "from pipeline import * #Load our package with classes pipeline, models, pp (preprocessing), plot, and more\n",
            "path= pp.userpath(os.environ.get(\"USER\", os.environ.get(\"USERNAME\")), project=\"hcc\") # Choose your own project here, only works if you added specific project in user_settings.json\n",
            "pl=load_Pipeline(user_input[\"path\"] + \"/Models/Pipelines/RFC/Pipeline_HCC_par_Model_A_RFC.joblib\") #Change for pipeline you want\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#TOP 15 Feature Imp Plots\n",
            "for chosen_subset in [\"all\", \"par\"]: #Patients at risk or all patients\n",
            "    for chosen_model in [\"Model_TOP15\"]:   #\n",
            "        pl=load_Pipeline(user_input[\"path\"] + f\"/Models/Pipelines/RFC/Pipeline_HCC_{chosen_subset}_{chosen_model}_RFC.joblib\") #Change for pipeline you want\n",
            "        pl.feature_imp_barplot(n_features=60, bar_height_factor=20, func_for_aggregation=np.mean, fontsize=16, short_names=True, fig_width=3.5, borderpad=1.5, linewidth=1.5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#TOP 30 Feature Imps\n",
            "for chosen_subset in [\"all\", \"par\"]: #Patients at risk or all patients\n",
            "    for chosen_model in [\"Model_C\", \"Model_E\"]:   #\n",
            "        pl=load_Pipeline(path + f\"/Models/Pipelines/RFC/Pipeline_HCC_{chosen_subset}_{chosen_model}_RFC.joblib\") #Change for pipeline you want\n",
            "        pl.feature_imp_barplot(n_features=30, bar_height_factor=20, func_for_aggregation=np.mean, fontsize=20, short_names=True, )"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Optional: Raw Data Exploration"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.data.X_val"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.eval.test_train_pred.get(\"train\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.eval.test_train_pred.get(\"test\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.eval.test_train_pred.get(\"val\").status.value_counts()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.eval.test_train_pred.get(\"val\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.model_type"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.user_input.fig_path"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.user_input.hyperparameters"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### AUC Train Test Val"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Plot the AUROCS for the training and validation data in simple raw fashion\n",
            "pl.roc_auc_test_train(fontsize=14, title=False, border_width=1, save_fig=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.eval.test_train_pred.get(\"val\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Raw prediction values visualization"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.eval.test_train_pred.get(\"val\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plot.create_violin_plot(pl, data=pl.eval.test_train_pred.get(\"val\"), model=pl.master_RFC,ohe=pl.ohe, truth=\"status_cancerreg\", gap=-0.1, width=0.8, thresholds_choice=[0,.35,.55,1], show_thresholds=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#Dump violin object to joblib for the Web-calculator\n",
            "import joblib\n",
            "fig, ax= plot.create_violin_plot(pl, data=pl.eval.test_train_pred.get(\"val\"), model=pl.master_RFC,ohe=pl.ohe, truth=\"status_cancerreg\", gap=-0.1, width=0.8, thresholds_choice=[0,.35,.55,1])\n",
            "\n",
            "joblib.dump(fig, f'{pl.user_input.fig_path}/violin_top15_app.joblib')"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Explainability"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Feature Importance"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plot.feature_imp_barplot(n_features=20, bar_height_factor=30, func_for_aggregation=np.mean, fontsize=25, short_names=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.feature_imp_barplot(n_features=20, bar_height_factor=30, func_for_aggregation=np.mean, fontsize=20, short_names=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#TOP 15 Feature Imp Plots\n",
            "for chosen_subset in [\"all\", \"par\"]: #Patients at risk or all patients\n",
            "    for chosen_model in [\"Model_TOP15\"]:   #\n",
            "        pl=load_Pipeline(user_input[\"path\"] + f\"/Models/Pipelines/RFC/Pipeline_HCC_{chosen_subset}_{chosen_model}_RFC.joblib\") #Change for pipeline you want\n",
            "        plot.shap_analysis(pl, sample_size=20000, max_display=15, fig_size=(12, 6))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Feature Importance Aggregated for Modality\n",
            "Basically only the \"small plot\" from the feature importance. Can be handy, e.g. if you display the big plot just for 1 model, and for the others just this to get an overview"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Change color scheme as needed\n",
            "pl.mapper.color_groups ={\"Demography\\n& Lifestyle\": '#A6CEE3', \"EHR\": '#FFC766', \"Blood count\\n& Serum\": '#FB9A99', \"Genomics\": '#B2DF8A', \"Metabolomics\": '#C79FD6', \"Metadata\": \"#C79FD6\"}\n",
            "\n",
            "# Call function. Make sure to decide whether mean or sum is more appropriate for your data\n",
            "plot.feature_imp_aggregation(pl, func_for_aggregation=np.mean, height_factor=0.8)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Feature Importance Table"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Table for all feature importances\n",
            "pl.master_RFC.feature_importances_()\n",
            "\n",
            "export_feature_importance_to_excel(pl)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### SHAP"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plot.shap_analysis(pl, sample_size=20000, max_display=20, fig_size=(12, 4), short_names=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.plot_correlations(print_columns=False)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Confusion Matrices\n",
            "4 or 6 field tables with x axis= Ground truth, y axis = prediction. Thresholds for classification have to be given as argument"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exports the colorbar for the confusion matrix plots (only needed once for multi-panel, ergo not included as default for the confusion matrices themselves)\n",
            "plot.save_colorbar(pip_self=pl, figsize=(0.5, 6), font_size=22)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Conf Whole population"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#All\n",
            "plot.wrapper_eval_prediction_mono(pip_self=pl,X=pl.data.X_val,y_true=pl.data.z_val[\"status_cancerreg\"],model=pl.master_RFC,thresholds=[0.45, 0.55, 0.35, 0.5, 0.4, 0.3],figsize=(15,10), font_size=22)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plot.wrapper_eval_prediction_mono(pip_self=pl,X=pl.data.X_val,y_true=pl.data.z_val[\"status_cancerreg\"],model=pl.master_RFC,thresholds=[0.45],figsize=(15,10), font_size=22)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for chosen_subset in [\"all\", \"par\"]: #Patients at risk or all patients\n",
            "    for chosen_model in [\"Model_TOP15\"]:   #\n",
            "        pl=load_Pipeline(path + f\"/Models/Pipelines/RFC/Pipeline_HCC_{chosen_subset}_{chosen_model}_RFC.joblib\") #Change for pipeline you want\n",
            "        #All\n",
            "        plot.wrapper_eval_prediction_multi(pip_self=pl,X=pl.data.X_val,y_true=pl.data.z_val[\"status_cancerreg\"],model=pl.master_RFC,thresholds=[(0.4, 0.6), (0.35, 0.55)],incorp_threh_in_y_label=True,figsize=(13,5), n_rows=1, font_size=22)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Conf Stratified"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#Males 6x\n",
            "plot.wrapper_eval_prediction_multi(pip_self=pl,X=pl.data.X_val_sex,y_true=pl.data.y_val_sex,model=pl.master_RFC,thresholds=[(0.35, 0.55)],incorp_threh_in_y_label=True,figsize=(13,5), n_rows=1, font_size=22, stratify={'column': 'SEX', 'value': 1})\n",
            "\n",
            "#Females 6x\n",
            "plot.wrapper_eval_prediction_multi(pip_self=pl,X=pl.data.X_val_sex,y_true=pl.data.y_val_sex,model=pl.master_RFC,thresholds=[(0.35, 0.55)],incorp_threh_in_y_label=True,figsize=(13,5), n_rows=1, font_size=22, stratify={'column': 'SEX', 'value': 0})"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#Males 6x\n",
            "plot.wrapper_eval_prediction_multi(pip_self=pl,X=pl.data.X_val,y_true=pl.data.z_val,model=pl.master_RFC,thresholds=[(0.4, 0.6), (0.35, 0.55)],incorp_threh_in_y_label=True,figsize=(13,5), n_rows=1, font_size=22, stratify={'column': 'SEX', 'value': 1})"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#Females 6x\n",
            "plot.wrapper_eval_prediction_multi(pip_self=pl,X=pl.data.X_val,y_true=pl.data.z_val,model=pl.master_RFC,thresholds=[(0.4, 0.6), (0.35, 0.55)],incorp_threh_in_y_label=True,figsize=(13,5), n_rows=1, font_size=22, stratify={'column': 'SEX', 'value': 0})"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Males 4x\n",
            "plot.wrapper_eval_prediction_mono(pip_self=pl,X=pl.data.X_val,y_true=pl.data.z_val[\"status_cancerreg\"], model=pl.master_RFC,thresholds=[0.45, 0.55, 0.35, 0.5, 0.4, 0.3],\n",
            "    figsize=(15,10), font_size=22, stratify={'column': 'SEX', 'value': 1}  # For males\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Females 4x\n",
            "plot.wrapper_eval_prediction_mono(pip_self=pl,X=pl.data.X_val,y_true=pl.data.z_val[\"status_cancerreg\"], model=pl.master_RFC, thresholds=[0.45, 0.55, 0.35, 0.5, 0.4, 0.3],\n",
            "    figsize=(15,10), font_size=22, stratify={'column': 'SEX', 'value': 0}  # For females\n",
            ")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Kaplan Meier\n",
            "\n",
            "The `plot_KM` function generates **Kaplan-Meier survival curves** to estimate the probability of event-free survival over time. It is commonly used in medical research to analyze **time-to-event data**, such as disease progression or patient survival.\n",
            "\n",
            "- **Survival Probability** is plotted over time (days, months, or years).\n",
            "- **Stepwise Curve** shows when events (e.g., diagnosis, death) occur.\n",
            "- **Censoring Marks** indicate patients lost to follow-up before the event.\n",
            "\n",
            "- Stratifies patients into **risk groups** (Low, Medium, High).\n",
            "- Evaluates **time until ***Blank*** diagnosis** or other clinical outcomes.\n",
            "- Compares survival distributions between different **AI-predicted risk levels**.\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for chosen_subset in [\"all\", \"par\"]: #Patients at risk or all patients\n",
            "    for chosen_model in [\"Model_TOP15\"]:   #\n",
            "        pl=load_Pipeline(path + f\"/Models/Pipelines/RFC/Pipeline_HCC_{chosen_subset}_{chosen_model}_RFC.joblib\") #Change for pipeline you want\n",
            "        #For ideal scale\n",
            "        plot.plot_KaplanMeierRisk(pl, thresholds=[0.35, 0.6], y_scale= \"default\", x_scale='y', font_size=24)\n",
            "\n",
            "        # For actual scale 0 to 1:\n",
            "        plot.plot_KaplanMeierRisk(pl, thresholds=[0.35, 0.6], y_scale= [0, 1], x_scale='y', font_size=24)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### AUC Over Time"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for chosen_subset in [\"all\", \"par\"]: #Patients at risk or all patients\n",
            "    for chosen_model in [\"Model_TOP15\"]:   #\n",
            "        pl=load_Pipeline(path + f\"/Models/Pipelines/RFC/Pipeline_HCC_{chosen_subset}_{chosen_model}_RFC.joblib\") #Change for pipeline you want\n",
            "        # Allows to plot the ROC and PRC for different timeframes. Good for an overview of \"how far into the future\" can we predict?\n",
            "        plot.plot_auc_time(\n",
            "            pl,\n",
            "            target=\"status_cancerreg\",\n",
            "            metric=\"both\",  # \"both\", \"AUROC\", or \"AUPRC\"\n",
            "            plot_figure=True,\n",
            "            export_format=\"svg\",\n",
            "            y1_lim=(0.5,1.0),\n",
            "            y2_lim=(0,0.5),\n",
            "            resample=True,\n",
            "            remarks=\"\",\n",
            "            figsize=(10,5))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Plot Single Trees"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn import tree\n",
            "tree.plot_tree(pl.trained_model.models[0][0].estimator.estimator)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Evaluation Tables\n",
            "### Optional: Load pipeline from here \n",
            "(redundant to pipeline call at start of the script, just for the sake of less scrolling time)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "from pipeline import * #Load our package with classes pipeline, models, pp (preprocessing), plot, and more\n",
            "path= pp.userpath(os.environ.get(\"USER\", os.environ.get(\"USERNAME\")), project=\"hcc\") # Choose your own project here, only works if you added specific project in user_settings.json\n",
            "\n",
            "pl=load_Pipeline(path + \"/Models/Pipelines/RFC/Pipeline_HCC_all_Model_TOP15_RFC.joblib\") #Change for pipeline you want\n",
            "\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Create summary tables for model metrics"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Threshold independent metrics (AUCs, AUPRCs, both per model and for the average)\n",
            "pl.evaluation_summary_independent()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#Threshold-dependent metrics ('Precision, Recall, Accuracy, F1 Score, F-beta, Balanced accuracy, PPV, NPV\n",
            "pl.evaluation_summary_threshold_dependent(thresholds=np.arange(0.7, 0.29, -0.02), beta=10)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Model Calibration"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Run Calibration"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Load pipeline\n",
            "from pipeline import * #Load our package with classes pipeline, models, pp (preprocessing), plot, and more\n",
            "path= pp.userpath(os.environ.get(\"USER\", os.environ.get(\"USERNAME\")), project=\"hcc\") # Choose your own project here, only works if you added specific project in user_settings.json\n",
            "\n",
            "pl=load_Pipeline(path + \"/Models/Pipelines/RFC/Pipeline_HCC_all_Model_TOP15_RFC.joblib\") #Change for pipeline you want"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl_ext = joblib.load(\"D:\\OneDrive - Uniklinik RWTH Aachen\\drive\\public\\projects\\hcc\\Models\\Validation_Objects\\Pipeline_HCC_all_Model_TOP15_RFC_external_val.joblib\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.calibration = CalibrationLayer(pl) #Initializes the calibration class, making it possible to call the calibration functions\n",
            "\n",
            "# Option A: Calibrate on 5fold cv test/train\n",
            "pl.calibration.calibrate_with_cv_on_train(method='sigmoid', n_folds=5) #Calibrate on 5fold train data. Method can be 'sigmoid' or 'isotonic'\n",
            "\n",
            "# Option B: Calibrate on holdout\n",
            "# pl.calibration.calibrate_on_holdout(\n",
            "#     method='sigmoid',  # Options: 'sigmoid' (recommended for rare events) or 'isotonic'\n",
            "# )\n",
            "\n",
            "pl.save_Pipeline() #Store pipeline object including the new calibration class"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Plot Calibration Plot"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Plots an extensive multi-panel calibration figure\n",
            "pl.calibration.plot_calibration_curve(X_eval=pl.data.X_val, y_eval=pl.data.z_val[\"status_cancerreg\"], fontsize=13, dataset_name=\"UKB\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Export the ext val object including the calibration object"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "ext_val=ExportExtVal(pl) # this inherits from the original class and copies all attributes of the pipeline class, it removes all patient level information for privacy reasons; assure this manually afterwards!\n",
            "\n",
            "ext_val.save() #Stores the ext_val including the calibration that was run before"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Optional: Change coloring for plot\n",
            "\n",
            "# pl.calibration.default_plot_colors.update({\n",
            "#             'color_prior': \"#C13617\",\n",
            "#             'edgecolor_prior': 'darkred',\n",
            "#             'color_past': '#385579',\n",
            "#             'edgecolor_past': 'darkblue',\n",
            "#             'control_fill': '#E8E8E8',\n",
            "#             'control_edge': \"#A1A1A1\",\n",
            "#             'cal_marker_color': '#4ECDC4',\n",
            "#             'cal_marker_edge': '#006B66',\n",
            "#             'before_bar': 'lightcoral',\n",
            "#             'after_bar': 'lightgreen'\n",
            "# })"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pl.calibration.get_summary()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Additional Info"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Print input column names per model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data = {}\n",
            "\n",
            "for chosen_subset in [\"all\"]:\n",
            "    for chosen_model in [\"Model_C\", \"Model_TOP75\", \"Model_TOP30\", \"Model_TOP15\", \"Model_AMAP-RFC\"]:   #\n",
            "        user_input={}\n",
            "        #Set metadata of prediction model\n",
            "        ##################################################################################################################################\n",
            "        user_input[\"export_date_data\"]=\"27_05_2024\"\n",
            "        user_input[\"DOI\"] = \"HCC\"\n",
            "        user_input[\"row_subset\"] = chosen_subset  # \"par\" or \"all\" or \"par_Cirrhosis\"\n",
            "        user_input[\"col_subset\"] = chosen_model\n",
            "        pl=load_Pipeline(path + f\"/Models/Pipelines/RFC/Pipeline_HCC_{chosen_subset}_{chosen_model}_RFC.joblib\") #Change for pipeline you want\n",
            "        columns = [col for col in sorted(pl.data.X_val.columns.tolist()) if col not in [\"split_ext\", \"split_int\"]]\n",
            "        data[chosen_model] = columns\n",
            "\n",
            "# Create DataFrame\n",
            "df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in data.items()]))\n",
            "\n",
            "# Save DataFrame to Excel\n",
            "export_path = pl.user_input.fig_path + \"/output.xlsx\"\n",
            "df.to_excel(export_path, index=False)\n",
            "\n",
            "print(df)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Print input column names per reduced model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# List of model options\n",
            "model_options = ['top75', 'top30', 'top15', 'keep_amap']\n",
            "\n",
            "# Create a dictionary to store feature lists for each model\n",
            "feature_lists = {}\n",
            "\n",
            "\n",
            "# Process each model option\n",
            "for model in model_options:\n",
            "    # Filter features where the model column is 1\n",
            "    features = pl.data.reduce_df[pl.data.reduce_df[model] == 1]['Feature'].tolist()\n",
            "    feature_lists[model] = features\n",
            "\n",
            "# Find the maximum length of feature lists\n",
            "max_length = max(len(features) for features in feature_lists.values())\n",
            "\n",
            "# Create a new dataframe with feature lists\n",
            "df_reduced_models = pd.DataFrame({\n",
            "    model: pd.Series(features + [None] * (max_length - len(features)))\n",
            "    for model, features in feature_lists.items()\n",
            "})\n",
            "\n",
            "print(df_reduced_models.head())\n",
            "df_reduced_models.to_excel(f\"{path}/supplement/Reduced_Model_Features.xlsx\")\n",
            "\n",
            "# new_df.to_csv('feature_lists_by_model.csv', index=False)\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "mve12",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
