{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Analysis Center\n",
    "Welcome! This documentation covers the usage of the ROC curve analysis framework consisting of two main components:\n",
    "1. `roc_analysis_project.ipynb` - A project-specific notebook for generating ROC curves that you should copy into your project-code folder\n",
    "2. `wrapper_roc_analysis.py` - A centralized utility script with reusable functions\n",
    "\n",
    "Overview\n",
    "\n",
    "This framework allows you to create, visualize, and statistically compare ROC curves from multiple machine learning models, scenarios, and cohorts. The system is designed with a centralized/distributed architecture:\n",
    "\n",
    "- The **wrapper script** (`wrapper_roc_analysis.py`) contains all core functionality and is maintained centrally\n",
    "- The **notebook** (`roc_analysis_project.ipynb`) can be copied to project-specific directories and customized while importing the centralized utilities\n",
    "\n",
    "Key Features\n",
    "\n",
    "- Create ROC curves for multiple modeling scenarios across different cohorts\n",
    "- Compare multiple estimator types (e.g., RFC vs XGB)\n",
    "- Statistical comparison of ROC curves using DeLong's test\n",
    "- Consistent visual styling with customizable color schemes\n",
    "- Publication-ready figures in vector format (SVG)\n",
    "\n",
    "Getting Started\n",
    "\n",
    "- Prerequisites\n",
    "\n",
    "The framework requires a series of Python packages, including pandas, numpy, matplotlib, scikit-learn, scipy, seaborn, pyyaml\n",
    "Additionally, it expects a project structure with model outputs in specific locations as defined in the notebook.\n",
    "\n",
    "- Basic Usage\n",
    "\n",
    "1. Copy the `roc_analysis_project.ipynb` notebook to your project directory\n",
    "2. Ensure the `wrapper_roc_analysis.py` script is accessible in your Python path\n",
    "3. Update file paths in the notebook to match your project structure\n",
    "4. Run the notebook cells to generate ROC curves for your specific models\n",
    "\n",
    "- Data Structure\n",
    "\n",
    "The framework expects TPR (True Positive Rate) data in a specific format:\n",
    "- An Excel/CSV file containing TPR values from cross-validation\n",
    "- Data should be structured with columns representing different model configurations\n",
    "- Column names should follow a specific naming convention: `{cohort}_{fold}_{scenario}_model{model_number}`\n",
    "\n",
    "- Key Functions\n",
    "\n",
    "### From `wrapper_roc_analysis.py`\n",
    "\n",
    "#### Visualization Functions\n",
    "\n",
    "- `plot_roc_curve()`: Creates a single ROC curve\n",
    "- `plot_rocs()`: Plots multiple ROC curves with mean and standard deviation\n",
    "- `plot_rocs_wrapper()`: Higher-level function to plot ROC curves for multiple scenarios\n",
    "- `plot_rocs_multi_estimator()`: Compares ROC curves across different estimator types\n",
    "- `plot_colorbar()`: Creates a color legend for scenario visualization\n",
    "\n",
    "#### Statistical Functions\n",
    "\n",
    "- `perform_delong_test()`: Executes DeLong's test to compare AUCs\n",
    "- `delong_roc_test()`: Core implementation of DeLong's test\n",
    "- `delong_roc_variance()`: Computes variance for DeLong's test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "sys.path.append(\"../../modeling_pipeline\") #Because the project is in a different folder (two levels up), we need to add the path to the sys path\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pipeline import * #Load our package with classes pipeline, models, pp (preprocessing), plot, and more\n",
    "from wrapper_roc_analysis import * #Load our wrapper for the ROC analysis\n",
    "\n",
    "#This allows us to automatically reload the packages we are working on in the background, no \"Restart Kernel\" needed\n",
    "\n",
    "\n",
    "\n",
    "############### CHANGE THIS ############\n",
    "path= pp.userpath(os.environ.get(\"USER\", os.environ.get(\"USERNAME\")), project=\"hcc\") # Choose your own project here, only works if you added specific project in user_settings.json\n",
    "############### CHANGE THIS ############\n",
    "\n",
    "\n",
    "fig_path = f\"{path}/visuals\"\n",
    "auroc_path = f\"{fig_path}/AUROCs\"\n",
    "if not os.path.exists(auroc_path):\n",
    "    os.makedirs(auroc_path)\n",
    "\n",
    "# Load the default color dictionary\n",
    "yaml_colors_path = \"custom_colors.yaml\"\n",
    "with open(yaml_colors_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "scenarios_colors = config.get(\"scenarios_colors\", {}) # Extract the color dictionary\n",
    "print(\"Successfully loaded color dictionary with\", len(scenarios_colors), \"entries\")\n",
    "\n",
    "scenario_lists = config.get(\"scenario_lists\", {}) # Extract the color dictionary\n",
    "print(\"Successfully loaded scenario list with\", len(scenario_lists), \"entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Color Schemes\n",
    "\n",
    "The framework uses a YAML file (`default_colors.yaml`) to define color schemes for different scenarios:\n",
    "\n",
    "```yaml\n",
    "scenarios_colors:\n",
    "  A: '#8A2BE2'  # BlueViolet\n",
    "  B: '#FF7F50'  # Coral\n",
    "  C: '#20B2AA'  # LightSeaGreen\n",
    "  # More colors...\n",
    "\n",
    "### Scenarios\n",
    "\n",
    "You can define different scenarios/constellations to plot in one model by changing these variables\n",
    "scenario_lists:\n",
    "  incremental: ['A', 'B', 'C', 'D', 'E']\n",
    "  separate: ['Demographics', 'Diagnosis', 'Blood', 'SNP', 'Metabolomics']\n",
    "  # More scenario groups...\n",
    "```\n",
    "\n",
    "You can also define custom colors directly in the notebook:\n",
    "\n",
    "```python\n",
    "my_colors = {\n",
    "    'A': '#8A2BE2',  # BlueViolet\n",
    "    'B': '#FF7F50',  # Coral\n",
    "    # More colors...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import for trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Load and process manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Raw load (Optional)\n",
    "# tprs_joblib_path = os.path.join(path + \"/Models/Pipelines/RFC/combined_output/val/TPRS_combined.joblib\")\n",
    "\n",
    "# tprs = joblib.load(tprs_joblib_path)\n",
    "# tprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the tprs\n",
    "# tprs=pd.read_excel(path+'/Models/Pipelines/'+model_type+'/combined_output/val/TPRS_combined.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "# columns=tprs.columns.tolist()\n",
    "# mapper=pd.DataFrame({'col_names':columns})\n",
    "# mapper[\"estimator\"] = model_type\n",
    "# mapper['cohort']=[i.split('_')[0] for i in mapper.col_names]\n",
    "# mapper['scenario']=[i.split('_')[2] for i in mapper.col_names]\n",
    "# mapper['model']=[i.split('_model')[1] for i in mapper.col_names]\n",
    "# mapper.set_index('col_names',inplace=True)\n",
    "# tprs.transpose()\n",
    "# mapped_tprs=pd.concat([mapper,tprs.transpose()],axis=1).set_index(['cohort','scenario','model', 'estimator'])\n",
    "# mapped_tprs.groupby(level=['cohort','scenario']).agg('mean').transpose()\n",
    "# mapped_tprs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B (recommended): Automatic load via pre-configured joblib and mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proper, processed load\n",
    "model_type = \"RFC\"\n",
    "mapped_tprs = load_tprs(path, [model_type], drop_special_models=\"Sensitivity\")\n",
    "\n",
    "mapped_tprs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import for literature benchmarks\n",
    "You can call them separately in the plot AUROCs, but it is better to add them to the mapped tprs at one point, especially for the Delonge Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Create mask dictionaries \n",
    "At one point you will need dictionaries defining all eids and PAR eids, both for the whole dataset as well as for validation/testing only. If not created until now, you can load lists that include them and store it as JSON for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary with all patients if not done so before\n",
    "# par_eids = pd.read_csv(path +'/data/09_09_2024/par_eids.csv')[\"x\"]\n",
    "# par_eids\n",
    "\n",
    "# all_eids = pd.read_csv(path+ '/data/dataframes/df_covariates.csv')[\"eid\"]\n",
    "# all_eids\n",
    "\n",
    "\n",
    "# cohort_eids_dict = {\n",
    "#     \"all\": list(all_eids),\n",
    "#     \"par\": list(par_eids)\n",
    "# }\n",
    "\n",
    "# #Print summary\n",
    "# print(f\"Cohort 'all': {len(cohort_eids_dict['all'])} patients\")\n",
    "# print(f\"Cohort 'par': {len(cohort_eids_dict['par'])} patients\")\n",
    "# print(f\"Overlap: {len(set(cohort_eids_dict['all']).intersection(set(cohort_eids_dict['par'])))} patients\")\n",
    "\n",
    "# # Step 2: Save the dictionary to a file\n",
    "# with open(path + \"/data/cohort_dict_all.json\", 'w') as f:\n",
    "#     json.dump(cohort_eids_dict, f, indent=2)\n",
    "# print(\"Saved cohort_eids_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the benchmark data (usually not in TPRS format but just a dataframe, prediction and ground truth\n",
    "#benchmark_par=pd.read_csv(path+'/Models/df_amap_par.csv') # Either as ONE Benchmark (change this or the next line, these are the default benchmarks in the plots) #PAR = Patients at risk\n",
    "#benchmark_all=pd.read_csv(path+'/Models/df_amap.csv') #All = For ALL Patients\n",
    "benchmarks= pd.read_csv(path+'/data/dataframes/df_benchmark.csv') #or as dataframe with multiple benchmarks\n",
    "\n",
    "# Load the cohort dictionary for subsetting of benchmarks according to the cohort names (e.g. \"All\" or \"PAR\")\n",
    "# This is needed to create the benchmark_dict, which is a dictionary of dataframes with the benchmark data for each cohort\n",
    "with open(path + \"/data/cohort_dict_test.json\", 'r') as f:\n",
    "            cohort_dict_test= json.load(f)\n",
    "with open(path + \"/data/cohort_dict_all.json\", 'r') as f:\n",
    "            cohort_dict_all= json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "#For dedicated cohorts (e.g. \"PAR\" or \"All\"), we need to create a dictionary with the filtered benchmark data for each cohort\n",
    "benchmark_dict = create_benchmark_dict_from_master(benchmarks, cohort_dict_all)\n",
    "\n",
    "benchmark_prot_dict = create_benchmark_dict_from_master(benchmarks, cohort_dict_all, required_non_na_cols='AFP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only select rows where proteomics data is available (= non-NA for AFP)\n",
    "benchmarks_proteomics = benchmarks[~benchmarks[\"AFP\"].isna()]\n",
    "\n",
    "benchmarks_proteomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the scores you want to use (need to be represented as column names in the benchmarks df and, accordingly in the benchmark_dict dataframes)\n",
    "benchmark_names = [\"aMAP\", \"APRI\", \"FIB4\", \"NFS\", \"LiverRisk\", \"Liver cirrhosis\", \"AFP\"]\n",
    "\n",
    "# Add benchmarks to mapped_tprs\n",
    "mapped_tprs = add_benchmarks_to_mapped_tprs(\n",
    "    mapped_tprs=mapped_tprs,\n",
    "    benchmark_dict=benchmark_dict,\n",
    "    benchmark_names=benchmark_names,  # Optional: specify which benchmarks to include\n",
    "    n_folds=5\n",
    ")\n",
    "\n",
    "# Verify the structure\n",
    "print(\"\\nUpdated mapped_tprs index levels:\")\n",
    "for i, level in enumerate(mapped_tprs.index.names):\n",
    "    unique_values = mapped_tprs.index.get_level_values(i).unique()\n",
    "    print(f\"  Level {i} ({level}): {unique_values.tolist()}\")\n",
    "\n",
    "# Check that benchmarks were added correctly\n",
    "benchmark_indices = mapped_tprs.index[\n",
    "    mapped_tprs.index.get_level_values('estimator') == 'linear'\n",
    "]\n",
    "print(f\"\\nAdded {len(benchmark_indices)} benchmark entries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `perform_delong_test()`: Executes DeLong's test to compare AUCs\n",
    "- `delong_roc_test()`: Core implementation of DeLong's test\n",
    "- `delong_roc_variance()`: Computes variance for DeLong's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for all * all comparisons\n",
    "delong_results_all = perform_delong_test(\n",
    "    all_tprs=mapped_tprs,\n",
    "    cohorts=['all', 'par'],\n",
    "    scenarios=['C', 'TOP75', 'TOP30', 'TOP15', 'AMAP-RFC'],\n",
    "    estimators=['RFC'],\n",
    "    compare_all=True\n",
    ")\n",
    "\n",
    "# Usage for comparing against a reference scenario\n",
    "delong_results_ref = perform_delong_test(\n",
    "    all_tprs=mapped_tprs,\n",
    "    cohorts=['all', 'par'],\n",
    "    scenarios=['A', 'B', 'C', 'D', 'E'],\n",
    "    estimators=['RFC'],\n",
    "    compare_all=False,\n",
    "    reference_scenario='C',\n",
    "    reference_estimator='RFC'\n",
    ")\n",
    "\n",
    "#Compare TOP15-RFC specifically against linear models\n",
    "linear_scenarios = ['aMAP', 'APRI', 'FIB4', 'NFS', 'Liver cirrhosis']\n",
    "linear_combos = [(scenario, 'linear') for scenario in linear_scenarios]\n",
    "\n",
    "delong_results_top15_vs_linear = perform_delong_test_custom(\n",
    "    all_tprs=mapped_tprs,\n",
    "    cohorts=['all', 'par'],\n",
    "    ref_combo=('TOP15', 'RFC'),\n",
    "    comparison_combos=linear_combos\n",
    ")\n",
    "\n",
    "\n",
    "def save_results_to_excel(results, file_name):\n",
    "    with pd.ExcelWriter(file_name) as writer:\n",
    "        for cohort, df in results.items():\n",
    "            df.to_excel(writer, sheet_name=cohort, index=False)\n",
    "\n",
    "save_results_to_excel(delong_results_all, f\"{path}/tables/delong_test_results_all.xlsx\")\n",
    "save_results_to_excel(delong_results_ref, f\"{path}/tables/delong_test_results_reference.xlsx\")\n",
    "save_results_to_excel(delong_results_top15_vs_linear, f\"{path}/tables/delong_test_results_top15_vs_linear.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "# Print results\n",
    "for result_type, delong_results in [(\"All Comparisons\", delong_results_all), (\"Reference Comparisons\", delong_results_ref), (\"Benchmark Comparisons\", delong_results_top15_vs_linear)]:\n",
    "    print(f\"\\n--- {result_type} ---\")\n",
    "    for cohort, results in delong_results.items():\n",
    "        print(f\"\\nDeLong Test Results for {cohort}:\")\n",
    "        print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_scenarios = ['aMAP', 'APRI', 'FIB4', 'NFS', 'Cirrhosis']\n",
    "linear_combos = [(scenario, 'linear') for scenario in linear_scenarios]\n",
    "\n",
    "delong_results_amap_rfc_vs_linear = perform_delong_test(\n",
    "    all_tprs=mapped_tprs,\n",
    "    cohorts=['all', 'par'],\n",
    "    scenarios=['aMAP', 'APRI', 'FIB4', 'NFS', 'Cirrhosis'],\n",
    "    estimators=['linear'],\n",
    "    compare_all=False,\n",
    "    reference_scenario='AMAP-RFC',\n",
    "    reference_estimator='RFC'\n",
    ")\n",
    "save_results_to_excel(delong_results_amap_rfc_vs_linear, f\"{path}/tables/delong_test_results_amap_rfc_vs_linear.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dataframes.items():\n",
    "    print(f\"{key}: {len(df)} rows (from {key})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUROCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Workflows in the Notebook\n",
    "\n",
    "The notebook contains several example workflows, applied to a variety of model constellations (e.g. incremental modalities, i.e from demographics to added diagnosis, added blood, added omics), iterative feature reduction with alpha-level indicating the amount of features, or assessing a number of different benchmarks\n",
    "\n",
    "1. **Basic ROC comparison**: Compare ROC curves from different model scenarios\n",
    "   ```python\n",
    "   fig, ax = plt.subplots(figsize=(6, 5))\n",
    "   plot_roc_curve(test_scores=benchmarks[\"aMAP\"], true_labels=benchmarks.status, ax=ax)\n",
    "   plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['incremental'], 'all',\n",
    "                      title='All Patients', fig_type=\"AUROCS_combined\")\n",
    "   ```\n",
    "\n",
    "2. **Literature benchmark comparison**: Compare your models against published benchmarks\n",
    "   ```python\n",
    "   fig, ax = plt.subplots(figsize=(6, 5))\n",
    "   plot_roc_curve(test_scores=benchmarks[\"aMAP\"], true_labels=benchmarks.status, ax=ax, label=\"aMAP\")\n",
    "   plot_roc_curve(test_scores=benchmarks[\"APRI\"], true_labels=benchmarks.status, ax=ax, label=\"APRI\")\n",
    "   plot_roc_curve(test_scores=benchmarks[\"FIB4\"], true_labels=benchmarks.status, ax=ax, label=\"FIB4\")\n",
    "   plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['c'], 'all',\n",
    "                      title='Literature Benchmark', fig_type=\"AUROCS_combined\")\n",
    "   ```\n",
    "\n",
    "\n",
    "3. **Multiple estimator comparison**: Compare different model types (RFC vs XGB)\n",
    "   ```python\n",
    "   plot_rocs_multi_estimator(\n",
    "       all_tprs=all_tprs,\n",
    "       scenario_list='incremental',\n",
    "       cohorts=['all'],\n",
    "       scenarios_colors=scenarios_colors,\n",
    "       n_splits=5,\n",
    "       fig_path=auroc_path,\n",
    "       title='All Patients',\n",
    "       fig_type=\"AUROCS_multi_Estimator\"\n",
    "   )\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up colors. Per default, the colors in default_colors.yaml will be used. But you can also define your own colors here. To use \"my_colors, change the color argument in the AUROC function you want to adapt\"\n",
    "\n",
    "my_colors = {\n",
    "    'A': '#8A2BE2',  # BlueViolet\n",
    "    'B': '#FF7F50',  # Coral\n",
    "    'C': '#20B2AA',  # LightSeaGreen\n",
    "    'D': '#9932CC',  # DarkOrchid\n",
    "    'E': '#FF8C00',  # DarkOrange\n",
    "    'Demographics': '#4B0082',  # Indigo\n",
    "    'Diagnosis': '#FFA07A',  # LightSalmon\n",
    "    'Blood': '#00CED1',  # DarkTurquoise\n",
    "    'SNP': '#BA55D3',  # MediumOrchid\n",
    "    'Metabolomics': '#F08080',  # LightCoral\n",
    "}\n",
    "\n",
    "plot_colorbar(scenario_lists['incremental'])\n",
    "plot_colorbar(scenario_lists['separate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Imputation Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['no_impute'], 'all',\n",
    "                    title='Literature Benchmark (All UKB)', fig_type=\"AUROCS_no_impute\", fig_path=fig_path)\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['no_impute'], 'par',\n",
    "                    title='Literature Benchmark (All UKB)', fig_type=\"AUROCS_no_impute\", fig_path=fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFP Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row_subsets = [\"all\", \"par\"]\n",
    "for row_subset in row_subsets:\n",
    "\n",
    "    # All AFP Benchmark\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    plot_roc_curve(test_scores=benchmark_prot_dict[row_subset][\"aMAP\"], true_labels=benchmark_prot_dict[row_subset].status, ax=ax, label=\"aMAP\", linestyle=\"-\", fig_path=fig_path)\n",
    "    plot_roc_curve(test_scores=benchmark_prot_dict[row_subset][\"AFP\"], true_labels=benchmark_prot_dict[row_subset].status, ax=ax, label=\"AFP\", linestyle=\"-\", fig_path=fig_path, color='#4895ad')\n",
    "\n",
    "    plot_roc_curve(test_scores=benchmark_prot_dict[row_subset][\"Liver cirrhosis\"], true_labels=benchmark_prot_dict[row_subset].status, ax=ax, label=\"Cirrhosis\", linestyle=\"-\", color= \"#385579\")\n",
    "    plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['c'], row_subset,\n",
    "                    title=f'AFP Benchmark (UKB) {row_subset}', fig_type=\"AUROCS_combined\", fig_path=fig_path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All AFP Benchmark\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmarks_proteomics[\"aMAP\"], true_labels=benchmarks_proteomics.status, ax=ax, label=\"aMAP\", linestyle=\"-\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmarks_proteomics[\"AFP\"], true_labels=benchmarks_proteomics.status, ax=ax, label=\"AFP\", linestyle=\"--\", fig_path=fig_path)\n",
    "\n",
    "plot_roc_curve(test_scores=benchmarks_proteomics[\"Liver cirrhosis\"], true_labels=benchmarks_proteomics.status, ax=ax, label=\"Cirrhosis\", linestyle=\"-\", color= \"#385579\")\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['c'], 'par',\n",
    "                   title='AFP Benchmark (UKB)', fig_type=\"AUROCS_combined\", fig_path=fig_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literature Score comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['benchmarks'], 'all',\n",
    "                    title='Literature Benchmark (All UKB)', fig_type=\"AUROCS_combined\", fig_path=fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['benchmarks'], 'par',\n",
    "                    title='Literature Benchmark (All UKB)', fig_type=\"AUROCS_combined\", fig_path=fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmarks[\"aMAP\"], true_labels=benchmarks.status, ax=ax, label=\"aMAP\", linestyle=\"-\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmarks[\"APRI\"], true_labels=benchmarks.status, ax=ax, label=\"APRI\", linestyle=\"--\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmarks[\"FIB4\"], true_labels=benchmarks.status, ax=ax, label=\"FIB4\", linestyle=\"-.\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmarks[\"NFS\"], true_labels=benchmarks.status, ax=ax, label=\"NFS\", linestyle=\":\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmarks[\"LiverRisk\"], true_labels=benchmarks.status, ax=ax, label=\"LiverRisk\", linestyle=\"dashed\", color= \"#385579\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"Liver cirrhosis\"], true_labels=benchmarks.status, ax=ax, label=\"Cirrhosis\", linestyle=\"-\", color= \"#385579\")\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['c'], 'all',\n",
    "                title='Literature Benchmark (All UKB)', fig_type=\"AUROCS_combined\", fig_path=fig_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmarks[\"aMAP\"], true_labels=benchmarks.status, ax=ax, label=\"aMAP\", linestyle=\"-\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmarks[\"APRI\"], true_labels=benchmarks.status, ax=ax, label=\"APRI\", linestyle=\"--\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmarks[\"FIB4\"], true_labels=benchmarks.status, ax=ax, label=\"FIB4\", linestyle=\"-.\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmarks[\"NFS\"], true_labels=benchmarks.status, ax=ax, label=\"NFS\", linestyle=\":\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmarks[\"Liver cirrhosis\"], true_labels=benchmarks.status, ax=ax, label=\"Cirrhosis\", linestyle=\"-\", color= \"#385579\")\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['c'], 'all',\n",
    "                   title='Literature Benchmark (All UKB)', fig_type=\"AUROCS_combined\", fig_path=fig_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"par\"][\"aMAP\"], true_labels=benchmark_dict[\"par\"].status, ax=ax, label=\"aMAP\", linestyle=\"-\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"par\"][\"APRI\"], true_labels=benchmark_dict[\"par\"].status, ax=ax, label=\"APRI\", linestyle=\"--\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"par\"][\"FIB4\"], true_labels=benchmark_dict[\"par\"].status, ax=ax, label=\"FIB4\", linestyle=\"-.\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"par\"][\"NFS\"], true_labels=benchmark_dict[\"par\"].status, ax=ax, label=\"NFS\", linestyle=\":\", fig_path=fig_path)\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"par\"][\"LiverRisk\"], true_labels=benchmark_dict[\"par\"].status, ax=ax, label=\"LiverRisk\", linestyle=\"dashed\", color= \"#385579\")\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"par\"][\"Liver cirrhosis\"], true_labels=benchmark_dict[\"par\"].status, ax=ax, label=\"Cirrhosis\", linestyle=\"-\", color= \"#385579\")\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['c'], 'all',\n",
    "                   title='Literature Benchmark (PAR UKB)', fig_type=\"AUROCS_combined\", fig_path=fig_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined AUROCs (Incremental) for one estimator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "colors = viridis(np.linspace(0, 1, 4))\n",
    "plot_roc_curve(test_scores=benchmarks[\"aMAP\"], true_labels=benchmarks.status, ax=ax, label=\"aMAP\", color=colors[0])\n",
    "plot_roc_curve(test_scores=benchmarks[\"APRI\"], true_labels=benchmarks.status, ax=ax, label=\"APRI\", color=colors[1])\n",
    "plot_roc_curve(test_scores=benchmarks[\"FIB4\"], true_labels=benchmarks.status, ax=ax, label=\"FIB4\", color=colors[2])\n",
    "plot_roc_curve(test_scores=benchmarks[\"NFS\"], true_labels=benchmarks.status, ax=ax, label=\"NFS\", color=colors[3])\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['c'], 'all',\n",
    "                   title='Literature Benchmark', fig_type=\"AUROCS_combined\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greyscale colouring\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmarks[\"aMAP\"], true_labels=benchmarks.status, ax=ax, label=\"aMAP\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"APRI\"], true_labels=benchmarks.status, ax=ax, label=\"APRI\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"FIB4\"], true_labels=benchmarks.status, ax=ax, label=\"FIB4\")\n",
    "plot_roc_curve(test_scores=benchmarks[\"NFS\"], true_labels=benchmarks.status, ax=ax, label=\"NFS\")\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['incremental'], 'all',\n",
    "                   title='Literature Benchmark', fig_type=\"AUROCS_combined\", fig_path=fig_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"par\"][\"aMAP\"], true_labels=benchmark_dict[\"par\"].status, ax=ax)\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['incremental'], 'par', scenarios_colors=scenarios_colors,\n",
    "                   title=\"\", fig_type=\"AUROCS_combined\", fig_path=fig_path, linewidth=1.5, font_size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"all\"][\"aMAP\"], true_labels=benchmark_dict[\"all\"].status, ax=ax)\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['incremental'], 'all', scenarios_colors=scenarios_colors,\n",
    "                   title=\"\", fig_type=\"AUROCS_combined\", fig_path=fig_path, linewidth=1.5, font_size=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separately trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"par\"][\"aMAP\"], true_labels=benchmark_dict[\"par\"].status, ax=ax)\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['separate'], 'par',\n",
    "                   title='Chronic Liver Disease', fig_type=\"AUROCS_separately\", fig_path=fig_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"all\"][\"aMAP\"], true_labels=benchmark_dict[\"all\"].status, ax=ax)\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['separate'], 'all',\n",
    "                   title='All', fig_type=\"AUROCS_separately\", fig_path=fig_path)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(6, 5))\n",
    "# plot_roc_curve(test_scores=benchmark_all[\"aMAP\"], true_labels=benchmark_all.status, ax=ax)\n",
    "# plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['small'], 'all',\n",
    "#                    title=\"All Patients - Small Models\", fig_type=\"AUROCS_small_models\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"all\"][\"aMAP\"], true_labels=benchmark_dict[\"all\"].status, ax=ax)\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenarios=scenario_lists['small'], cohort='all', scenarios_colors=scenarios_colors,\n",
    "                   title=\"All - Small Models\", fig_type=\"AUROCS_small_models\", fig_path=fig_path, linewidth=1.5, font_size=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_roc_curve(test_scores=benchmark_dict[\"par\"][\"aMAP\"], true_labels=benchmark_dict[\"par\"].status, ax=ax)\n",
    "plot_rocs_wrapper(mapped_tprs, fig, ax, scenario_lists['small_par'], 'par',\n",
    "                   title=\"Patients at Risk - Small Models\", fig_type=\"AUROCS_small_models\", fig_path=fig_path, linewidth=1.5, font_size=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Model AUROCs (5 fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For all 5 scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "\n",
    "# Loop through each scenario in the scenarios_colors dictionary, and for each, plot the five different ROC turves obtained from five fold cross validation\n",
    "# PAR\n",
    "for scenario in scenario_lists[\"c\"]:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6.5))\n",
    "    tprs_data = mapped_tprs.loc[('par', scenario)].values\n",
    "    plot_rocs(tprs=tprs_data, fig=fig, ax=ax,\n",
    "              scenario=scenario,\n",
    "              plot_all=True, fill_bet=True,\n",
    "              title=f'Chronic liver disease - Scenario {scenario}',\n",
    "              fig_type=\"AUROC_sep\",\n",
    "              individual_alpha=0.5, #for the five-fold lines\n",
    "              individual_color=\"grey\",\n",
    "              individual_lw=1.5, #for the five-fold lines\n",
    "              mean_lw=2.5,   #for the mean line\n",
    "              fig_path=fig_path,\n",
    "              col_line=scenarios_colors[scenario],\n",
    "              font_size=20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL\n",
    "n_splits = 5\n",
    "for scenario in scenario_lists[\"incremental\"]:\n",
    "    fig_all, ax_all =plt.subplots()\n",
    "    tprs_data = mapped_tprs.loc[('all', scenario)].values\n",
    "    plot_rocs(tprs=tprs_data, fig=fig_all, ax=ax_all,\n",
    "              scenario=scenario, plot_all=True,fill_bet=True,\n",
    "              title=f\"All - Scenario {scenario}\", fig_type=\"AUROC_sep\",\n",
    "              individual_alpha=0.5, #for the five-fold lines\n",
    "              individual_color=\"grey\",\n",
    "              individual_lw=1.5, #for the five-fold lines\n",
    "              mean_lw=2.5, #for the mean line)\n",
    "              col_line=scenarios_colors[scenario],\n",
    "              fig_path=fig_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Estimators Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import: Multiple TPRS Files (e.g. for comparison RFC vs XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [\"XGB\", \"RFC\", \"CatBoost\", \"neuronMLP\"]\n",
    "line_styles = {\n",
    "    'XGB': '--',\n",
    "    'RFC': '-',\n",
    "    'CatBoost': ':',\n",
    "    'Log_l1': (0, (5, 5)),  # example dash tuple\n",
    "    'NeuronMLP': (0, (3, 5, 1, 5)) # Dashdotdotted\n",
    "\n",
    "\n",
    "    # ... other estimators if needed\n",
    "}\n",
    "\n",
    "base_path = path + '/Models/Pipelines/'\n",
    "all_tprs = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for model_type in model_types:\n",
    "    # File paths\n",
    "    tprs_joblib_path = os.path.join(base_path, model_type, \"combined_output/val/TPRS_combined.joblib\")\n",
    "    tprs_excel_path = os.path.join(base_path, model_type, \"combined_output/val/TPRS_combined.xlsx\")\n",
    "\n",
    "    # Try joblib first, fallback to Excel\n",
    "    if os.path.exists(tprs_joblib_path):\n",
    "        tprs = joblib.load(tprs_joblib_path)\n",
    "        print(f\"Loaded joblib TPRs for {model_type}\")\n",
    "    elif os.path.exists(tprs_excel_path):\n",
    "        tprs = pd.read_excel(tprs_excel_path)\n",
    "        print(f\"Loaded Excel TPRs for {model_type}\")\n",
    "    else:\n",
    "        print(f\"No TPR file found for {model_type} in either format.\")\n",
    "        continue  # Skip to next model\n",
    "\n",
    "    # Rename columns to remove '_met' suffix if present\n",
    "    try:\n",
    "        columns = [col.replace('_met', '') for col in tprs.columns]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing column names for {model_type}: {e}\")\n",
    "        columns = tprs.columns  # fallback\n",
    "\n",
    "    tprs.columns = columns\n",
    "\n",
    "    mapper=pd.DataFrame({'col_names':columns})\n",
    "    mapper[\"estimator\"] = model_type\n",
    "    mapper['cohort']=[i.split('_')[0] for i in mapper.col_names]\n",
    "    mapper['scenario']=[i.split('_')[2] for i in mapper.col_names]\n",
    "    mapper['model']=[i.split('_model')[1] for i in mapper.col_names]\n",
    "    mapper.set_index('col_names',inplace=True)\n",
    "    tprs.transpose()\n",
    "    mapped_tprs=pd.concat([mapper,tprs.transpose()],axis=1).set_index(['cohort','scenario','model', 'estimator'])\n",
    "    mapped_tprs.groupby(level=['cohort','scenario']).agg('mean').transpose()\n",
    "    mapped_tprs\n",
    "\n",
    "    # # Concatenate to the main DataFrame\n",
    "    all_tprs = pd.concat([all_tprs, mapped_tprs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAR Multi-Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rocs_multi_estimator(\n",
    "    all_tprs=all_tprs,\n",
    "    scenario_list=scenario_lists['incremental'],\n",
    "    model_types=model_types,\n",
    "    cohorts=['par'],\n",
    "    scenarios_colors=scenarios_colors,\n",
    "    n_splits=5,\n",
    "    fig_path=auroc_path,\n",
    "    title='PAR - AUROC Multi-Estimator',\n",
    "    line_styles=line_styles,\n",
    "    font_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Multi-Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rocs_multi_estimator(\n",
    "    all_tprs=all_tprs,\n",
    "    scenario_list=scenario_lists['incremental'],\n",
    "    model_types=model_types,\n",
    "    cohorts=['all'],\n",
    "    scenarios_colors=scenarios_colors,\n",
    "    n_splits=5,\n",
    "    fig_path=auroc_path,\n",
    "    title='All - AUROC Multi-Estimator',\n",
    "    line_styles=line_styles,\n",
    "    font_size=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
